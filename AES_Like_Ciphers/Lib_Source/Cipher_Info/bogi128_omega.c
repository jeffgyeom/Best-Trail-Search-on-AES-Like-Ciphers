#include "astbb.h"
#include "bit_perm_opt.h"

//((0, 1, 2, 3), (1, 0, 3, 2), (2, 3, 0, 1), (3, 2, 1, 0))
//[12, 9, 6, 3, 8, 13, 2, 7, 4, 1, 14, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_0(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a1ULL, 0x00005a580000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100620062ULL, 0x0064006400980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_0(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 0, 3, 2), (2, 3, 1, 0), (3, 2, 0, 1))
//[12, 9, 6, 3, 8, 13, 2, 7, 0, 5, 14, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_1(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a1ULL, 0x0000969400006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100620062ULL, 0x00a800a800540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_1(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060a0905060a0905ULL, 0x060a0905060a060aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 0, 3, 2), (3, 2, 0, 1), (2, 3, 1, 0))
//[8, 13, 6, 3, 12, 9, 2, 7, 4, 1, 14, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_2(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006961ULL, 0x00005a580000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a200a2ULL, 0x0064006400980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_2(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0605090a060509ULL, 0x0a0605090a060a06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 0, 3, 2), (3, 2, 1, 0), (2, 3, 0, 1))
//[8, 13, 6, 3, 12, 9, 2, 7, 0, 5, 14, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_3(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006961ULL, 0x0000969400006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a200a2ULL, 0x00a800a800540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_3(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 2, 3, 0), (2, 3, 0, 1), (3, 0, 1, 2))
//[12, 9, 6, 3, 0, 13, 10, 7, 4, 1, 14, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_4(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3339ccc6999c6663ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_4(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0093006c006c0093ULL, 0x0093006c006c0036ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 2, 3, 0), (3, 0, 1, 2), (2, 3, 0, 1))
//[8, 13, 6, 3, 12, 1, 10, 7, 0, 5, 14, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_5(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3335ccca555caaa3ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_5(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005300ac00ac0053ULL, 0x005300ac00ac003aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 3, 0, 2), (2, 0, 3, 1), (3, 2, 1, 0))
//[12, 9, 6, 3, 8, 1, 14, 7, 4, 13, 2, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_6(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000c3c1ULL, 0x00003c380000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100640064ULL, 0x0062006200980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_6(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a080eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (1, 3, 0, 2), (3, 2, 1, 0), (2, 0, 3, 1))
//[8, 13, 6, 3, 0, 9, 14, 7, 12, 5, 2, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_7(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096920000c3c1ULL, 0x00003c3400006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a800a8ULL, 0x00a200a200540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_7(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x060609090606040eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 0, 3, 1), (1, 3, 0, 2), (3, 2, 1, 0))
//[12, 5, 10, 3, 8, 13, 2, 7, 4, 1, 14, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_8(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000a5a1ULL, 0x00005a580000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100620062ULL, 0x0064006400980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_8(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c080eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 0, 3, 1), (3, 2, 1, 0), (1, 3, 0, 2))
//[4, 13, 10, 3, 12, 9, 2, 7, 0, 5, 14, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_9(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3800006961ULL, 0x000096940000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a200a2ULL, 0x00a800a800540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_9(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c040eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 0, 1), (1, 0, 3, 2), (3, 2, 1, 0))
//[12, 5, 10, 3, 8, 1, 14, 7, 4, 13, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_10(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c1ULL, 0x00003c380000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100640064ULL, 0x0062006200980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_10(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 0, 1), (1, 2, 3, 0), (3, 0, 1, 2))
//[12, 5, 10, 3, 0, 9, 14, 7, 4, 13, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_11(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5559aaa6999a6665ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_11(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0095006a006a0095ULL, 0x0095006a006a0056ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 0, 1), (3, 0, 1, 2), (1, 2, 3, 0))
//[4, 13, 10, 3, 8, 1, 14, 7, 12, 5, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_12(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9995666a5556aaa9ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_12(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005900a600a60059ULL, 0x005900a600a6009aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 0, 1), (3, 2, 1, 0), (1, 0, 3, 2))
//[4, 13, 10, 3, 0, 9, 14, 7, 12, 5, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_13(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c1ULL, 0x00003c340000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a800a8ULL, 0x00a200a200540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_13(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 1, 0), (1, 0, 3, 2), (3, 2, 0, 1))
//[12, 5, 10, 3, 8, 1, 14, 7, 0, 13, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_14(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c1ULL, 0x0000969200006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100640064ULL, 0x00c800c800320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_14(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060c0903060c0903ULL, 0x060c0903060c060cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (2, 3, 1, 0), (3, 2, 0, 1), (1, 0, 3, 2))
//[4, 13, 10, 3, 0, 9, 14, 7, 12, 1, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_15(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c1ULL, 0x00005a520000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a800a8ULL, 0x00c400c400320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_15(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c05030a0c0503ULL, 0x0a0c05030a0c0a0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 0, 1, 2), (1, 2, 3, 0), (2, 3, 0, 1))
//[8, 5, 14, 3, 12, 9, 2, 7, 0, 13, 6, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_16(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5553aaac333accc5ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_16(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003500ca00ca0035ULL, 0x003500ca00ca005cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 0, 1, 2), (2, 3, 0, 1), (1, 2, 3, 0))
//[4, 9, 14, 3, 8, 13, 2, 7, 12, 1, 6, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_17(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9993666c3336ccc9ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_17(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003900c600c60039ULL, 0x003900c600c6009cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 0, 1), (1, 0, 3, 2), (2, 3, 1, 0))
//[8, 5, 14, 3, 12, 1, 10, 7, 4, 13, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_18(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006961ULL, 0x00003c380000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c400c4ULL, 0x0062006200980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_18(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0603090c060309ULL, 0x0c0603090c060c06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 0, 1), (2, 3, 1, 0), (1, 0, 3, 2))
//[4, 9, 14, 3, 0, 13, 10, 7, 12, 5, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_19(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a1ULL, 0x00003c340000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c800c8ULL, 0x00a200a200540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_19(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a03050c0a0305ULL, 0x0c0a03050c0a0c0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 1, 0), (1, 0, 3, 2), (2, 3, 0, 1))
//[8, 5, 14, 3, 12, 1, 10, 7, 0, 13, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_20(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006961ULL, 0x0000969200006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c400c4ULL, 0x00c800c800320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_20(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 1, 0), (1, 3, 0, 2), (2, 0, 3, 1))
//[8, 5, 14, 3, 0, 13, 10, 7, 12, 1, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_21(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096940000a5a1ULL, 0x00005a5200006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c800c8ULL, 0x00c400c400320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_21(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x060609090606020eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 1, 0), (2, 0, 3, 1), (1, 3, 0, 2))
//[4, 9, 14, 3, 12, 1, 10, 7, 0, 13, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_22(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5800006961ULL, 0x000096920000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c400c4ULL, 0x00c800c800320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_22(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a020eULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 2, 3), (3, 2, 1, 0), (2, 3, 0, 1), (1, 0, 3, 2))
//[4, 9, 14, 3, 0, 13, 10, 7, 12, 1, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_23(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a1ULL, 0x00005a520000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c800c8ULL, 0x00c400c400320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_23(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 0, 2, 3), (2, 3, 0, 1), (3, 2, 1, 0))
//[12, 9, 6, 3, 8, 13, 2, 7, 4, 1, 10, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_24(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a1ULL, 0x0000696800009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100620062ULL, 0x0054005400a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_24(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0905060a090506ULL, 0x0a0905060a090a09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 0, 2, 3), (2, 3, 1, 0), (3, 2, 0, 1))
//[12, 9, 6, 3, 8, 13, 2, 7, 0, 5, 10, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_25(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000003c30ULL, 0x0000c3c00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000c000c0ULL, 0x0030003000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_25(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300c300c3ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000003c3ULL, 0x0000fc3c0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa59669ULL, 0x000000005aa59669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0000003300ffULL, 0x00cc0000003300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 0, 2, 3), (3, 2, 0, 1), (2, 3, 1, 0))
//[8, 13, 6, 3, 12, 9, 2, 7, 4, 1, 10, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_26(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000003c30ULL, 0x0000c3c00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000c000c0ULL, 0x0030003000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_26(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300c300c3ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000003c3ULL, 0x0000fc3c0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096695aa5ULL, 0x0000000096695aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0000003300ffULL, 0x00cc0000003300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 0, 2, 3), (3, 2, 1, 0), (2, 3, 0, 1))
//[8, 13, 6, 3, 12, 9, 2, 7, 0, 5, 10, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_27(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006961ULL, 0x0000a5a400005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a200a2ULL, 0x0098009800640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_27(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0605090a0605090aULL, 0x0605090a06050605ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 2, 0, 3), (2, 3, 1, 0), (3, 0, 2, 1))
//[12, 9, 6, 3, 0, 13, 10, 7, 8, 5, 2, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_28(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc39966ULL, 0x00000000936ccc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009600660069ULL, 0x00cc00c600330039ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_28(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003600c900c90036ULL, 0x003600c900c90036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x936c936c936c6c93ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 2, 0, 3), (3, 0, 2, 1), (2, 3, 1, 0))
//[8, 13, 6, 3, 12, 1, 10, 7, 4, 9, 2, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_29(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc355aaULL, 0x0000000053accc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005a00aa00a5ULL, 0x00cc00ca00330035ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_29(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003a00c500c5003aULL, 0x003a00c500c5003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x53ac53ac53acac53ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 3, 2, 0), (2, 0, 1, 3), (3, 2, 0, 1))
//[12, 9, 6, 3, 8, 1, 14, 7, 0, 5, 10, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_30(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c35a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000005a50ULL, 0x0000c3c00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000a000a0ULL, 0x0030003000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_30(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300810081ULL, 0x0024002400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000005a5ULL, 0x0000fc3c0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007e819669ULL, 0x000000007e819669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e80000001700ffULL, 0x00e80000001700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (1, 3, 2, 0), (3, 2, 0, 1), (2, 0, 1, 3))
//[8, 13, 6, 3, 0, 9, 14, 7, 4, 1, 10, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_31(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c396966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000009690ULL, 0x0000c3c000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000600060ULL, 0x0030003000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_31(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300410041ULL, 0x0028002800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000969ULL, 0x0000fc3c0000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000be415aa5ULL, 0x00000000be415aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e40000001b00ffULL, 0x00e40000001b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 0, 1, 3), (1, 3, 2, 0), (3, 2, 0, 1))
//[12, 5, 10, 3, 8, 13, 2, 7, 0, 9, 6, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_32(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a53c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000003c30ULL, 0x0000a5a00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000c000c0ULL, 0x0050005000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_32(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500810081ULL, 0x0042004200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000003c3ULL, 0x0000fa5a0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007e819669ULL, 0x000000007e819669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e80000001700ffULL, 0x00e80000001700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 0, 1, 3), (3, 2, 0, 1), (1, 3, 2, 0))
//[4, 13, 10, 3, 12, 9, 2, 7, 8, 1, 6, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_33(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x969669693c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000003c30ULL, 0x000069600000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000c000c0ULL, 0x0090009000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_33(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900410041ULL, 0x0082008200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000969000003c3ULL, 0x0000f6960000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000be415aa5ULL, 0x00000000be415aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e40000001b00ffULL, 0x00e40000001b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 0, 1), (1, 0, 2, 3), (3, 2, 1, 0))
//[12, 5, 10, 3, 8, 1, 14, 7, 4, 9, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_34(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c1ULL, 0x0000696800009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100640064ULL, 0x0032003200c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_34(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0903060c090306ULL, 0x0c0903060c090c09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 0, 1), (3, 2, 1, 0), (1, 0, 2, 3))
//[4, 13, 10, 3, 0, 9, 14, 7, 8, 5, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_35(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c1ULL, 0x0000a5a400005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a800a8ULL, 0x0032003200c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_35(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c05030a0c05030aULL, 0x0c05030a0c050c05ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 1, 0), (1, 0, 2, 3), (3, 2, 0, 1))
//[12, 5, 10, 3, 8, 1, 14, 7, 0, 9, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_36(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000005a50ULL, 0x0000a5a00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000a000a0ULL, 0x0050005000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_36(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500a500a5ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000005a5ULL, 0x0000fa5a0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc39669ULL, 0x000000003cc39669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0000005500ffULL, 0x00aa0000005500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 1, 0), (1, 2, 0, 3), (3, 0, 2, 1))
//[12, 5, 10, 3, 0, 9, 14, 7, 8, 1, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_37(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa59966ULL, 0x00000000956aaa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009600660069ULL, 0x00aa00a600550059ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_37(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005600a900a90056ULL, 0x005600a900a90056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x956a956a956a6a95ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 1, 0), (3, 0, 2, 1), (1, 2, 0, 3))
//[4, 13, 10, 3, 8, 1, 14, 7, 0, 9, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_38(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000966955aaULL, 0x0000000059a66699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005a00aa00a5ULL, 0x0066006a00990095ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_38(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009a00650065009aULL, 0x009a00650065009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x59a659a659a6a659ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (2, 3, 1, 0), (3, 2, 0, 1), (1, 0, 2, 3))
//[4, 13, 10, 3, 0, 9, 14, 7, 8, 1, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_39(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000009690ULL, 0x0000696000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000600060ULL, 0x0090009000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_39(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900690069ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000969ULL, 0x0000f6960000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc35aa5ULL, 0x000000003cc35aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00660000009900ffULL, 0x00660000009900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 0, 2, 1), (1, 2, 0, 3), (2, 3, 1, 0))
//[8, 5, 14, 3, 12, 9, 2, 7, 4, 1, 10, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_40(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa533ccULL, 0x0000000035caaa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003c00cc00c3ULL, 0x00aa00ac00550053ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_40(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005c00a300a3005cULL, 0x005c00a300a3005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x35ca35ca35caca35ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 0, 2, 1), (2, 3, 1, 0), (1, 2, 0, 3))
//[4, 9, 14, 3, 8, 13, 2, 7, 0, 5, 10, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_41(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000966933ccULL, 0x0000000039c66699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003c00cc00c3ULL, 0x0066006c00990093ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_41(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009c00630063009cULL, 0x009c00630063009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x39c639c639c6c639ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 0, 1), (1, 0, 2, 3), (2, 3, 1, 0))
//[8, 5, 14, 3, 12, 1, 10, 7, 4, 9, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_42(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000005a50ULL, 0x0000a5a00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000a000a0ULL, 0x0050005000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_42(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500a500a5ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000005a5ULL, 0x0000fa5a0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096693cc3ULL, 0x0000000096693cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0000005500ffULL, 0x00aa0000005500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 0, 1), (1, 3, 2, 0), (2, 0, 1, 3))
//[8, 5, 14, 3, 0, 13, 10, 7, 4, 9, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_43(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a596966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000009690ULL, 0x0000a5a000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000600060ULL, 0x0050005000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_43(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500210021ULL, 0x0048004800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000969ULL, 0x0000fa5a0000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000de213cc3ULL, 0x00000000de213cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e20000001d00ffULL, 0x00e20000001d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 0, 1), (2, 0, 1, 3), (1, 3, 2, 0))
//[4, 9, 14, 3, 12, 1, 10, 7, 8, 5, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_44(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x969669695a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000005a50ULL, 0x000069600000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000a000a0ULL, 0x0090009000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_44(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900210021ULL, 0x0084008400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000969000005a5ULL, 0x0000f6960000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000de213cc3ULL, 0x00000000de213cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00e20000001d00ffULL, 0x00e20000001d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 0, 1), (2, 3, 1, 0), (1, 0, 2, 3))
//[4, 9, 14, 3, 0, 13, 10, 7, 8, 5, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_45(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000009690ULL, 0x0000696000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000600060ULL, 0x0090009000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_45(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900690069ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000969ULL, 0x0000f6960000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa53cc3ULL, 0x000000005aa53cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00660000009900ffULL, 0x00660000009900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 1, 0), (1, 0, 2, 3), (2, 3, 0, 1))
//[8, 5, 14, 3, 12, 1, 10, 7, 0, 9, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_46(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006961ULL, 0x0000c3c200003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c400c4ULL, 0x0098009800620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_46(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0603090c0603090cULL, 0x0603090c06030603ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 1, 3, 2), (3, 2, 1, 0), (2, 3, 0, 1), (1, 0, 2, 3))
//[4, 9, 14, 3, 0, 13, 10, 7, 8, 1, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_47(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a1ULL, 0x0000c3c200003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c800c8ULL, 0x0054005400a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_47(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a03050c0a03050cULL, 0x0a03050c0a030a03ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 0, 3, 2), (2, 3, 0, 1), (3, 1, 2, 0))
//[12, 9, 6, 3, 4, 13, 2, 11, 8, 1, 14, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_48(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5200003c38ULL, 0x0000c3c10000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100620062ULL, 0x0064006400980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_48(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc00acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0b02ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 0, 3, 2), (3, 1, 2, 0), (2, 3, 0, 1))
//[8, 13, 6, 3, 12, 5, 2, 11, 0, 9, 14, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_49(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200003c34ULL, 0x0000c3c100006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a200a2ULL, 0x00a800a800540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_49(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060702ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 0, 2), (2, 0, 3, 1), (3, 1, 2, 0))
//[12, 9, 6, 3, 4, 1, 14, 11, 8, 13, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_50(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000039c639c6ULL, 0x0000000036c936c9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_50(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 0, 2), (2, 1, 3, 0), (3, 0, 2, 1))
//[12, 9, 6, 3, 0, 5, 14, 11, 8, 13, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_51(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500006969ULL, 0x00005a5a00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050509090909ULL, 0x0a0a0a0a06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_51(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0905060a0905060aULL, 0x0905060a0905060aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 0, 2), (3, 0, 2, 1), (2, 1, 3, 0))
//[8, 13, 6, 3, 4, 1, 14, 11, 12, 9, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_52(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000a5a5ULL, 0x0000969600005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090905050505ULL, 0x060606060a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_52(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05090a0605090a06ULL, 0x05090a0605090a06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 0, 2), (3, 1, 2, 0), (2, 0, 3, 1))
//[8, 13, 6, 3, 0, 5, 14, 11, 12, 9, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_53(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ca35caULL, 0x000000003ac53ac5ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_53(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 2, 0), (2, 0, 3, 1), (3, 1, 0, 2))
//[12, 9, 6, 3, 4, 1, 14, 11, 0, 13, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_54(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000039c639c6ULL, 0x000000009c639c63ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_54(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00930039006c00c6ULL, 0x00930039006c00c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (1, 3, 2, 0), (3, 1, 0, 2), (2, 0, 3, 1))
//[8, 13, 6, 3, 0, 5, 14, 11, 12, 1, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_55(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ca35caULL, 0x000000005ca35ca3ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_55(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0053003500ac00caULL, 0x0053003500ac00caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 0, 3, 1), (1, 3, 0, 2), (3, 1, 2, 0))
//[12, 5, 10, 3, 4, 13, 2, 11, 8, 1, 14, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_56(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000059a659a6ULL, 0x0000000056a956a9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_56(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 0, 3, 1), (1, 3, 2, 0), (3, 1, 0, 2))
//[12, 5, 10, 3, 4, 13, 2, 11, 0, 9, 14, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_57(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000059a659a6ULL, 0x000000009a659a65ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_57(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00950059006a00a6ULL, 0x00950059006a00a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 0, 3, 1), (3, 1, 0, 2), (1, 3, 2, 0))
//[4, 13, 10, 3, 12, 5, 2, 11, 8, 1, 14, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_58(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000956a956aULL, 0x0000000056a956a9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_58(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0059009500a6006aULL, 0x0059009500a6006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 0, 3, 1), (3, 1, 2, 0), (1, 3, 0, 2))
//[4, 13, 10, 3, 12, 5, 2, 11, 0, 9, 14, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_59(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000956a956aULL, 0x000000009a659a65ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_59(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 1, 3, 0), (1, 3, 0, 2), (3, 0, 2, 1))
//[12, 5, 10, 3, 0, 13, 6, 11, 8, 1, 14, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_60(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300006969ULL, 0x00003c3c00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030309090909ULL, 0x0c0c0c0c06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_60(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0903060c0903060cULL, 0x0903060c0903060cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 1, 3, 0), (3, 0, 2, 1), (1, 3, 0, 2))
//[4, 13, 10, 3, 12, 1, 6, 11, 0, 9, 14, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_61(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000a5a5ULL, 0x00003c3c00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030305050505ULL, 0x0c0c0c0c0a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_61(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05030a0c05030a0cULL, 0x05030a0c05030a0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 3, 0, 1), (1, 0, 3, 2), (3, 1, 2, 0))
//[12, 5, 10, 3, 4, 1, 14, 11, 8, 13, 2, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_62(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3400005a58ULL, 0x0000a5a10000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0091009100640064ULL, 0x0062006200980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_62(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa00caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0d04ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (2, 3, 0, 1), (3, 1, 2, 0), (1, 0, 3, 2))
//[4, 13, 10, 3, 0, 5, 14, 11, 12, 9, 2, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_63(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3800009694ULL, 0x000069610000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0051005100a800a8ULL, 0x00a200a200540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_63(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x00990066006600c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0d08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 0, 2, 1), (1, 3, 0, 2), (2, 1, 3, 0))
//[8, 5, 14, 3, 4, 13, 2, 11, 12, 1, 10, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_64(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000c3c3ULL, 0x0000969600003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090903030303ULL, 0x060606060c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_64(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03090c0603090c06ULL, 0x03090c0603090c06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 0, 2, 1), (2, 1, 3, 0), (1, 3, 0, 2))
//[4, 9, 14, 3, 12, 5, 2, 11, 0, 13, 10, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_65(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000c3c3ULL, 0x00005a5a00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050503030303ULL, 0x0a0a0a0a0c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_65(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03050c0a03050c0aULL, 0x03050c0a03050c0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 0, 2), (1, 3, 2, 0), (2, 0, 3, 1))
//[8, 5, 14, 3, 0, 13, 6, 11, 12, 9, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_66(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ac53acULL, 0x000000003ac53ac5ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_66(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0035005300ca00acULL, 0x0035005300ca00acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 0, 2), (2, 0, 3, 1), (1, 3, 2, 0))
//[4, 9, 14, 3, 12, 1, 6, 11, 8, 13, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_67(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000936c936cULL, 0x0000000036c936c9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_67(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0039009300c6006cULL, 0x0039009300c6006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 2, 0), (1, 0, 3, 2), (2, 3, 0, 1))
//[8, 5, 14, 3, 12, 1, 6, 11, 0, 13, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_68(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400005a52ULL, 0x0000a5a100006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c400c4ULL, 0x00c800c800320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_68(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060704ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 2, 0), (1, 3, 0, 2), (2, 0, 3, 1))
//[8, 5, 14, 3, 0, 13, 6, 11, 12, 1, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_69(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ac53acULL, 0x000000005ca35ca3ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_69(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 2, 0), (2, 0, 3, 1), (1, 3, 0, 2))
//[4, 9, 14, 3, 12, 1, 6, 11, 0, 13, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_70(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000936c936cULL, 0x000000009c639c63ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_70(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 1, 3), (3, 1, 2, 0), (2, 3, 0, 1), (1, 0, 3, 2))
//[4, 9, 14, 3, 0, 13, 6, 11, 12, 1, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_71(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5800009692ULL, 0x000069610000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0031003100c800c8ULL, 0x00c400c400320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_71(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x00990066006600a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0b08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 0, 2, 3), (2, 3, 1, 0), (3, 1, 0, 2))
//[12, 9, 6, 3, 4, 13, 2, 11, 0, 5, 10, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_72(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c3a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c300000a5a0ULL, 0x0000c3c000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000500050ULL, 0x0030003000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_72(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300420042ULL, 0x0018001800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000a5aULL, 0x0000fc3c0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000db249669ULL, 0x00000000db249669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004d000000b200ffULL, 0x004d000000b200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 0, 2, 3), (3, 1, 0, 2), (2, 3, 1, 0))
//[8, 13, 6, 3, 12, 5, 2, 11, 4, 1, 10, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_73(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c369699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000006960ULL, 0x0000c3c000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000900090ULL, 0x0030003000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_73(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300820082ULL, 0x0014001400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000696ULL, 0x0000fc3c0000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000d7285aa5ULL, 0x00000000d7285aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008d0000007200ffULL, 0x008d0000007200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 0, 2), (2, 0, 1, 3), (3, 1, 2, 0))
//[12, 9, 6, 3, 4, 1, 14, 11, 8, 5, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_74(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000039c639c6ULL, 0x00000000639c639cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_74(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00630036009c00c9ULL, 0x00630036009c00c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 0, 2), (3, 1, 2, 0), (2, 0, 1, 3))
//[8, 13, 6, 3, 0, 5, 14, 11, 4, 9, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_75(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ca35caULL, 0x00000000a35ca35cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_75(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a3003a005c00c5ULL, 0x00a3003a005c00c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 2, 0), (2, 0, 1, 3), (3, 1, 0, 2))
//[12, 9, 6, 3, 4, 1, 14, 11, 0, 5, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_76(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000039c639c6ULL, 0x00000000c936c936ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_76(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300000000ULL, 0x003c003c00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000c3cULL, 0x0000fc3c0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff009669ULL, 0x00000000ff009669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00690000009600ffULL, 0x00690000009600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 2, 0), (2, 1, 0, 3), (3, 0, 1, 2))
//[12, 9, 6, 3, 0, 5, 14, 11, 4, 1, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_77(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500006969ULL, 0x0000969600005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050509090909ULL, 0x060606060a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_77(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x060a060a09050905ULL, 0x09050905060a060aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965a69a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 2, 0), (3, 0, 1, 2), (2, 1, 0, 3))
//[8, 13, 6, 3, 4, 1, 14, 11, 0, 5, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_78(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000a5a5ULL, 0x00005a5a00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090905050505ULL, 0x0a0a0a0a06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_78(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a060a0605090509ULL, 0x050905090a060a06ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a96a5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (1, 3, 2, 0), (3, 1, 0, 2), (2, 0, 1, 3))
//[8, 13, 6, 3, 0, 5, 14, 11, 4, 1, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_79(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ca35caULL, 0x00000000c53ac53aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_79(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300000000ULL, 0x003c003c00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000c3cULL, 0x0000fc3c0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff005aa5ULL, 0x00000000ff005aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00a50000005a00ffULL, 0x00a50000005a00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 0, 1, 3), (1, 3, 0, 2), (3, 1, 2, 0))
//[12, 5, 10, 3, 4, 13, 2, 11, 8, 1, 6, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_80(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000059a659a6ULL, 0x00000000659a659aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_80(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00650056009a00a9ULL, 0x00650056009a00a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 0, 1, 3), (1, 3, 2, 0), (3, 1, 0, 2))
//[12, 5, 10, 3, 4, 13, 2, 11, 0, 9, 6, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_81(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000059a659a6ULL, 0x00000000a956a956ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_81(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500000000ULL, 0x005a005a00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000a5aULL, 0x0000fa5a0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff009669ULL, 0x00000000ff009669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00690000009600ffULL, 0x00690000009600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 0, 1, 3), (3, 1, 0, 2), (1, 3, 2, 0))
//[4, 13, 10, 3, 12, 5, 2, 11, 8, 1, 6, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_82(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000956a956aULL, 0x00000000659a659aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_82(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900000000ULL, 0x0096009600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000696ULL, 0x0000f6960000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff005aa5ULL, 0x00000000ff005aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00a50000005a00ffULL, 0x00a50000005a00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 0, 1, 3), (3, 1, 2, 0), (1, 3, 0, 2))
//[4, 13, 10, 3, 12, 5, 2, 11, 0, 9, 6, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_83(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000956a956aULL, 0x00000000a956a956ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_83(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a9009a00560065ULL, 0x00a9009a00560065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 1, 0, 3), (1, 3, 2, 0), (3, 0, 1, 2))
//[12, 5, 10, 3, 0, 13, 6, 11, 4, 9, 2, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_84(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300006969ULL, 0x0000969600003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030309090909ULL, 0x060606060c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_84(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x060c060c09030903ULL, 0x09030903060c060cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963c69c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 1, 0, 3), (3, 0, 1, 2), (1, 3, 2, 0))
//[4, 13, 10, 3, 12, 1, 6, 11, 8, 5, 2, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_85(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000a5a5ULL, 0x00005a5a00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030305050505ULL, 0x0a0a0a0a0c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_85(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c0a0c05030503ULL, 0x050305030a0c0a0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3ca5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 3, 1, 0), (1, 0, 2, 3), (3, 1, 0, 2))
//[12, 5, 10, 3, 4, 1, 14, 11, 0, 9, 6, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_86(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a5c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a500000c3c0ULL, 0x0000a5a000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000300030ULL, 0x0050005000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_86(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500240024ULL, 0x0018001800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000c3cULL, 0x0000fa5a0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000bd429669ULL, 0x00000000bd429669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002b000000d400ffULL, 0x002b000000d400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (2, 3, 1, 0), (3, 1, 0, 2), (1, 0, 2, 3))
//[4, 13, 10, 3, 0, 5, 14, 11, 8, 1, 6, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_87(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96966969c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000096900000c3c0ULL, 0x0000696000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000300030ULL, 0x0090009000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_87(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900280028ULL, 0x0014001400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff6000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000c3cULL, 0x0000f6960000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007d825aa5ULL, 0x000000007d825aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0027000000d800ffULL, 0x0027000000d800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 0, 1, 2), (1, 3, 2, 0), (2, 1, 0, 3))
//[8, 5, 14, 3, 4, 13, 2, 11, 0, 9, 6, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_88(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000c3c3ULL, 0x00003c3c00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090903030303ULL, 0x0c0c0c0c06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_88(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c060c0603090309ULL, 0x030903090c060c06ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c96c3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 0, 1, 2), (2, 1, 0, 3), (1, 3, 2, 0))
//[4, 9, 14, 3, 12, 5, 2, 11, 8, 1, 6, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_89(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000c3c3ULL, 0x00003c3c00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050503030303ULL, 0x0c0c0c0c0a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_89(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a0c0a03050305ULL, 0x030503050c0a0c0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5ac3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 0, 2), (1, 0, 2, 3), (2, 3, 1, 0))
//[8, 5, 14, 3, 12, 1, 6, 11, 4, 9, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_90(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a569699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000006960ULL, 0x0000a5a000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000900090ULL, 0x0050005000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_90(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500840084ULL, 0x0012001200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000696ULL, 0x0000fa5a0000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000b7483cc3ULL, 0x00000000b7483cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008b0000007400ffULL, 0x008b0000007400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 0, 2), (1, 3, 2, 0), (2, 0, 1, 3))
//[8, 5, 14, 3, 0, 13, 6, 11, 4, 9, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_91(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ac53acULL, 0x00000000a35ca35cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_91(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500000000ULL, 0x005a005a00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000a5aULL, 0x0000fa5a0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff003cc3ULL, 0x00000000ff003cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00c30000003c00ffULL, 0x00c30000003c00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 0, 2), (2, 0, 1, 3), (1, 3, 2, 0))
//[4, 9, 14, 3, 12, 1, 6, 11, 8, 5, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_92(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000936c936cULL, 0x00000000639c639cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_92(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900000000ULL, 0x0096009600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000696ULL, 0x0000f6960000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff003cc3ULL, 0x00000000ff003cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00c30000003c00ffULL, 0x00c30000003c00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 0, 2), (2, 3, 1, 0), (1, 0, 2, 3))
//[4, 9, 14, 3, 0, 13, 6, 11, 8, 5, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_93(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96966969a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000096900000a5a0ULL, 0x0000696000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000500050ULL, 0x0090009000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_93(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900480048ULL, 0x0012001200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff6000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000a5aULL, 0x0000f6960000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007b843cc3ULL, 0x000000007b843cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0047000000b800ffULL, 0x0047000000b800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 2, 0), (1, 3, 0, 2), (2, 0, 1, 3))
//[8, 5, 14, 3, 0, 13, 6, 11, 4, 1, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_94(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ac53acULL, 0x00000000c53ac53aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_94(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c5005c003a00a3ULL, 0x00c5005c003a00a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 2, 3, 1), (3, 1, 2, 0), (2, 0, 1, 3), (1, 3, 0, 2))
//[4, 9, 14, 3, 12, 1, 6, 11, 0, 5, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_95(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000936c936cULL, 0x00000000c936c936ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_95(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c9009c00360063ULL, 0x00c9009c00360063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 0, 2, 3), (2, 1, 3, 0), (3, 2, 0, 1))
//[12, 9, 6, 3, 8, 5, 2, 15, 0, 13, 10, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_96(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c9366699ULL, 0x000000009669cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006600630099009cULL, 0x00cc00c30033003cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_96(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0093006c006c0093ULL, 0x0093006c006c0093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x36c936c936c9c936ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 0, 2, 3), (3, 2, 0, 1), (2, 1, 3, 0))
//[8, 13, 6, 3, 4, 9, 2, 15, 12, 1, 10, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_97(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c53aaa55ULL, 0x000000005aa5cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a30055005cULL, 0x00cc00c30033003cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_97(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005300ac00ac0053ULL, 0x005300ac00ac0053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ac53ac53ac5c53aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 0, 3), (2, 0, 3, 1), (3, 1, 2, 0))
//[12, 9, 6, 3, 4, 1, 10, 15, 8, 13, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_98(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500009696ULL, 0x00005a5a00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050506060606ULL, 0x0a0a0a0a09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_98(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05060a0905060a09ULL, 0x05060a0905060a09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 0, 3), (2, 1, 3, 0), (3, 0, 2, 1))
//[12, 9, 6, 3, 0, 5, 10, 15, 8, 13, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_99(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_99(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc0033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996696996ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 0, 3), (3, 0, 2, 1), (2, 1, 3, 0))
//[8, 13, 6, 3, 4, 1, 10, 15, 12, 9, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_100(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_100(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc0033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa5a55aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 0, 3), (3, 1, 2, 0), (2, 0, 3, 1))
//[8, 13, 6, 3, 0, 5, 10, 15, 12, 9, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_101(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900005a5aULL, 0x000096960000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090a0a0a0aULL, 0x0606060605050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_101(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090a0605090a0605ULL, 0x090a0605090a0605ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 3, 0), (2, 1, 0, 3), (3, 0, 2, 1))
//[12, 9, 6, 3, 0, 5, 10, 15, 8, 1, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_102(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_102(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004100ee00280077ULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003d320000cbc4ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcb34ef103dc27f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005d007900ab00e9ULL, 0x00a2008600540016ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca53caacULL, 0x00000000caac35acULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (1, 2, 3, 0), (3, 0, 2, 1), (2, 1, 0, 3))
//[8, 13, 6, 3, 4, 1, 10, 15, 0, 9, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_103(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_103(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008100ee002400bbULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003d320000c7c8ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc738ef103dc2bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x009d00b5006700e5ULL, 0x0062004a0098001aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c693c66cULL, 0x00000000c66c396cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 0, 3, 1), (1, 2, 0, 3), (3, 1, 2, 0))
//[12, 5, 10, 3, 4, 9, 2, 15, 8, 1, 14, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_104(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300009696ULL, 0x00003c3c00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030306060606ULL, 0x0c0c0c0c09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_104(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03060c0903060c09ULL, 0x03060c0903060c09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 0, 3, 1), (3, 1, 2, 0), (1, 2, 0, 3))
//[4, 13, 10, 3, 8, 5, 2, 15, 0, 9, 14, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_105(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300005a5aULL, 0x00003c3c0000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030303030a0a0a0aULL, 0x0c0c0c0c05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_105(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030a0c05030a0c05ULL, 0x030a0c05030a0c05ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 0, 3), (1, 2, 3, 0), (3, 0, 2, 1))
//[12, 5, 10, 3, 0, 9, 6, 15, 8, 13, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_106(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_106(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002100ee00480077ULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005b540000ada2ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xad52ef105ba47f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003b007900cd00e9ULL, 0x00c4008600320016ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac35accaULL, 0x00000000acca53caULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 0, 3), (3, 0, 2, 1), (1, 2, 3, 0))
//[4, 13, 10, 3, 8, 1, 6, 15, 12, 9, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_107(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_107(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002100ee008400bbULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000979800006d62ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6d92ef109768bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003700b500cd00e5ULL, 0x00c8004a0032001aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c396cc6ULL, 0x000000006cc693c6ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 3, 0), (1, 0, 2, 3), (3, 2, 0, 1))
//[12, 5, 10, 3, 8, 1, 6, 15, 0, 9, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_108(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a9566699ULL, 0x000000009669aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006600650099009aULL, 0x00aa00a50055005aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_108(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0095006a006a0095ULL, 0x0095006a006a0095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x56a956a956a9a956ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 3, 0), (1, 2, 0, 3), (3, 0, 2, 1))
//[12, 5, 10, 3, 0, 9, 6, 15, 8, 1, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_109(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_109(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa0055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996696996ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 3, 0), (3, 0, 2, 1), (1, 2, 0, 3))
//[4, 13, 10, 3, 8, 1, 6, 15, 0, 9, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_110(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_110(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa5a55aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (2, 1, 3, 0), (3, 2, 0, 1), (1, 0, 2, 3))
//[4, 13, 10, 3, 0, 9, 6, 15, 8, 1, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_111(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000659aaa55ULL, 0x000000005aa56699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a900550056ULL, 0x0066006900990096ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_111(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005900a600a60059ULL, 0x005900a600a60059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9a659a659a65659aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 0, 2, 1), (1, 2, 0, 3), (2, 1, 3, 0))
//[8, 5, 14, 3, 4, 9, 2, 15, 12, 1, 10, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_112(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_112(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa0055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc3c33cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 0, 2, 1), (1, 2, 3, 0), (2, 1, 0, 3))
//[8, 5, 14, 3, 4, 9, 2, 15, 0, 13, 10, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_113(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_113(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008100ee004200ddULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005b540000a7a8ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa758ef105ba4df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x009b00d3006700e3ULL, 0x0064002c0098001cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a695a66aULL, 0x00000000a66a596aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 0, 2, 1), (2, 1, 0, 3), (1, 2, 3, 0))
//[4, 9, 14, 3, 8, 5, 2, 15, 12, 1, 10, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_114(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_114(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004100ee008200ddULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000979800006b64ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6b94ef109768df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005700d300ab00e3ULL, 0x00a8002c0054001cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a596aa6ULL, 0x000000006aa695a6ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 0, 2, 1), (2, 1, 3, 0), (1, 2, 0, 3))
//[4, 9, 14, 3, 8, 5, 2, 15, 0, 13, 10, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_115(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_115(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc3c33cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 1, 2, 0), (1, 2, 0, 3), (2, 0, 3, 1))
//[8, 5, 14, 3, 0, 9, 6, 15, 12, 1, 10, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_116(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900003c3cULL, 0x000096960000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090c0c0c0cULL, 0x0606060603030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_116(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090c0603090c0603ULL, 0x090c0603090c0603ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 1, 2, 0), (2, 0, 3, 1), (1, 2, 0, 3))
//[4, 9, 14, 3, 8, 1, 6, 15, 0, 13, 10, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_117(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500003c3cULL, 0x00005a5a0000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050505050c0c0c0cULL, 0x0a0a0a0a03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_117(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050c0a03050c0a03ULL, 0x050c0a03050c0a03ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 2, 0, 1), (1, 0, 2, 3), (2, 1, 3, 0))
//[8, 5, 14, 3, 4, 1, 10, 15, 12, 9, 2, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_118(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a35ccc33ULL, 0x000000003cc3aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c50033003aULL, 0x00aa00a50055005aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_118(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003500ca00ca0035ULL, 0x003500ca00ca0035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ca35ca35ca3a35cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 1, 2), (3, 2, 0, 1), (2, 1, 3, 0), (1, 0, 2, 3))
//[4, 9, 14, 3, 0, 5, 10, 15, 8, 13, 2, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_119(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000639ccc33ULL, 0x000000003cc36699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c900330036ULL, 0x0066006900990096ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_119(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003900c600c60039ULL, 0x003900c600c60039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9c639c639c63639cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 0, 3, 2), (2, 1, 0, 3), (3, 2, 1, 0))
//[12, 9, 6, 3, 8, 5, 2, 15, 4, 1, 14, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_120(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3339ccc66663999cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_120(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003600c900c90036ULL, 0x003600c900c90093ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 0, 3, 2), (3, 2, 1, 0), (2, 1, 0, 3))
//[8, 13, 6, 3, 4, 9, 2, 15, 0, 5, 14, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_121(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3335cccaaaa3555cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_121(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003a00c500c5003aULL, 0x003a00c500c50053ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 0, 3), (2, 1, 3, 0), (3, 0, 1, 2))
//[12, 9, 6, 3, 0, 5, 10, 15, 4, 13, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_122(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_122(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008200dd001400bbULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003e310000c7c8ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7c83fd02e31cfb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ea006b0075006dULL, 0x00150094008a0092ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005c3a5cc5ULL, 0x000000005cc5a3c5ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 0, 3), (3, 0, 1, 2), (2, 1, 3, 0))
//[8, 13, 6, 3, 4, 1, 10, 15, 12, 5, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_123(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_123(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004200dd00180077ULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003e310000cbc4ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xbc43fd02e31cf708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00e600a700b900adULL, 0x0019005800460052ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c369cc9ULL, 0x000000009cc963c9ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 3, 0), (2, 0, 1, 3), (3, 1, 0, 2))
//[12, 9, 6, 3, 4, 1, 10, 15, 0, 5, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_124(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500009696ULL, 0x0000696900005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050506060606ULL, 0x090909090a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_124(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a090a0905060506ULL, 0x050605060a090a09ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a69a5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 3, 0), (2, 1, 0, 3), (3, 0, 1, 2))
//[12, 9, 6, 3, 0, 5, 10, 15, 4, 1, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_125(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_125(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609060909060906ULL, 0x0906090606090609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966969969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 3, 0), (3, 0, 1, 2), (2, 1, 0, 3))
//[8, 13, 6, 3, 4, 1, 10, 15, 0, 5, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_126(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_126(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a050a05050a050aULL, 0x050a050a0a050a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa5a55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (1, 2, 3, 0), (3, 1, 0, 2), (2, 0, 1, 3))
//[8, 13, 6, 3, 0, 5, 10, 15, 4, 1, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_127(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900005a5aULL, 0x0000a5a500009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090a0a0a0aULL, 0x0505050506060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_127(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x06050605090a090aULL, 0x090a090a06050605ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a5695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 0, 1, 3), (1, 2, 3, 0), (3, 1, 0, 2))
//[12, 5, 10, 3, 4, 9, 2, 15, 0, 13, 6, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_128(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300009696ULL, 0x0000696900003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030306060606ULL, 0x090909090c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_128(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c090c0903060306ULL, 0x030603060c090c09ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c69c3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 0, 1, 3), (3, 1, 0, 2), (1, 2, 3, 0))
//[4, 13, 10, 3, 8, 5, 2, 15, 12, 1, 6, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_129(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300005a5aULL, 0x0000a5a500003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030303030a0a0a0aULL, 0x050505050c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_129(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c050c05030a030aULL, 0x030a030a0c050c05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca5c35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 0, 3), (1, 0, 3, 2), (3, 2, 1, 0))
//[12, 5, 10, 3, 8, 1, 6, 15, 4, 13, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_130(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5559aaa66665999aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_130(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005600a900a90056ULL, 0x005600a900a90095ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 0, 3), (1, 2, 3, 0), (3, 0, 1, 2))
//[12, 5, 10, 3, 0, 9, 6, 15, 4, 13, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_131(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_131(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609060909060906ULL, 0x0906090606090609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966969969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 0, 3), (3, 0, 1, 2), (1, 2, 3, 0))
//[4, 13, 10, 3, 8, 1, 6, 15, 12, 5, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_132(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_132(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a050a05050a050aULL, 0x050a050a0a050a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa5a55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 0, 3), (3, 2, 1, 0), (1, 0, 3, 2))
//[4, 13, 10, 3, 0, 9, 6, 15, 12, 5, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_133(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9995666aaaa95556ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_133(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009a00650065009aULL, 0x009a006500650059ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 3, 0), (1, 2, 0, 3), (3, 0, 1, 2))
//[12, 5, 10, 3, 0, 9, 6, 15, 4, 1, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_134(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_134(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008400bb001200ddULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005e510000a7a8ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7a85fb04e51afd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ec006d0073006bULL, 0x00130092008c0094ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003a5c3aa3ULL, 0x000000003aa3c5a3ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (2, 1, 3, 0), (3, 0, 1, 2), (1, 2, 0, 3))
//[4, 13, 10, 3, 8, 1, 6, 15, 0, 5, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_135(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_135(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00480077001200ddULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00009e9100006b64ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xb649f708e916fd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ec00ad00b300a7ULL, 0x00130052004c0058ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000369c3663ULL, 0x000000003663c963ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 0, 1, 2), (1, 2, 0, 3), (2, 1, 3, 0))
//[8, 5, 14, 3, 4, 9, 2, 15, 12, 1, 6, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_136(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_136(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002400bb00180077ULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005e510000ada2ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xda25fb04e51af708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00e600c700d900cbULL, 0x0019003800260034ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a569aa9ULL, 0x000000009aa965a9ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 0, 1, 2), (1, 2, 3, 0), (2, 1, 0, 3))
//[8, 5, 14, 3, 4, 9, 2, 15, 0, 13, 6, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_137(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_137(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c030c03030c030cULL, 0x030c030c0c030c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc3c33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 0, 1, 2), (2, 1, 0, 3), (1, 2, 3, 0))
//[4, 9, 14, 3, 8, 5, 2, 15, 12, 1, 6, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_138(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_138(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c030c03030c030cULL, 0x030c030c0c030c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc3c33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 0, 1, 2), (2, 1, 3, 0), (1, 2, 0, 3))
//[4, 9, 14, 3, 8, 5, 2, 15, 0, 13, 6, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_139(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_139(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00280077001400bbULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00009e9100006d62ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xd629f708e916fb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ea00cb00d500c7ULL, 0x00150034002a0038ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000569a5665ULL, 0x000000005665a965ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 1, 0, 2), (1, 2, 3, 0), (2, 0, 1, 3))
//[8, 5, 14, 3, 0, 9, 6, 15, 4, 13, 2, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_140(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900003c3cULL, 0x0000c3c300009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090c0c0c0cULL, 0x0303030306060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_140(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x06030603090c090cULL, 0x090c090c06030603ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c3693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 1, 0, 2), (2, 0, 1, 3), (1, 2, 3, 0))
//[4, 9, 14, 3, 8, 1, 6, 15, 12, 5, 2, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_141(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500003c3cULL, 0x0000c3c300005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050505050c0c0c0cULL, 0x030303030a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_141(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a030a03050c050cULL, 0x050c050c0a030a03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac3a53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 2, 1, 0), (1, 0, 3, 2), (2, 1, 0, 3))
//[8, 5, 14, 3, 4, 1, 10, 15, 0, 13, 6, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_142(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5553aaacccc5333aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_142(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005c00a300a3005cULL, 0x005c00a300a30035ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((0, 3, 2, 1), (3, 2, 1, 0), (2, 1, 0, 3), (1, 0, 3, 2))
//[4, 9, 14, 3, 0, 5, 10, 15, 12, 1, 6, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_143(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9993666cccc93336ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_143(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009c00630063009cULL, 0x009c006300630039ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 1, 3, 2), (2, 3, 0, 1), (3, 2, 1, 0))
//[12, 9, 2, 7, 8, 13, 6, 3, 4, 1, 14, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_144(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009692ULL, 0x00005a580000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200510051ULL, 0x0064006400980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_144(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090a0605090a0605ULL, 0x090a0605090a090aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 1, 3, 2), (2, 3, 1, 0), (3, 2, 0, 1))
//[12, 9, 2, 7, 8, 13, 6, 3, 0, 5, 14, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_145(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000003c30ULL, 0x0000c3c00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000c000c0ULL, 0x0030003000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_145(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300c300c3ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000003c3ULL, 0x0000fc3c0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000006996a55aULL, 0x000000006996a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0000003300ffULL, 0x00cc0000003300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 1, 3, 2), (3, 2, 0, 1), (2, 3, 1, 0))
//[8, 13, 2, 7, 12, 9, 6, 3, 4, 1, 14, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_146(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000003c30ULL, 0x0000c3c00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000c000c0ULL, 0x0030003000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_146(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300c300c3ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000003c3ULL, 0x0000fc3c0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a6996ULL, 0x00000000a55a6996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0000003300ffULL, 0x00cc0000003300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 1, 3, 2), (3, 2, 1, 0), (2, 3, 0, 1))
//[8, 13, 2, 7, 12, 9, 6, 3, 0, 5, 14, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_147(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a52ULL, 0x0000969400006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200910091ULL, 0x00a800a800540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_147(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05060a0905060a09ULL, 0x05060a0905060506ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 2, 3, 1), (2, 3, 1, 0), (3, 1, 0, 2))
//[12, 9, 2, 7, 4, 13, 10, 3, 0, 5, 14, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_148(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c396966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000009690ULL, 0x0000c3c000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000600060ULL, 0x0030003000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_148(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300410041ULL, 0x0028002800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000969ULL, 0x0000fc3c0000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000eb14a55aULL, 0x00000000eb14a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004e000000b100ffULL, 0x004e000000b100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 2, 3, 1), (3, 1, 0, 2), (2, 3, 1, 0))
//[8, 13, 2, 7, 12, 5, 10, 3, 4, 1, 14, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_149(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c35a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000005a50ULL, 0x0000c3c00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000a000a0ULL, 0x0030003000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_149(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300810081ULL, 0x0024002400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c3000005a5ULL, 0x0000fc3c0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000e7186996ULL, 0x00000000e7186996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008e0000007100ffULL, 0x008e0000007100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 3, 1, 2), (2, 1, 3, 0), (3, 2, 0, 1))
//[12, 9, 2, 7, 8, 5, 14, 3, 0, 13, 6, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_150(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca3555aaULL, 0x00000000a55acc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005300aa00acULL, 0x00cc00c30033003cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_150(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a3005c005c00a3ULL, 0x00a3005c005c00a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x35ca35ca35caca35ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (0, 3, 1, 2), (3, 2, 0, 1), (2, 1, 3, 0))
//[8, 13, 2, 7, 4, 9, 14, 3, 12, 1, 6, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_151(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c6399966ULL, 0x000000006996cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009900930066006cULL, 0x00cc00c30033003cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_151(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0063009c009c0063ULL, 0x0063009c009c0063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x39c639c639c6c639ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 1, 3, 0), (0, 3, 1, 2), (3, 2, 0, 1))
//[12, 1, 10, 7, 8, 13, 6, 3, 0, 5, 14, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_152(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac5333ccULL, 0x00000000c33caa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003500cc00caULL, 0x00aa00a50055005aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_152(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c5003a003a00c5ULL, 0x00c5003a003a00c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x53ac53ac53acac53ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 1, 3, 0), (3, 2, 0, 1), (0, 3, 1, 2))
//[0, 13, 10, 7, 12, 9, 6, 3, 4, 1, 14, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_153(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c9333ccULL, 0x00000000c33c6699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003900cc00c6ULL, 0x0066006900990096ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_153(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c90036003600c9ULL, 0x00c90036003600c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x936c936c936c6c93ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 0, 1), (0, 1, 3, 2), (3, 2, 1, 0))
//[12, 1, 10, 7, 8, 5, 14, 3, 4, 13, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_154(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009694ULL, 0x00003c380000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400310031ULL, 0x0062006200980098ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_154(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090c0603090c0603ULL, 0x090c0603090c090cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 0, 1), (3, 2, 1, 0), (0, 1, 3, 2))
//[0, 13, 10, 7, 4, 9, 14, 3, 12, 5, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_155(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a58ULL, 0x00003c340000c3c2ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800310031ULL, 0x00a200a200540054ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_155(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050c0a03050c0a03ULL, 0x050c0a03050c050cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 1, 0), (0, 1, 3, 2), (3, 2, 0, 1))
//[12, 1, 10, 7, 8, 5, 14, 3, 0, 13, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_156(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000005a50ULL, 0x0000a5a00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000a000a0ULL, 0x0050005000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_156(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500a500a5ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000005a5ULL, 0x0000fa5a0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000006996c33cULL, 0x000000006996c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0000005500ffULL, 0x00aa0000005500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 1, 0), (0, 2, 3, 1), (3, 1, 0, 2))
//[12, 1, 10, 7, 4, 9, 14, 3, 0, 13, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_157(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a596966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000009690ULL, 0x0000a5a000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000600060ULL, 0x0050005000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_157(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500210021ULL, 0x0048004800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000969ULL, 0x0000fa5a0000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ed12c33cULL, 0x00000000ed12c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002e000000d100ffULL, 0x002e000000d100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 1, 0), (3, 1, 0, 2), (0, 2, 3, 1))
//[0, 13, 10, 7, 8, 5, 14, 3, 12, 1, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_158(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x969669695a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000005a50ULL, 0x000069600000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000a000a0ULL, 0x0090009000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_158(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900210021ULL, 0x0084008400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000969000005a5ULL, 0x0000f6960000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ed12c33cULL, 0x00000000ed12c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002e000000d100ffULL, 0x002e000000d100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (2, 3, 1, 0), (3, 2, 0, 1), (0, 1, 3, 2))
//[0, 13, 10, 7, 4, 9, 14, 3, 12, 1, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_159(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000009690ULL, 0x0000696000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000600060ULL, 0x0090009000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_159(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900690069ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000969ULL, 0x0000f6960000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55ac33cULL, 0x00000000a55ac33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00660000009900ffULL, 0x00660000009900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 1, 0, 2), (0, 2, 3, 1), (2, 3, 1, 0))
//[8, 1, 14, 7, 12, 9, 6, 3, 4, 13, 2, 11, 0, 5, 10, 15]
void BOGI128_omega_diffusion_160(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a53c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000003c30ULL, 0x0000a5a00000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000c000c0ULL, 0x0050005000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_160(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500810081ULL, 0x0042004200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000003c3ULL, 0x0000fa5a0000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000e7186996ULL, 0x00000000e7186996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008e0000007100ffULL, 0x008e0000007100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 1, 0, 2), (2, 3, 1, 0), (0, 2, 3, 1))
//[0, 9, 14, 7, 8, 13, 6, 3, 12, 5, 2, 11, 4, 1, 10, 15]
void BOGI128_omega_diffusion_161(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x969669693c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000003c30ULL, 0x000069600000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000c000c0ULL, 0x0090009000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_161(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900410041ULL, 0x0082008200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000969000003c3ULL, 0x0000f6960000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000eb14a55aULL, 0x00000000eb14a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004e000000b100ffULL, 0x004e000000b100ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 0, 1), (0, 1, 3, 2), (2, 3, 1, 0))
//[8, 1, 14, 7, 12, 5, 10, 3, 4, 13, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_162(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000005a50ULL, 0x0000a5a00000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000a000a0ULL, 0x0050005000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_162(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500a500a5ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a5000005a5ULL, 0x0000fa5a0000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c6996ULL, 0x00000000c33c6996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0000005500ffULL, 0x00aa0000005500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 0, 1), (0, 3, 1, 2), (2, 1, 3, 0))
//[8, 1, 14, 7, 4, 13, 10, 3, 12, 5, 2, 11, 0, 9, 6, 15]
void BOGI128_omega_diffusion_163(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a6599966ULL, 0x000000006996aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009900950066006aULL, 0x00aa00a50055005aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_163(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0065009a009a0065ULL, 0x0065009a009a0065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x59a659a659a6a659ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 0, 1), (2, 1, 3, 0), (0, 3, 1, 2))
//[0, 9, 14, 7, 12, 5, 10, 3, 4, 13, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_164(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a9555aaULL, 0x00000000a55a6699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005900aa00a6ULL, 0x0066006900990096ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_164(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a90056005600a9ULL, 0x00a90056005600a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x956a956a956a6a95ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 0, 1), (2, 3, 1, 0), (0, 1, 3, 2))
//[0, 9, 14, 7, 4, 13, 10, 3, 12, 5, 2, 11, 8, 1, 6, 15]
void BOGI128_omega_diffusion_165(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000969000009690ULL, 0x0000696000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000600060ULL, 0x0090009000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_165(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900690069ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000969ULL, 0x0000f6960000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33ca55aULL, 0x00000000c33ca55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00660000009900ffULL, 0x00660000009900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 1, 0), (0, 1, 3, 2), (2, 3, 0, 1))
//[8, 1, 14, 7, 12, 5, 10, 3, 0, 13, 6, 11, 4, 9, 2, 15]
void BOGI128_omega_diffusion_166(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c34ULL, 0x0000969200006968ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400910091ULL, 0x00c800c800320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_166(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03060c0903060c09ULL, 0x03060c0903060306ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 2, 3), (3, 2, 1, 0), (2, 3, 0, 1), (0, 1, 3, 2))
//[0, 9, 14, 7, 4, 13, 10, 3, 12, 1, 6, 11, 8, 5, 2, 15]
void BOGI128_omega_diffusion_167(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c38ULL, 0x00005a520000a5a4ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800510051ULL, 0x00c400c400320032ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_167(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030a0c05030a0c05ULL, 0x030a0c05030a030aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 1, 2, 3), (2, 3, 0, 1), (3, 2, 1, 0))
//[12, 9, 2, 7, 8, 13, 6, 3, 4, 1, 10, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_168(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009692ULL, 0x0000696800009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200510051ULL, 0x0054005400a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_168(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 1, 2, 3), (2, 3, 1, 0), (3, 2, 0, 1))
//[12, 9, 2, 7, 8, 13, 6, 3, 0, 5, 10, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_169(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009692ULL, 0x0000a5a400005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200510051ULL, 0x0098009800640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_169(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05090a0605090a06ULL, 0x05090a0605090509ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 1, 2, 3), (3, 2, 0, 1), (2, 3, 1, 0))
//[8, 13, 2, 7, 12, 9, 6, 3, 4, 1, 10, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_170(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a52ULL, 0x0000696800009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200910091ULL, 0x0054005400a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_170(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0905060a0905060aULL, 0x0905060a09050905ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 1, 2, 3), (3, 2, 1, 0), (2, 3, 0, 1))
//[8, 13, 2, 7, 12, 9, 6, 3, 0, 5, 10, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_171(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc33cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a52ULL, 0x0000a5a400005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200910091ULL, 0x0098009800640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_171(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 2, 1, 3), (2, 3, 0, 1), (3, 1, 2, 0))
//[12, 9, 2, 7, 4, 13, 10, 3, 8, 1, 6, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_172(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100003c38ULL, 0x0000c3c200009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200510051ULL, 0x0054005400a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_172(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090b01ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 2, 1, 3), (3, 1, 2, 0), (2, 3, 0, 1))
//[8, 13, 2, 7, 12, 5, 10, 3, 0, 9, 6, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_173(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100003c34ULL, 0x0000c3c200005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200910091ULL, 0x0098009800640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_173(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050701ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 3, 2, 1), (2, 1, 0, 3), (3, 2, 1, 0))
//[12, 9, 2, 7, 8, 5, 14, 3, 4, 1, 10, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_174(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x333accc55553aaacULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_174(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003500ca00ca0035ULL, 0x003500ca00ca00a3ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (0, 3, 2, 1), (3, 2, 1, 0), (2, 1, 0, 3))
//[8, 13, 2, 7, 4, 9, 14, 3, 0, 5, 10, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_175(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3336ccc99993666cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_175(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003900c600c60039ULL, 0x003900c600c60063ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 1, 0, 3), (0, 3, 2, 1), (3, 2, 1, 0))
//[12, 1, 10, 7, 8, 13, 6, 3, 4, 9, 2, 15, 0, 5, 14, 11]
void BOGI128_omega_diffusion_176(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x555caaa33335cccaULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_176(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005300ac00ac0053ULL, 0x005300ac00ac00c5ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 1, 0, 3), (3, 2, 1, 0), (0, 3, 2, 1))
//[0, 13, 10, 7, 12, 9, 6, 3, 8, 5, 2, 15, 4, 1, 14, 11]
void BOGI128_omega_diffusion_177(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x999c66633339ccc6ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_177(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0093006c006c0093ULL, 0x0093006c006c00c9ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 0, 1), (0, 1, 2, 3), (3, 2, 1, 0))
//[12, 1, 10, 7, 8, 5, 14, 3, 4, 9, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_178(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009694ULL, 0x0000696800009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400310031ULL, 0x0032003200c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_178(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 0, 1), (0, 2, 1, 3), (3, 1, 2, 0))
//[12, 1, 10, 7, 4, 9, 14, 3, 8, 5, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_179(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100005a58ULL, 0x0000a5a400009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400310031ULL, 0x0032003200c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_179(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090d01ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 0, 1), (3, 1, 2, 0), (0, 2, 1, 3))
//[0, 13, 10, 7, 8, 5, 14, 3, 4, 9, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_180(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100009694ULL, 0x0000696800005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800310031ULL, 0x0032003200c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_180(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050d01ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 0, 1), (3, 2, 1, 0), (0, 1, 2, 3))
//[0, 13, 10, 7, 4, 9, 14, 3, 8, 5, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_181(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a58ULL, 0x0000a5a400005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800310031ULL, 0x0032003200c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_181(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 1, 0), (0, 1, 2, 3), (3, 2, 0, 1))
//[12, 1, 10, 7, 8, 5, 14, 3, 0, 9, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_182(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696100009694ULL, 0x0000c3c200003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400310031ULL, 0x0098009800620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_182(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03090c0603090c06ULL, 0x03090c0603090309ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (2, 3, 1, 0), (3, 2, 0, 1), (0, 1, 2, 3))
//[0, 13, 10, 7, 4, 9, 14, 3, 8, 1, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_183(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100005a58ULL, 0x0000c3c200003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800310031ULL, 0x0054005400a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_183(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03050c0a03050c0aULL, 0x03050c0a03050305ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 1, 2, 0), (0, 2, 1, 3), (2, 3, 0, 1))
//[8, 1, 14, 7, 12, 9, 6, 3, 0, 5, 10, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_184(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100005a52ULL, 0x0000a5a400003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400910091ULL, 0x0098009800620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_184(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030701ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 1, 2, 0), (2, 3, 0, 1), (0, 2, 1, 3))
//[0, 9, 14, 7, 8, 13, 6, 3, 4, 1, 10, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_185(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100009692ULL, 0x0000696800003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800510051ULL, 0x0054005400a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_185(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030b01ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 0, 1), (0, 1, 2, 3), (2, 3, 1, 0))
//[8, 1, 14, 7, 12, 5, 10, 3, 4, 9, 2, 15, 0, 13, 6, 11]
void BOGI128_omega_diffusion_186(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c34ULL, 0x0000696800009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400910091ULL, 0x0032003200c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_186(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0903060c0903060cULL, 0x0903060c09030903ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 0, 1), (2, 3, 1, 0), (0, 1, 2, 3))
//[0, 9, 14, 7, 4, 13, 10, 3, 8, 5, 2, 15, 12, 1, 6, 11]
void BOGI128_omega_diffusion_187(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c38ULL, 0x0000a5a400005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800510051ULL, 0x0032003200c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_187(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05030a0c05030a0cULL, 0x05030a0c05030503ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 1, 0), (0, 1, 2, 3), (2, 3, 0, 1))
//[8, 1, 14, 7, 12, 5, 10, 3, 0, 9, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_188(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa55aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c34ULL, 0x0000c3c200003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400910091ULL, 0x0098009800620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_188(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 1, 0), (0, 3, 2, 1), (2, 1, 0, 3))
//[8, 1, 14, 7, 4, 13, 10, 3, 0, 9, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_189(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5556aaa99995666aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_189(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005900a600a60059ULL, 0x005900a600a60065ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 1, 0), (2, 1, 0, 3), (0, 3, 2, 1))
//[0, 9, 14, 7, 12, 5, 10, 3, 8, 1, 6, 15, 4, 13, 2, 11]
void BOGI128_omega_diffusion_190(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x999a66655559aaa6ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_190(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0095006a006a0095ULL, 0x0095006a006a00a9ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 0, 3, 2), (3, 2, 1, 0), (2, 3, 0, 1), (0, 1, 2, 3))
//[0, 9, 14, 7, 4, 13, 10, 3, 8, 1, 6, 15, 12, 5, 2, 11]
void BOGI128_omega_diffusion_191(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966996696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100003c38ULL, 0x0000c3c200003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800510051ULL, 0x0054005400a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_191(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 1, 3, 2), (2, 3, 1, 0), (3, 0, 2, 1))
//[12, 9, 2, 7, 0, 13, 6, 11, 8, 5, 14, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_192(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3aa55ULL, 0x00000000a35ccc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a50055005aULL, 0x00cc00c50033003aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_192(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003500ca00ca0035ULL, 0x003500ca00ca0035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa35ca35ca35c5ca3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 1, 3, 2), (3, 0, 2, 1), (2, 3, 1, 0))
//[8, 13, 2, 7, 12, 1, 6, 11, 4, 9, 14, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_193(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc36699ULL, 0x00000000639ccc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006900990096ULL, 0x00cc00c900330036ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_193(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003900c600c60039ULL, 0x003900c600c60039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x639c639c639c9c63ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 1, 2), (2, 0, 3, 1), (3, 1, 2, 0))
//[12, 9, 2, 7, 4, 1, 14, 11, 8, 13, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_194(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000a5a5ULL, 0x0000696900005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060605050505ULL, 0x090909090a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_194(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0605090a0605090aULL, 0x0605090a0605090aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 1, 2), (2, 1, 3, 0), (3, 0, 2, 1))
//[12, 9, 2, 7, 0, 5, 14, 11, 8, 13, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_195(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_195(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc0033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55a5aa5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 1, 2), (3, 0, 2, 1), (2, 1, 3, 0))
//[8, 13, 2, 7, 4, 1, 14, 11, 12, 9, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_196(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_196(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc0033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669969669ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 1, 2), (3, 1, 2, 0), (2, 0, 3, 1))
//[8, 13, 2, 7, 0, 5, 14, 11, 12, 9, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_197(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00006969ULL, 0x0000a5a500009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a09090909ULL, 0x0505050506060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_197(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0905060a090506ULL, 0x0a0905060a090506ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 2, 1), (2, 1, 3, 0), (3, 0, 1, 2))
//[12, 9, 2, 7, 0, 5, 14, 11, 4, 13, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_198(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_198(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008100ee002400bbULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003d320000c7c8ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7c83fe01d32cfb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00d9005b0076005eULL, 0x002600a4008900a1ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c396cc6ULL, 0x000000006cc693c6ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (0, 3, 2, 1), (3, 0, 1, 2), (2, 1, 3, 0))
//[8, 13, 2, 7, 4, 1, 14, 11, 12, 5, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_199(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_199(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004100ee00280077ULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003d320000cbc4ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xbc43fe01d32cf708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00d5009700ba009eULL, 0x002a006800450061ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac35accaULL, 0x00000000acca53caULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 0, 3, 1), (0, 3, 1, 2), (3, 1, 2, 0))
//[12, 1, 10, 7, 4, 13, 2, 11, 8, 5, 14, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_200(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000c3c3ULL, 0x0000696900003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060603030303ULL, 0x090909090c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_200(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0603090c0603090cULL, 0x0603090c0603090cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 0, 3, 1), (3, 1, 2, 0), (0, 3, 1, 2))
//[0, 13, 10, 7, 12, 5, 2, 11, 4, 9, 14, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_201(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a0000c3c3ULL, 0x0000a5a500003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a03030303ULL, 0x050505050c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_201(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a03050c0a03050cULL, 0x0a03050c0a03050cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 1, 3, 0), (0, 3, 1, 2), (3, 0, 2, 1))
//[12, 1, 10, 7, 0, 13, 6, 11, 8, 5, 14, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_202(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_202(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa0055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33c3cc3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 1, 3, 0), (0, 3, 2, 1), (3, 0, 1, 2))
//[12, 1, 10, 7, 0, 13, 6, 11, 4, 9, 14, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_203(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_203(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008100ee004200ddULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005b540000a7a8ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7a85fe01b54afd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00b9003d0076003eULL, 0x004600c2008900c1ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a596aa6ULL, 0x000000006aa695a6ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 1, 3, 0), (3, 0, 1, 2), (0, 3, 2, 1))
//[0, 13, 10, 7, 12, 1, 6, 11, 8, 5, 14, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_204(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_204(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004100ee008200ddULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000979800006b64ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xb649fe017986fd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0075003d00ba003eULL, 0x008a00c2004500c1ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a695a66aULL, 0x00000000a66a596aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 1, 3, 0), (3, 0, 2, 1), (0, 3, 1, 2))
//[0, 13, 10, 7, 12, 1, 6, 11, 4, 9, 14, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_205(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_205(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33c3cc3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 3, 1, 0), (0, 1, 3, 2), (3, 0, 2, 1))
//[12, 1, 10, 7, 0, 5, 14, 11, 8, 13, 6, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_206(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5cc33ULL, 0x00000000c53aaa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c30033003cULL, 0x00aa00a30055005cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_206(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005300ac00ac0053ULL, 0x005300ac00ac0053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc53ac53ac53a3ac5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (2, 3, 1, 0), (3, 0, 2, 1), (0, 1, 3, 2))
//[0, 13, 10, 7, 4, 1, 14, 11, 12, 9, 6, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_207(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009669cc33ULL, 0x00000000c9366699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c30033003cULL, 0x006600630099009cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_207(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0093006c006c0093ULL, 0x0093006c006c0093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc936c936c93636c9ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 1, 2), (0, 3, 2, 1), (2, 1, 3, 0))
//[8, 1, 14, 7, 4, 13, 2, 11, 12, 9, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_208(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_208(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002100ee00480077ULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005b540000ada2ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xda25fe01b54af708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00b3009700dc009eULL, 0x004c006800230061ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca53caacULL, 0x00000000caac35acULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 1, 2), (2, 1, 3, 0), (0, 3, 2, 1))
//[0, 9, 14, 7, 12, 5, 2, 11, 8, 13, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_209(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_209(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002100ee008400bbULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000979800006d62ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xd629fe017986fb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0073005b00dc005eULL, 0x008c00a4002300a1ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c693c66cULL, 0x00000000c66c396cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 2, 1), (0, 1, 3, 2), (2, 3, 1, 0))
//[8, 1, 14, 7, 12, 5, 2, 11, 4, 13, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_210(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa56699ULL, 0x00000000659aaa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006900990096ULL, 0x00aa00a900550056ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_210(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005900a600a60059ULL, 0x005900a600a60059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x659a659a659a9a65ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 2, 1), (0, 3, 1, 2), (2, 1, 3, 0))
//[8, 1, 14, 7, 4, 13, 2, 11, 12, 5, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_211(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_211(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa0055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669969669ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 2, 1), (2, 1, 3, 0), (0, 3, 1, 2))
//[0, 9, 14, 7, 12, 5, 2, 11, 4, 13, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_212(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_212(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x0099006600660099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55a5aa5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 0, 2, 1), (2, 3, 1, 0), (0, 1, 3, 2))
//[0, 9, 14, 7, 4, 13, 2, 11, 12, 5, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_213(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009669aa55ULL, 0x00000000a9566699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a50055005aULL, 0x006600650099009aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_213(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0095006a006a0095ULL, 0x0095006a006a0095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa956a956a95656a9ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 1, 2, 0), (0, 3, 1, 2), (2, 0, 3, 1))
//[8, 1, 14, 7, 0, 13, 6, 11, 12, 5, 10, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_214(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00006969ULL, 0x0000c3c300009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c09090909ULL, 0x0303030306060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_214(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0903060c090306ULL, 0x0c0903060c090306ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 0, 3), (3, 1, 2, 0), (2, 0, 3, 1), (0, 3, 1, 2))
//[0, 9, 14, 7, 12, 1, 6, 11, 4, 13, 10, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_215(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c0000a5a5ULL, 0x0000c3c300005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c05050505ULL, 0x030303030a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_215(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c05030a0c05030aULL, 0x0c05030a0c05030aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 1, 2, 3), (2, 3, 0, 1), (3, 0, 1, 2))
//[12, 9, 2, 7, 0, 13, 6, 11, 4, 1, 10, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_216(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x333accc5aaac5553ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_216(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a3005c005c00a3ULL, 0x00a3005c005c0035ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 1, 2, 3), (3, 0, 1, 2), (2, 3, 0, 1))
//[8, 13, 2, 7, 12, 1, 6, 11, 0, 5, 10, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_217(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3336ccc9666c9993ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_217(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0063009c009c0063ULL, 0x0063009c009c0039ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 1, 2), (2, 1, 0, 3), (3, 0, 2, 1))
//[12, 9, 2, 7, 0, 5, 14, 11, 8, 1, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_218(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_218(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004200dd00180077ULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003e310000cbc4ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcb34df203ec17f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x006e007a009b00daULL, 0x0091008500640025ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c963c99cULL, 0x00000000c99c369cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 1, 2), (3, 0, 2, 1), (2, 1, 0, 3))
//[8, 13, 2, 7, 4, 1, 14, 11, 0, 9, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_219(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_219(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008200dd001400bbULL, 0x00cf0000003f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00003e310000c7c8ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc738df203ec1bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ae00b6005700d6ULL, 0x0051004900a80029ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c5a3c55cULL, 0x00000000c55c3a5cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 2, 1), (2, 0, 1, 3), (3, 1, 0, 2))
//[12, 9, 2, 7, 4, 1, 14, 11, 0, 5, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_220(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000a5a5ULL, 0x00005a5a00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060605050505ULL, 0x0a0a0a0a09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_220(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x090a090a06050605ULL, 0x06050605090a090aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695a96a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 2, 1), (2, 1, 0, 3), (3, 0, 1, 2))
//[12, 9, 2, 7, 0, 5, 14, 11, 4, 1, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_221(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_221(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a050a0a050a05ULL, 0x0a050a05050a050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55a5aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 2, 1), (3, 0, 1, 2), (2, 1, 0, 3))
//[8, 13, 2, 7, 4, 1, 14, 11, 0, 5, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_222(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_222(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906090606090609ULL, 0x0609060909060906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699696696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (0, 3, 2, 1), (3, 1, 0, 2), (2, 0, 1, 3))
//[8, 13, 2, 7, 0, 5, 14, 11, 4, 1, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_223(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00006969ULL, 0x000096960000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a09090909ULL, 0x0606060605050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_223(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050605060a090a09ULL, 0x0a090a0905060506ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a5965a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 0, 1, 3), (0, 3, 2, 1), (3, 1, 0, 2))
//[12, 1, 10, 7, 4, 13, 2, 11, 0, 9, 6, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_224(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000c3c3ULL, 0x00003c3c00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060603030303ULL, 0x0c0c0c0c09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_224(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x090c090c06030603ULL, 0x06030603090c090cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693c96c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 0, 1, 3), (3, 1, 0, 2), (0, 3, 2, 1))
//[0, 13, 10, 7, 12, 5, 2, 11, 8, 1, 6, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_225(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a0000c3c3ULL, 0x00003c3c0000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a03030303ULL, 0x0c0c0c0c05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_225(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050c050c0a030a03ULL, 0x0a030a03050c050cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53c5ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 1, 0, 3), (0, 3, 1, 2), (3, 0, 2, 1))
//[12, 1, 10, 7, 0, 13, 6, 11, 8, 5, 2, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_226(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_226(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002400bb00180077ULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005e510000ada2ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xad52bf405ea17f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x006e007c009d00bcULL, 0x0091008300620043ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a965a99aULL, 0x00000000a99a569aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 1, 0, 3), (0, 3, 2, 1), (3, 0, 1, 2))
//[12, 1, 10, 7, 0, 13, 6, 11, 4, 9, 2, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_227(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_227(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c030c0c030c03ULL, 0x0c030c03030c030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33c3cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 1, 0, 3), (3, 0, 1, 2), (0, 3, 2, 1))
//[0, 13, 10, 7, 12, 1, 6, 11, 8, 5, 2, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_228(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_228(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c030c0c030c03ULL, 0x0c030c03030c030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33c3cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 1, 0, 3), (3, 0, 2, 1), (0, 3, 1, 2))
//[0, 13, 10, 7, 12, 1, 6, 11, 4, 9, 2, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_229(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_229(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00280077001400bbULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00009e9100006d62ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6d927f809e61bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ae00bc005d007cULL, 0x0051004300a20083ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000065a96556ULL, 0x0000000065569a56ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 3, 0, 1), (0, 1, 2, 3), (3, 0, 1, 2))
//[12, 1, 10, 7, 0, 5, 14, 11, 4, 9, 2, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_230(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x555caaa3ccca3335ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_230(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c5003a003a00c5ULL, 0x00c5003a003a0053ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (2, 3, 0, 1), (3, 0, 1, 2), (0, 1, 2, 3))
//[0, 13, 10, 7, 4, 1, 14, 11, 8, 5, 2, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_231(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x999c6663ccc63339ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_231(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c90036003600c9ULL, 0x00c9003600360093ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 1, 2), (0, 1, 2, 3), (2, 3, 0, 1))
//[8, 1, 14, 7, 12, 5, 2, 11, 0, 9, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_232(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5556aaa9666a9995ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_232(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0065009a009a0065ULL, 0x0065009a009a0059ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 1, 2), (0, 3, 2, 1), (2, 1, 0, 3))
//[8, 1, 14, 7, 4, 13, 2, 11, 0, 9, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_233(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_233(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906090606090609ULL, 0x0609060909060906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699696696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 1, 2), (2, 1, 0, 3), (0, 3, 2, 1))
//[0, 9, 14, 7, 12, 5, 2, 11, 8, 1, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_234(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_234(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a050a0a050a05ULL, 0x0a050a05050a050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55a5aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 1, 2), (2, 3, 0, 1), (0, 1, 2, 3))
//[0, 9, 14, 7, 4, 13, 2, 11, 8, 1, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_235(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x999a6665aaa65559ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_235(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a90056005600a9ULL, 0x00a9005600560095ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 2, 1), (0, 3, 1, 2), (2, 1, 0, 3))
//[8, 1, 14, 7, 4, 13, 2, 11, 0, 5, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_236(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_236(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008400bb001200ddULL, 0x00af0000005f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00005e510000a7a8ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa758bf405ea1df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ce00d6003700b6ULL, 0x0031002900c80049ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a3c5a33aULL, 0x00000000a33a5c3aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 0, 2, 1), (2, 1, 0, 3), (0, 3, 1, 2))
//[0, 9, 14, 7, 12, 5, 2, 11, 4, 1, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_237(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_237(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00480077001200ddULL, 0x006f0000009f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00009e9100006b64ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6b947f809e61df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ce00da003b007aULL, 0x0031002500c40085ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000063c96336ULL, 0x0000000063369c36ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 1, 0, 2), (0, 3, 2, 1), (2, 0, 1, 3))
//[8, 1, 14, 7, 0, 13, 6, 11, 4, 9, 2, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_238(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00006969ULL, 0x000096960000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c09090909ULL, 0x0606060603030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_238(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030603060c090c09ULL, 0x0c090c0903060306ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c3963c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 2, 3, 0), (3, 1, 0, 2), (2, 0, 1, 3), (0, 3, 2, 1))
//[0, 9, 14, 7, 12, 1, 6, 11, 8, 5, 2, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_239(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c0000a5a5ULL, 0x00005a5a0000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c05050505ULL, 0x0a0a0a0a03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_239(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030a030a0c050c05ULL, 0x0c050c05030a030aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35a3ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 1, 2, 3), (2, 0, 3, 1), (3, 2, 1, 0))
//[12, 9, 2, 7, 8, 1, 6, 15, 4, 13, 10, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_240(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069610000c3c2ULL, 0x00003c3800009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200540054ULL, 0x0051005100a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_240(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x090906060909080dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 1, 2, 3), (3, 2, 1, 0), (2, 0, 3, 1))
//[8, 13, 2, 7, 0, 9, 6, 15, 12, 5, 10, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_241(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a10000c3c2ULL, 0x00003c3400005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200980098ULL, 0x0091009100640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010102020202ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_241(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003300cc00cc0033ULL, 0x003300cc00cc00acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a0505040dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 1, 3), (2, 0, 3, 1), (3, 1, 2, 0))
//[12, 9, 2, 7, 4, 1, 10, 15, 8, 13, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_242(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003ac53ac5ULL, 0x0000000035ca35caULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_242(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 1, 3), (2, 1, 3, 0), (3, 0, 2, 1))
//[12, 9, 2, 7, 0, 5, 10, 15, 8, 13, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_243(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600005a5aULL, 0x000069690000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060a0a0a0aULL, 0x0909090905050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_243(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0605090a060509ULL, 0x0a0605090a060509ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 1, 3), (3, 0, 2, 1), (2, 1, 3, 0))
//[8, 13, 2, 7, 4, 1, 10, 15, 12, 9, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_244(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00009696ULL, 0x0000a5a500006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a06060606ULL, 0x0505050509090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_244(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060a0905060a0905ULL, 0x060a0905060a0905ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 1, 3), (3, 1, 2, 0), (2, 0, 3, 1))
//[8, 13, 2, 7, 0, 5, 10, 15, 12, 9, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_245(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000036c936c9ULL, 0x0000000039c639c6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_245(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 3, 1), (2, 0, 1, 3), (3, 1, 2, 0))
//[12, 9, 2, 7, 4, 1, 10, 15, 8, 5, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_246(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003ac53ac5ULL, 0x0000000053ac53acULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_246(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0053003500ac00caULL, 0x0053003500ac00caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (0, 2, 3, 1), (3, 1, 2, 0), (2, 0, 1, 3))
//[8, 13, 2, 7, 0, 5, 10, 15, 4, 9, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_247(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000036c936c9ULL, 0x00000000936c936cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_247(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00930039006c00c6ULL, 0x00930039006c00c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 1, 3), (0, 2, 3, 1), (3, 1, 2, 0))
//[12, 1, 10, 7, 4, 9, 2, 15, 8, 13, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_248(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005ca35ca3ULL, 0x0000000035ca35caULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_248(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0035005300ca00acULL, 0x0035005300ca00acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 1, 3), (3, 1, 2, 0), (0, 2, 3, 1))
//[0, 13, 10, 7, 8, 5, 2, 15, 12, 9, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_249(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c639c63ULL, 0x0000000039c639c6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_249(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0039009300c6006cULL, 0x0039009300c6006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 3, 1), (0, 1, 2, 3), (3, 2, 1, 0))
//[12, 1, 10, 7, 8, 5, 2, 15, 4, 9, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_250(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069610000a5a4ULL, 0x00005a5800009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400320032ULL, 0x0031003100c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_250(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x090906060909080bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 3, 1), (0, 2, 1, 3), (3, 1, 2, 0))
//[12, 1, 10, 7, 4, 9, 2, 15, 8, 5, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_251(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005ca35ca3ULL, 0x0000000053ac53acULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_251(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 3, 1), (3, 1, 2, 0), (0, 2, 1, 3))
//[0, 13, 10, 7, 8, 5, 2, 15, 4, 9, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_252(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c639c63ULL, 0x00000000936c936cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_252(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 0, 3, 1), (3, 2, 1, 0), (0, 1, 2, 3))
//[0, 13, 10, 7, 4, 9, 2, 15, 8, 5, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_253(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a100006968ULL, 0x0000969400005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800320032ULL, 0x0031003100c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_253(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x00990066006600a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050407ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 1, 3, 0), (0, 2, 1, 3), (3, 0, 2, 1))
//[12, 1, 10, 7, 0, 9, 6, 15, 8, 5, 14, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_254(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600003c3cULL, 0x000069690000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060c0c0c0cULL, 0x0909090903030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_254(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0603090c060309ULL, 0x0c0603090c060309ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (2, 1, 3, 0), (3, 0, 2, 1), (0, 2, 1, 3))
//[0, 13, 10, 7, 8, 1, 6, 15, 4, 9, 14, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_255(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00003c3cULL, 0x0000a5a50000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a0c0c0c0cULL, 0x0505050503030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_255(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a03050c0a0305ULL, 0x0c0a03050c0a0305ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 0, 2, 1), (0, 2, 1, 3), (2, 1, 3, 0))
//[8, 1, 14, 7, 4, 9, 2, 15, 12, 5, 10, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_256(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00009696ULL, 0x0000c3c300006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c06060606ULL, 0x0303030309090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_256(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060c0903060c0903ULL, 0x060c0903060c0903ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 0, 2, 1), (2, 1, 3, 0), (0, 2, 1, 3))
//[0, 9, 14, 7, 8, 5, 2, 15, 4, 13, 10, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_257(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00005a5aULL, 0x0000c3c30000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c0a0a0a0aULL, 0x0303030305050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_257(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c05030a0c0503ULL, 0x0a0c05030a0c0503ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 1, 2, 0), (0, 2, 1, 3), (2, 0, 3, 1))
//[8, 1, 14, 7, 0, 9, 6, 15, 12, 5, 10, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_258(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000056a956a9ULL, 0x0000000059a659a6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_258(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 1, 2, 0), (0, 2, 3, 1), (2, 0, 1, 3))
//[8, 1, 14, 7, 0, 9, 6, 15, 4, 13, 10, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_259(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000056a956a9ULL, 0x00000000956a956aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_259(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00950059006a00a6ULL, 0x00950059006a00a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 1, 2, 0), (2, 0, 1, 3), (0, 2, 3, 1))
//[0, 9, 14, 7, 8, 1, 6, 15, 12, 5, 10, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_260(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a659a65ULL, 0x0000000059a659a6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_260(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0059009500a6006aULL, 0x0059009500a6006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 1, 2, 0), (2, 0, 3, 1), (0, 2, 1, 3))
//[0, 9, 14, 7, 8, 1, 6, 15, 4, 13, 10, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_261(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a659a65ULL, 0x00000000956a956aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_261(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 2, 1, 0), (0, 1, 2, 3), (2, 0, 3, 1))
//[8, 1, 14, 7, 0, 5, 10, 15, 12, 9, 6, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_262(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c10000a5a4ULL, 0x00005a5200003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400980098ULL, 0x0091009100620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010104040404ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_262(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005500aa00aa0055ULL, 0x005500aa00aa00caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c0303020bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 0, 2), (3, 2, 1, 0), (2, 0, 3, 1), (0, 1, 2, 3))
//[0, 9, 14, 7, 4, 1, 10, 15, 8, 13, 6, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_263(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c100006968ULL, 0x0000969200003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800540054ULL, 0x0051005100a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0101010108080808ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_263(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099006600660099ULL, 0x00990066006600c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030207ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 1, 3, 2), (2, 0, 1, 3), (3, 2, 0, 1))
//[12, 9, 2, 7, 8, 1, 6, 15, 0, 5, 14, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_264(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c369699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3000006960ULL, 0x0000c3c000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000900090ULL, 0x0030003000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_264(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300820082ULL, 0x0014001400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc0006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000696ULL, 0x0000fc3c0000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007d82a55aULL, 0x000000007d82a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d80000002700ffULL, 0x00d80000002700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 1, 3, 2), (3, 2, 0, 1), (2, 0, 1, 3))
//[8, 13, 2, 7, 0, 9, 6, 15, 4, 1, 14, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_265(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c3a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00003c300000a5a0ULL, 0x0000c3c000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c000c000500050ULL, 0x0030003000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_265(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300420042ULL, 0x0018001800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000a5aULL, 0x0000fc3c0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000bd426996ULL, 0x00000000bd426996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d40000002b00ffULL, 0x00d40000002b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 1, 3), (2, 0, 3, 1), (3, 1, 0, 2))
//[12, 9, 2, 7, 4, 1, 10, 15, 0, 13, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_266(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003ac53ac5ULL, 0x00000000ac53ac53ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_266(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a3003a005c00c5ULL, 0x00a3003a005c00c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 1, 3), (3, 1, 0, 2), (2, 0, 3, 1))
//[8, 13, 2, 7, 0, 5, 10, 15, 12, 1, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_267(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000036c936c9ULL, 0x000000006c936c93ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_267(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00630036009c00c9ULL, 0x00630036009c00c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 3, 1), (2, 0, 1, 3), (3, 1, 0, 2))
//[12, 9, 2, 7, 4, 1, 10, 15, 0, 5, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_268(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003ac53ac5ULL, 0x00000000ca35ca35ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_268(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300000000ULL, 0x003c003c00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000c3cULL, 0x0000fc3c0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00a55aULL, 0x00000000ff00a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x005a000000a500ffULL, 0x005a000000a500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 3, 1), (2, 1, 0, 3), (3, 0, 1, 2))
//[12, 9, 2, 7, 0, 5, 10, 15, 4, 1, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_269(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600005a5aULL, 0x0000a5a500006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060a0a0a0aULL, 0x0505050509090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_269(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050905090a060a06ULL, 0x0a060a0605090509ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a5695a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 3, 1), (3, 0, 1, 2), (2, 1, 0, 3))
//[8, 13, 2, 7, 4, 1, 10, 15, 0, 5, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_270(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00009696ULL, 0x000069690000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a06060606ULL, 0x0909090905050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_270(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0033003300cc00ccULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x09050905060a060aULL, 0x060a060a09050905ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a5965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (0, 2, 3, 1), (3, 1, 0, 2), (2, 0, 1, 3))
//[8, 13, 2, 7, 0, 5, 10, 15, 4, 1, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_271(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000036c936c9ULL, 0x00000000c639c639ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_271(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c300c300000000ULL, 0x003c003c00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3fffc000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000003c300000c3cULL, 0x0000fc3c0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff006996ULL, 0x00000000ff006996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00960000006900ffULL, 0x00960000006900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 1, 3), (0, 1, 3, 2), (3, 2, 0, 1))
//[12, 1, 10, 7, 8, 5, 2, 15, 0, 13, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_272(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a569699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5000006960ULL, 0x0000a5a000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000900090ULL, 0x0050005000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_272(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500840084ULL, 0x0012001200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa0006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000696ULL, 0x0000fa5a0000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007b84c33cULL, 0x000000007b84c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b80000004700ffULL, 0x00b80000004700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 1, 3), (0, 2, 3, 1), (3, 1, 0, 2))
//[12, 1, 10, 7, 4, 9, 2, 15, 0, 13, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_273(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005ca35ca3ULL, 0x00000000ac53ac53ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_273(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500000000ULL, 0x005a005a00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000a5aULL, 0x0000fa5a0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00c33cULL, 0x00000000ff00c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x003c000000c300ffULL, 0x003c000000c300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 1, 3), (3, 1, 0, 2), (0, 2, 3, 1))
//[0, 13, 10, 7, 8, 5, 2, 15, 12, 1, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_274(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c639c63ULL, 0x000000006c936c93ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_274(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900000000ULL, 0x0096009600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000696ULL, 0x0000f6960000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00c33cULL, 0x00000000ff00c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x003c000000c300ffULL, 0x003c000000c300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 1, 3), (3, 2, 0, 1), (0, 1, 3, 2))
//[0, 13, 10, 7, 4, 9, 2, 15, 12, 1, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_275(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96966969a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000096900000a5a0ULL, 0x0000696000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000500050ULL, 0x0090009000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_275(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900480048ULL, 0x0012001200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff6000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000a5aULL, 0x0000f6960000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000b748c33cULL, 0x00000000b748c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00740000008b00ffULL, 0x00740000008b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 3, 1), (0, 2, 1, 3), (3, 1, 0, 2))
//[12, 1, 10, 7, 4, 9, 2, 15, 0, 5, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_276(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005ca35ca3ULL, 0x00000000ca35ca35ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_276(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c5005c003a00a3ULL, 0x00c5005c003a00a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 0, 3, 1), (3, 1, 0, 2), (0, 2, 1, 3))
//[0, 13, 10, 7, 8, 5, 2, 15, 4, 1, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_277(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c639c63ULL, 0x00000000c639c639ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_277(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c9009c00360063ULL, 0x00c9009c00360063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 1, 0, 3), (0, 2, 3, 1), (3, 0, 1, 2))
//[12, 1, 10, 7, 0, 9, 6, 15, 4, 13, 2, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_278(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600003c3cULL, 0x0000c3c300006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060c0c0c0cULL, 0x0303030309090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_278(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030903090c060c06ULL, 0x0c060c0603090309ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c3693c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (2, 1, 0, 3), (3, 0, 1, 2), (0, 2, 3, 1))
//[0, 13, 10, 7, 8, 1, 6, 15, 12, 5, 2, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_279(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00003c3cULL, 0x0000c3c30000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a0c0c0c0cULL, 0x0303030305050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_279(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030503050c0a0c0aULL, 0x0c0a0c0a03050305ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a53c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 0, 1, 2), (0, 2, 3, 1), (2, 1, 0, 3))
//[8, 1, 14, 7, 4, 9, 2, 15, 0, 13, 6, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_280(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00009696ULL, 0x000069690000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c06060606ULL, 0x0909090903030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_280(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0055005500aa00aaULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x09030903060c060cULL, 0x060c060c09030903ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c3963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 0, 1, 2), (2, 1, 0, 3), (0, 2, 3, 1))
//[0, 9, 14, 7, 8, 5, 2, 15, 12, 1, 6, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_281(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00005a5aULL, 0x0000a5a50000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c0a0a0a0aULL, 0x0505050503030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_281(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0099009900660066ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050305030a0c0a0cULL, 0x0a0c0a0c05030503ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c35a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 1, 0, 2), (0, 2, 1, 3), (2, 0, 3, 1))
//[8, 1, 14, 7, 0, 9, 6, 15, 12, 5, 2, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_282(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000056a956a9ULL, 0x000000006a956a95ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_282(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00650056009a00a9ULL, 0x00650056009a00a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 1, 0, 2), (0, 2, 3, 1), (2, 0, 1, 3))
//[8, 1, 14, 7, 0, 9, 6, 15, 4, 13, 2, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_283(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000056a956a9ULL, 0x00000000a659a659ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_283(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500000000ULL, 0x005a005a00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000a5aULL, 0x0000fa5a0000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff006996ULL, 0x00000000ff006996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00960000006900ffULL, 0x00960000006900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 1, 0, 2), (2, 0, 1, 3), (0, 2, 3, 1))
//[0, 9, 14, 7, 8, 1, 6, 15, 12, 5, 2, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_284(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a659a65ULL, 0x000000006a956a95ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_284(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900000000ULL, 0x0096009600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff60006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000696ULL, 0x0000f6960000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00a55aULL, 0x00000000ff00a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x005a000000a500ffULL, 0x005a000000a500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 1, 0, 2), (2, 0, 3, 1), (0, 2, 1, 3))
//[0, 9, 14, 7, 8, 1, 6, 15, 4, 13, 2, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_285(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a659a65ULL, 0x00000000a659a659ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_285(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a9009a00560065ULL, 0x00a9009a00560065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 2, 0, 1), (0, 1, 3, 2), (2, 0, 1, 3))
//[8, 1, 14, 7, 0, 5, 10, 15, 4, 13, 2, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_286(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a5c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00005a500000c3c0ULL, 0x0000a5a000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a000a000300030ULL, 0x0050005000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_286(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a500a500240024ULL, 0x0018001800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5fffa000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000005a500000c3cULL, 0x0000fa5a0000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000db246996ULL, 0x00000000db246996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b20000004d00ffULL, 0x00b20000004d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((1, 3, 2, 0), (3, 2, 0, 1), (2, 0, 1, 3), (0, 1, 3, 2))
//[0, 9, 14, 7, 4, 1, 10, 15, 12, 5, 2, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_287(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96966969c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000096900000c3c0ULL, 0x0000696000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0060006000300030ULL, 0x0090009000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_287(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0069006900280028ULL, 0x0014001400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9fff6000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000096900000c3cULL, 0x0000f6960000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000d728a55aULL, 0x00000000d728a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00720000008d00ffULL, 0x00720000008d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 1, 3, 2), (1, 3, 2, 0), (3, 2, 0, 1))
//[12, 5, 2, 11, 8, 13, 6, 3, 0, 9, 14, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_288(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x696996963c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000003c30ULL, 0x000096900000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000c000c0ULL, 0x0060006000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_288(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600820082ULL, 0x0041004100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000696000003c3ULL, 0x0000f9690000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007d82a55aULL, 0x000000007d82a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d80000002700ffULL, 0x00d80000002700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 1, 3, 2), (3, 2, 0, 1), (1, 3, 2, 0))
//[4, 13, 2, 11, 12, 9, 6, 3, 8, 1, 14, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_289(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a3c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000003c30ULL, 0x00005a500000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000c000c0ULL, 0x00a000a000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_289(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00420042ULL, 0x0081008100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000003c3ULL, 0x0000f5a50000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000bd426996ULL, 0x00000000bd426996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d40000002b00ffULL, 0x00d40000002b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 2, 3, 1), (1, 3, 0, 2), (3, 1, 2, 0))
//[12, 5, 2, 11, 4, 13, 10, 3, 8, 1, 14, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_290(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a956a95ULL, 0x0000000056a956a9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_290(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0056006500a9009aULL, 0x0056006500a9009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 2, 3, 1), (1, 3, 2, 0), (3, 1, 0, 2))
//[12, 5, 2, 11, 4, 13, 10, 3, 0, 9, 14, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_291(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a956a95ULL, 0x000000009a659a65ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_291(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600000000ULL, 0x0069006900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000969ULL, 0x0000f9690000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00a55aULL, 0x00000000ff00a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x005a000000a500ffULL, 0x005a000000a500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 2, 3, 1), (3, 1, 0, 2), (1, 3, 2, 0))
//[4, 13, 2, 11, 12, 5, 10, 3, 8, 1, 14, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_292(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a659a659ULL, 0x0000000056a956a9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_292(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00000000ULL, 0x00a500a500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000005a5ULL, 0x0000f5a50000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff006996ULL, 0x00000000ff006996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00960000006900ffULL, 0x00960000006900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 2, 3, 1), (3, 1, 2, 0), (1, 3, 0, 2))
//[4, 13, 2, 11, 12, 5, 10, 3, 0, 9, 14, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_293(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a659a659ULL, 0x000000009a659a65ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_293(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009a00a900650056ULL, 0x009a00a900650056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 3, 2, 1), (1, 2, 3, 0), (3, 1, 0, 2))
//[12, 5, 2, 11, 4, 9, 14, 3, 0, 13, 10, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_294(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000a5a5ULL, 0x00005a5a00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030305050505ULL, 0x0a0a0a0a0c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_294(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a0c0a03050305ULL, 0x030503050c0a0c0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5ac3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (0, 3, 2, 1), (3, 1, 0, 2), (1, 2, 3, 0))
//[4, 13, 2, 11, 8, 5, 14, 3, 12, 1, 10, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_295(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300006969ULL, 0x0000969600003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030309090909ULL, 0x060606060c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_295(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c060c0603090309ULL, 0x030903090c060c06ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c96c3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 2, 3, 0), (0, 3, 2, 1), (3, 1, 0, 2))
//[12, 1, 6, 11, 4, 13, 10, 3, 0, 9, 14, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_296(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000c3c3ULL, 0x00003c3c00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050503030303ULL, 0x0c0c0c0c0a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_296(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c0a0c05030503ULL, 0x050305030a0c0a0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3ca5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 2, 3, 0), (3, 1, 0, 2), (0, 3, 2, 1))
//[0, 13, 6, 11, 12, 5, 10, 3, 8, 1, 14, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_297(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000c3c3ULL, 0x00003c3c00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090903030303ULL, 0x0c0c0c0c06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_297(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x060c060c09030903ULL, 0x09030903060c060cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963c69c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 0, 2), (0, 2, 3, 1), (3, 1, 2, 0))
//[12, 1, 6, 11, 4, 9, 14, 3, 8, 13, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_298(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c936c93ULL, 0x0000000036c936c9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_298(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0036006300c9009cULL, 0x0036006300c9009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 0, 2), (3, 1, 2, 0), (0, 2, 3, 1))
//[0, 13, 6, 11, 8, 5, 14, 3, 12, 9, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_299(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac53ac53ULL, 0x000000003ac53ac5ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_299(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003a00a300c5005cULL, 0x003a00a300c5005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 2, 0), (0, 1, 3, 2), (3, 2, 0, 1))
//[12, 1, 6, 11, 8, 5, 14, 3, 0, 13, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_300(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x696996965a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000005a50ULL, 0x000096900000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000a000a0ULL, 0x0060006000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_300(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600840084ULL, 0x0021002100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000696000005a5ULL, 0x0000f9690000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007b84c33cULL, 0x000000007b84c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b80000004700ffULL, 0x00b80000004700ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 2, 0), (0, 2, 3, 1), (3, 1, 0, 2))
//[12, 1, 6, 11, 4, 9, 14, 3, 0, 13, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_301(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c936c93ULL, 0x000000009c639c63ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_301(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600000000ULL, 0x0069006900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000969ULL, 0x0000f9690000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00c33cULL, 0x00000000ff00c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x003c000000c300ffULL, 0x003c000000c300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 2, 0), (3, 1, 0, 2), (0, 2, 3, 1))
//[0, 13, 6, 11, 8, 5, 14, 3, 12, 1, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_302(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac53ac53ULL, 0x000000005ca35ca3ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_302(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00000000ULL, 0x00a500a500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000005a5ULL, 0x0000f5a50000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00c33cULL, 0x00000000ff00c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x003c000000c300ffULL, 0x003c000000c300ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (1, 3, 2, 0), (3, 2, 0, 1), (0, 1, 3, 2))
//[0, 13, 6, 11, 4, 9, 14, 3, 12, 1, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_303(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a96966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000009690ULL, 0x00005a5000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000600060ULL, 0x00a000a000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_303(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00480048ULL, 0x0021002100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000969ULL, 0x0000f5a50000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000b748c33cULL, 0x00000000b748c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00740000008b00ffULL, 0x00740000008b00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 0, 2), (0, 2, 3, 1), (1, 3, 2, 0))
//[4, 1, 14, 11, 12, 9, 6, 3, 8, 13, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_304(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c639c639ULL, 0x0000000036c936c9ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_304(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00000000ULL, 0x00c300c300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000003c3ULL, 0x0000f3c30000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff006996ULL, 0x00000000ff006996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00960000006900ffULL, 0x00960000006900ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 0, 2), (0, 3, 2, 1), (1, 2, 3, 0))
//[4, 1, 14, 11, 8, 13, 6, 3, 12, 9, 2, 7, 0, 5, 10, 15]
void BOGI128_omega_diffusion_305(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500006969ULL, 0x0000969600005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050509090909ULL, 0x060606060a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_305(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a060a0605090509ULL, 0x050905090a060a06ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a96a5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 0, 2), (1, 2, 3, 0), (0, 3, 2, 1))
//[0, 5, 14, 11, 12, 9, 6, 3, 8, 13, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_306(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000a5a5ULL, 0x00005a5a00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090905050505ULL, 0x0a0a0a0a06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_306(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x060a060a09050905ULL, 0x09050905060a060aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965a69a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 0, 2), (1, 3, 2, 0), (0, 2, 3, 1))
//[0, 5, 14, 11, 8, 13, 6, 3, 12, 9, 2, 7, 4, 1, 10, 15]
void BOGI128_omega_diffusion_307(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca35ca35ULL, 0x000000003ac53ac5ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_307(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00000000ULL, 0x00c300c300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000003c3ULL, 0x0000f3c30000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00a55aULL, 0x00000000ff00a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x005a000000a500ffULL, 0x005a000000a500ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 2, 0), (0, 2, 3, 1), (1, 3, 0, 2))
//[4, 1, 14, 11, 12, 9, 6, 3, 0, 13, 10, 7, 8, 5, 2, 15]
void BOGI128_omega_diffusion_308(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c639c639ULL, 0x000000009c639c63ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_308(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009c00c900630036ULL, 0x009c00c900630036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 1, 2, 0), (1, 3, 0, 2), (0, 2, 3, 1))
//[0, 5, 14, 11, 8, 13, 6, 3, 12, 1, 10, 7, 4, 9, 2, 15]
void BOGI128_omega_diffusion_309(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca35ca35ULL, 0x000000005ca35ca3ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_309(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005c00c500a3003aULL, 0x005c00c500a3003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 2, 0, 1), (0, 1, 3, 2), (1, 3, 2, 0))
//[4, 1, 14, 11, 12, 5, 10, 3, 8, 13, 2, 7, 0, 9, 6, 15]
void BOGI128_omega_diffusion_310(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c5a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000005a50ULL, 0x00003c300000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000a000a0ULL, 0x00c000c000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_310(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00240024ULL, 0x0081008100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000005a5ULL, 0x0000f3c30000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000db246996ULL, 0x00000000db246996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b20000004d00ffULL, 0x00b20000004d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 1, 3), (3, 2, 0, 1), (1, 3, 2, 0), (0, 1, 3, 2))
//[0, 5, 14, 11, 4, 13, 10, 3, 12, 9, 2, 7, 8, 1, 6, 15]
void BOGI128_omega_diffusion_311(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c96966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000009690ULL, 0x00003c3000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000600060ULL, 0x00c000c000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_311(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00280028ULL, 0x0041004100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000969ULL, 0x0000f3c30000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000d728a55aULL, 0x00000000d728a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00720000008d00ffULL, 0x00720000008d00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 1, 2, 3), (1, 3, 0, 2), (3, 2, 1, 0))
//[12, 5, 2, 11, 8, 13, 6, 3, 4, 1, 10, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_312(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3400009692ULL, 0x000069680000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200510051ULL, 0x0054005400a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_312(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c080dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 1, 2, 3), (3, 2, 1, 0), (1, 3, 0, 2))
//[4, 13, 2, 11, 12, 9, 6, 3, 0, 5, 10, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_313(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3800005a52ULL, 0x0000a5a40000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200910091ULL, 0x0098009800640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_313(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c040dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 2, 1, 3), (1, 3, 0, 2), (3, 1, 2, 0))
//[12, 5, 2, 11, 4, 13, 10, 3, 8, 1, 6, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_314(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a956a95ULL, 0x00000000659a659aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_314(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 2, 1, 3), (1, 3, 2, 0), (3, 1, 0, 2))
//[12, 5, 2, 11, 4, 13, 10, 3, 0, 9, 6, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_315(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006a956a95ULL, 0x00000000a956a956ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_315(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a6006a00590095ULL, 0x00a6006a00590095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 2, 1, 3), (3, 1, 0, 2), (1, 3, 2, 0))
//[4, 13, 2, 11, 12, 5, 10, 3, 8, 1, 6, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_316(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a659a659ULL, 0x00000000659a659aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_316(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006a00a600950059ULL, 0x006a00a600950059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 2, 1, 3), (3, 1, 2, 0), (1, 3, 0, 2))
//[4, 13, 2, 11, 12, 5, 10, 3, 0, 9, 6, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_317(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3cc3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a659a659ULL, 0x00000000a956a956ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_317(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 3, 1, 2), (1, 2, 0, 3), (3, 1, 2, 0))
//[12, 5, 2, 11, 4, 9, 14, 3, 8, 1, 6, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_318(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000a5a5ULL, 0x00003c3c00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030305050505ULL, 0x0c0c0c0c0a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_318(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03050c0a03050c0aULL, 0x03050c0a03050c0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (0, 3, 1, 2), (3, 1, 2, 0), (1, 2, 0, 3))
//[4, 13, 2, 11, 8, 5, 14, 3, 0, 9, 6, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_319(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300006969ULL, 0x00003c3c00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030309090909ULL, 0x0c0c0c0c06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_319(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03090c0603090c06ULL, 0x03090c0603090c06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 2, 0, 3), (0, 3, 1, 2), (3, 1, 2, 0))
//[12, 1, 6, 11, 4, 13, 10, 3, 8, 5, 2, 15, 0, 9, 14, 7]
void BOGI128_omega_diffusion_320(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000c3c3ULL, 0x00005a5a00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050503030303ULL, 0x0a0a0a0a0c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_320(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05030a0c05030a0cULL, 0x05030a0c05030a0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 2, 0, 3), (3, 1, 2, 0), (0, 3, 1, 2))
//[0, 13, 6, 11, 12, 5, 10, 3, 4, 9, 2, 15, 8, 1, 14, 7]
void BOGI128_omega_diffusion_321(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000c3c3ULL, 0x0000969600003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090903030303ULL, 0x060606060c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_321(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0903060c0903060cULL, 0x0903060c0903060cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 0, 2), (0, 1, 2, 3), (3, 2, 1, 0))
//[12, 1, 6, 11, 8, 5, 14, 3, 4, 9, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_322(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5200009694ULL, 0x000069680000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400310031ULL, 0x0032003200c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_322(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a080bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 0, 2), (0, 2, 1, 3), (3, 1, 2, 0))
//[12, 1, 6, 11, 4, 9, 14, 3, 8, 5, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_323(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c936c93ULL, 0x00000000639c639cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_323(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 0, 2), (3, 1, 2, 0), (0, 2, 1, 3))
//[0, 13, 6, 11, 8, 5, 14, 3, 4, 9, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_324(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac53ac53ULL, 0x00000000a35ca35cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_324(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 0, 2), (3, 2, 1, 0), (0, 1, 2, 3))
//[0, 13, 6, 11, 4, 9, 14, 3, 8, 5, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_325(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200005a58ULL, 0x0000a5a400006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800310031ULL, 0x0032003200c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_325(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060407ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 2, 0), (0, 2, 1, 3), (3, 1, 0, 2))
//[12, 1, 6, 11, 4, 9, 14, 3, 0, 5, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_326(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006c936c93ULL, 0x00000000c936c936ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_326(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c6006c00390093ULL, 0x00c6006c00390093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (1, 3, 2, 0), (3, 1, 0, 2), (0, 2, 1, 3))
//[0, 13, 6, 11, 8, 5, 14, 3, 4, 1, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_327(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ac53ac53ULL, 0x00000000c53ac53aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_327(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ca00ac00350053ULL, 0x00ca00ac00350053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 0, 2), (0, 2, 1, 3), (1, 3, 2, 0))
//[4, 1, 14, 11, 12, 9, 6, 3, 8, 5, 2, 15, 0, 13, 10, 7]
void BOGI128_omega_diffusion_328(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c639c639ULL, 0x00000000639c639cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_328(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006c00c600930039ULL, 0x006c00c600930039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 0, 2), (1, 3, 2, 0), (0, 2, 1, 3))
//[0, 5, 14, 11, 8, 13, 6, 3, 4, 9, 2, 15, 12, 1, 10, 7]
void BOGI128_omega_diffusion_329(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca35ca35ULL, 0x00000000a35ca35cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_329(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ac00ca00530035ULL, 0x00ac00ca00530035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 2, 0), (0, 2, 1, 3), (1, 3, 0, 2))
//[4, 1, 14, 11, 12, 9, 6, 3, 0, 5, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_330(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5aa5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c639c639ULL, 0x00000000c936c936ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_330(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 2, 0), (0, 3, 1, 2), (1, 2, 0, 3))
//[4, 1, 14, 11, 8, 13, 6, 3, 0, 5, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_331(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500006969ULL, 0x00005a5a00009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050509090909ULL, 0x0a0a0a0a06060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_331(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05090a0605090a06ULL, 0x05090a0605090a06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 2, 0), (1, 2, 0, 3), (0, 3, 1, 2))
//[0, 5, 14, 11, 12, 9, 6, 3, 4, 1, 10, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_332(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069690000a5a5ULL, 0x0000969600005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909090905050505ULL, 0x060606060a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_332(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0905060a0905060aULL, 0x0905060a0905060aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 1, 2, 0), (1, 3, 0, 2), (0, 2, 1, 3))
//[0, 5, 14, 11, 8, 13, 6, 3, 4, 1, 10, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_333(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ca35ca35ULL, 0x00000000c53ac53aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_333(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 2, 1, 0), (0, 1, 2, 3), (1, 3, 0, 2))
//[4, 1, 14, 11, 12, 5, 10, 3, 0, 9, 6, 15, 8, 13, 2, 7]
void BOGI128_omega_diffusion_334(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5800003c34ULL, 0x0000c3c20000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400910091ULL, 0x0098009800620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_334(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a020bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 0, 3, 1), (3, 2, 1, 0), (1, 3, 0, 2), (0, 1, 2, 3))
//[0, 5, 14, 11, 4, 13, 10, 3, 8, 1, 6, 15, 12, 9, 2, 7]
void BOGI128_omega_diffusion_335(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400003c38ULL, 0x0000c3c200006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800510051ULL, 0x0054005400a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_335(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060207ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 2, 3, 1), (1, 3, 2, 0), (3, 0, 1, 2))
//[12, 5, 2, 11, 0, 13, 10, 7, 4, 9, 14, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_336(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300005a5aULL, 0x0000a5a500003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030303030a0a0a0aULL, 0x050505050c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_336(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050c050c0a030a03ULL, 0x0a030a03050c050cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53c5ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 2, 3, 1), (3, 0, 1, 2), (1, 3, 2, 0))
//[4, 13, 2, 11, 12, 1, 10, 7, 8, 5, 14, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_337(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300009696ULL, 0x0000696900003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030306060606ULL, 0x090909090c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_337(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x090c090c06030603ULL, 0x06030603090c090cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693c96c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 1, 2), (1, 2, 3, 0), (3, 0, 2, 1))
//[12, 5, 2, 11, 0, 9, 14, 7, 8, 13, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_338(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_338(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001200dd00480077ULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006b6400009e91ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9e61df206b947f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003b007a00ce00daULL, 0x00c4008500310025ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c369cc9ULL, 0x000000009cc963c9ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 1, 2), (3, 0, 2, 1), (1, 2, 3, 0))
//[4, 13, 2, 11, 8, 1, 14, 7, 12, 9, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_339(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_339(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001200dd008400bbULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000a7a800005e51ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ea1df20a758bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003700b600ce00d6ULL, 0x00c8004900310029ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005c3a5cc5ULL, 0x000000005cc5a3c5ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 2, 1), (1, 0, 3, 2), (3, 2, 1, 0))
//[12, 5, 2, 11, 8, 1, 14, 7, 4, 13, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_340(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x666a99955556aaa9ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_340(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0065009a009a0065ULL, 0x0065009a009a00a6ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 2, 1), (1, 2, 3, 0), (3, 0, 1, 2))
//[12, 5, 2, 11, 0, 9, 14, 7, 4, 13, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_341(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_341(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a050a0a050a05ULL, 0x0a050a05050a050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55a5aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 2, 1), (3, 0, 1, 2), (1, 2, 3, 0))
//[4, 13, 2, 11, 8, 1, 14, 7, 12, 5, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_342(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_342(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906090606090609ULL, 0x0609060909060906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699696696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (0, 3, 2, 1), (3, 2, 1, 0), (1, 0, 3, 2))
//[4, 13, 2, 11, 0, 9, 14, 7, 12, 5, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_343(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa65559999a6665ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_343(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a90056005600a9ULL, 0x00a900560056006aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 0, 3, 2), (0, 3, 2, 1), (3, 2, 1, 0))
//[12, 1, 6, 11, 8, 13, 2, 7, 4, 9, 14, 3, 0, 5, 10, 15]
//GIFT128
void BOGI128_omega_diffusion_344(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x666c99933336ccc9ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//Inverse GIFT128
void BOGI128_omega_inv_diffusion_344(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0063009c009c0063ULL, 0x0063009c009c00c6ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 0, 3, 2), (3, 2, 1, 0), (0, 3, 2, 1))
//[0, 13, 6, 11, 12, 9, 2, 7, 8, 5, 14, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_345(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaac5553333accc5ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_345(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a3005c005c00a3ULL, 0x00a3005c005c00caULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 2, 3, 0), (0, 3, 1, 2), (3, 0, 2, 1))
//[12, 1, 6, 11, 0, 13, 10, 7, 8, 5, 14, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_346(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_346(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001400bb00280077ULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006d6200009e91ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9e61bf406d927f80ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005d007c00ae00bcULL, 0x00a2008300510043ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a569aa9ULL, 0x000000009aa965a9ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 2, 3, 0), (0, 3, 2, 1), (3, 0, 1, 2))
//[12, 1, 6, 11, 0, 13, 10, 7, 4, 9, 14, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_347(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_347(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c030c0c030c03ULL, 0x0c030c03030c030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33c3cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 2, 3, 0), (3, 0, 1, 2), (0, 3, 2, 1))
//[0, 13, 6, 11, 12, 1, 10, 7, 8, 5, 14, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_348(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_348(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c030c0c030c03ULL, 0x0c030c03030c030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33c3cc3c33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 2, 3, 0), (3, 0, 2, 1), (0, 3, 1, 2))
//[0, 13, 6, 11, 12, 1, 10, 7, 4, 9, 14, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_349(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_349(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00180077002400bbULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ada200005e51ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ea17f80ad52bf40ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x009d00bc006e007cULL, 0x0062004300910083ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000569a5665ULL, 0x000000005665a965ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 3, 2, 0), (0, 2, 3, 1), (3, 0, 1, 2))
//[12, 1, 6, 11, 0, 9, 14, 7, 4, 13, 10, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_350(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500003c3cULL, 0x0000c3c300005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050505050c0c0c0cULL, 0x030303030a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_350(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030a030a0c050c05ULL, 0x0c050c05030a030aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35a3ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (1, 3, 2, 0), (3, 0, 1, 2), (0, 2, 3, 1))
//[0, 13, 6, 11, 8, 1, 14, 7, 12, 5, 10, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_351(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900003c3cULL, 0x0000c3c300009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090c0c0c0cULL, 0x0303030306060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_351(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030603060c090c09ULL, 0x0c090c0903060306ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c3963c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 1, 2), (0, 2, 3, 1), (1, 3, 2, 0))
//[4, 1, 14, 11, 12, 9, 2, 7, 8, 13, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_352(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500009696ULL, 0x0000696900005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050506060606ULL, 0x090909090a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_352(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x090a090a06050605ULL, 0x06050605090a090aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695a96a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 1, 2), (0, 3, 2, 1), (1, 2, 3, 0))
//[4, 1, 14, 11, 8, 13, 2, 7, 12, 9, 6, 3, 0, 5, 10, 15]
void BOGI128_omega_diffusion_353(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_353(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906090606090609ULL, 0x0609060909060906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699696696996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 1, 2), (1, 2, 3, 0), (0, 3, 2, 1))
//[0, 5, 14, 11, 12, 9, 2, 7, 8, 13, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_354(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_354(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a050a0a050a05ULL, 0x0a050a05050a050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55a5aa5a55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 1, 2), (1, 3, 2, 0), (0, 2, 3, 1))
//[0, 5, 14, 11, 8, 13, 2, 7, 12, 9, 6, 3, 4, 1, 10, 15]
void BOGI128_omega_diffusion_355(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900005a5aULL, 0x0000a5a500009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090a0a0a0aULL, 0x0505050506060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_355(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050605060a090a09ULL, 0x0a090a0905060506ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a5965a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 2, 1), (0, 3, 1, 2), (1, 2, 3, 0))
//[4, 1, 14, 11, 8, 13, 2, 7, 12, 5, 10, 3, 0, 9, 6, 15]
void BOGI128_omega_diffusion_356(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_356(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001400bb008200ddULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000c7c800003e31ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ec1bf40c738df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005700d600ae00b6ULL, 0x00a8002900510049ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003a5c3aa3ULL, 0x000000003aa3c5a3ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 0, 2, 1), (1, 2, 3, 0), (0, 3, 1, 2))
//[0, 5, 14, 11, 12, 9, 2, 7, 4, 13, 10, 3, 8, 1, 6, 15]
void BOGI128_omega_diffusion_357(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_357(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00180077004200ddULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000cbc400003e31ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ec17f80cb34df20ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x009b00da006e007aULL, 0x0064002500910085ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000369c3663ULL, 0x000000003663c963ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 2, 1, 0), (0, 3, 2, 1), (1, 0, 3, 2))
//[4, 1, 14, 11, 0, 13, 10, 7, 12, 9, 6, 3, 8, 5, 2, 15]
void BOGI128_omega_diffusion_358(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc63339999c6663ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_358(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c90036003600c9ULL, 0x00c900360036006cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 0, 3), (3, 2, 1, 0), (1, 0, 3, 2), (0, 3, 2, 1))
//[0, 5, 14, 11, 12, 1, 10, 7, 8, 13, 6, 3, 4, 9, 2, 15]
void BOGI128_omega_diffusion_359(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccca3335555caaa3ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_359(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c5003a003a00c5ULL, 0x00c5003a003a00acULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 2, 1, 3), (1, 3, 0, 2), (3, 0, 2, 1))
//[12, 5, 2, 11, 0, 13, 10, 7, 8, 1, 6, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_360(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca55ac3a53cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300005a5aULL, 0x00003c3c0000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030303030a0a0a0aULL, 0x0c0c0c0c05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_360(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a03050c0a03050cULL, 0x0a03050c0a03050cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 2, 1, 3), (3, 0, 2, 1), (1, 3, 0, 2))
//[4, 13, 2, 11, 12, 1, 10, 7, 0, 9, 6, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_361(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c6996c3693cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c300009696ULL, 0x00003c3c00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0303030306060606ULL, 0x0c0c0c0c09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_361(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0603090c0603090cULL, 0x0603090c0603090cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 1, 2), (1, 0, 2, 3), (3, 2, 0, 1))
//[12, 5, 2, 11, 8, 1, 14, 7, 0, 9, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_362(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009a6555aaULL, 0x00000000a55a9966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005600aa00a9ULL, 0x0099009600660069ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_362(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a60059005900a6ULL, 0x00a60059005900a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x659a659a659a9a65ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 1, 2), (1, 2, 0, 3), (3, 0, 2, 1))
//[12, 5, 2, 11, 0, 9, 14, 7, 8, 1, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_363(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_363(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55a5aa5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 1, 2), (3, 0, 2, 1), (1, 2, 0, 3))
//[4, 13, 2, 11, 8, 1, 14, 7, 0, 9, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_364(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_364(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669969669ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 1, 2), (3, 2, 0, 1), (1, 0, 2, 3))
//[4, 13, 2, 11, 0, 9, 14, 7, 8, 1, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_365(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000056a99966ULL, 0x00000000699655aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009a00660065ULL, 0x0055005a00aa00a5ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_365(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006a00950095006aULL, 0x006a00950095006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa956a956a95656a9ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 2, 1), (1, 2, 0, 3), (3, 0, 1, 2))
//[12, 5, 2, 11, 0, 9, 14, 7, 4, 1, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_366(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_366(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008400bb002100eeULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006d6200009798ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7986fb04d629fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00dc005e0073005bULL, 0x002300a1008c00a4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000396c3993ULL, 0x000000003993c693ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (0, 3, 2, 1), (3, 0, 1, 2), (1, 2, 0, 3))
//[4, 13, 2, 11, 8, 1, 14, 7, 0, 5, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_367(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_367(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00480077002100eeULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ada200005b54ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xb54af708da25fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00dc009e00b30097ULL, 0x00230061004c0068ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ac3553ULL, 0x000000003553ca53ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 0, 2, 3), (0, 3, 1, 2), (3, 2, 0, 1))
//[12, 1, 6, 11, 8, 13, 2, 7, 0, 5, 10, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_368(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000009c6333ccULL, 0x00000000c33c9966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003600cc00c9ULL, 0x0099009600660069ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_368(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c60039003900c6ULL, 0x00c60039003900c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x639c639c639c9c63ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 0, 2, 3), (3, 2, 0, 1), (0, 3, 1, 2))
//[0, 13, 6, 11, 12, 9, 2, 7, 4, 1, 10, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_369(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000005ca333ccULL, 0x00000000c33c55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003a00cc00c5ULL, 0x0055005a00aa00a5ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_369(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ca0035003500caULL, 0x00ca0035003500caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa35ca35ca35c5ca3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 2, 0, 3), (0, 3, 1, 2), (3, 0, 2, 1))
//[12, 1, 6, 11, 0, 13, 10, 7, 8, 5, 2, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_370(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_370(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33c3cc3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 2, 0, 3), (0, 3, 2, 1), (3, 0, 1, 2))
//[12, 1, 6, 11, 0, 13, 10, 7, 4, 9, 2, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_371(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_371(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008200dd004100eeULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006b6400009798ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x7986fd02b649fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ba003e0075003dULL, 0x004500c1008a00c2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000596a5995ULL, 0x000000005995a695ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 2, 0, 3), (3, 0, 1, 2), (0, 3, 2, 1))
//[0, 13, 6, 11, 12, 1, 10, 7, 8, 5, 2, 15, 4, 9, 14, 3]
void BOGI128_omega_diffusion_372(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_372(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004200dd008100eeULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000a7a800005b54ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xb54afd027a85fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0076003e00b9003dULL, 0x008900c1004600c2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000095a69559ULL, 0x0000000095596a59ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 2, 0, 3), (3, 0, 2, 1), (0, 3, 1, 2))
//[0, 13, 6, 11, 12, 1, 10, 7, 4, 9, 2, 15, 8, 5, 14, 3]
void BOGI128_omega_diffusion_373(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33cc33cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00c3003c003c00c3ULL, 0x003c00c300c3003cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_373(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33cc33cc33c3cc3ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 3, 0, 2), (0, 2, 1, 3), (3, 0, 2, 1))
//[12, 1, 6, 11, 0, 9, 14, 7, 8, 5, 2, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_374(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac33ca5c35aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500003c3cULL, 0x00005a5a0000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050505050c0c0c0cULL, 0x0a0a0a0a03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_374(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c05030a0c05030aULL, 0x0c05030a0c05030aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (1, 3, 0, 2), (3, 0, 2, 1), (0, 2, 1, 3))
//[0, 13, 6, 11, 8, 1, 14, 7, 4, 9, 2, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_375(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c33c69c396ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900003c3cULL, 0x000096960000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090c0c0c0cULL, 0x0606060603030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_375(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0903060c090306ULL, 0x0c0903060c090306ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 1, 2), (0, 3, 2, 1), (1, 2, 0, 3))
//[4, 1, 14, 11, 8, 13, 2, 7, 0, 9, 6, 15, 12, 5, 10, 3]
void BOGI128_omega_diffusion_376(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_376(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00280077004100eeULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000cbc400003d32ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xd32cf708bc43fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ba009e00d50097ULL, 0x00450061002a0068ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ca5335ULL, 0x000000005335ac35ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 1, 2), (1, 2, 0, 3), (0, 3, 2, 1))
//[0, 5, 14, 11, 12, 9, 2, 7, 8, 1, 6, 15, 4, 13, 10, 3]
void BOGI128_omega_diffusion_377(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_377(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002400bb008100eeULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000c7c800003d32ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xd32cfb047c83fe01ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0076005e00d9005bULL, 0x008900a1002600a4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000093c69339ULL, 0x0000000093396c39ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 2, 1), (0, 2, 1, 3), (1, 3, 0, 2))
//[4, 1, 14, 11, 12, 9, 2, 7, 0, 5, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_378(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a6996a5695aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a500009696ULL, 0x00005a5a00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0505050506060606ULL, 0x0a0a0a0a09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_378(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0605090a0605090aULL, 0x0605090a0605090aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 2, 1), (0, 3, 1, 2), (1, 2, 0, 3))
//[4, 1, 14, 11, 8, 13, 2, 7, 0, 5, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_379(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669966996ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0069009600960069ULL, 0x0096006900690096ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_379(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996699669969669ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 2, 1), (1, 2, 0, 3), (0, 3, 1, 2))
//[0, 5, 14, 11, 12, 9, 2, 7, 4, 1, 10, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_380(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55aa55aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00a5005a005a00a5ULL, 0x005a00a500a5005aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_380(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55aa55aa55a5aa5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 0, 2, 1), (1, 3, 0, 2), (0, 2, 1, 3))
//[0, 5, 14, 11, 8, 13, 2, 7, 4, 1, 10, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_381(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a55a69a596ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900005a5aULL, 0x000096960000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090909090a0a0a0aULL, 0x0606060605050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_381(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0905060a090506ULL, 0x0a0905060a090506ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 2, 0, 1), (0, 3, 1, 2), (1, 0, 2, 3))
//[4, 1, 14, 11, 0, 13, 10, 7, 8, 5, 2, 15, 12, 9, 6, 3]
void BOGI128_omega_diffusion_382(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000036c99966ULL, 0x00000000699633ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009c00660063ULL, 0x0033003c00cc00c3ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_382(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006c00930093006cULL, 0x006c00930093006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc936c936c93636c9ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 1, 3, 0), (3, 2, 0, 1), (1, 0, 2, 3), (0, 3, 1, 2))
//[0, 5, 14, 11, 12, 1, 10, 7, 4, 9, 2, 15, 8, 13, 6, 3]
void BOGI128_omega_diffusion_383(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000003ac555aaULL, 0x00000000a55a33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005c00aa00a3ULL, 0x0033003c00cc00c3ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_383(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ac0053005300acULL, 0x00ac0053005300acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc53ac53ac53a3ac5ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 2, 3), (1, 0, 3, 2), (3, 2, 1, 0))
//[12, 5, 2, 11, 8, 1, 6, 15, 4, 13, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_384(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c2ULL, 0x00003c380000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200540054ULL, 0x0051005100a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_384(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 2, 3), (1, 2, 3, 0), (3, 0, 1, 2))
//[12, 5, 2, 11, 0, 9, 6, 15, 4, 13, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_385(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x666a9995aaa95556ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_385(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a60059005900a6ULL, 0x00a6005900590065ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 2, 3), (3, 0, 1, 2), (1, 2, 3, 0))
//[4, 13, 2, 11, 8, 1, 6, 15, 12, 5, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_386(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000fcf00000f3fULL, 0x0000f0300000f0c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000c0033000300ccULL, 0x0033000c00cc0003ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0f0f03030f0fULL, 0x0f0f0c0c0f0f0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa655596665999aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_386(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006a00950095006aULL, 0x006a0095009500a9ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 2, 3), (3, 2, 1, 0), (1, 0, 3, 2))
//[4, 13, 2, 11, 0, 9, 6, 15, 12, 5, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_387(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c2ULL, 0x00003c340000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200980098ULL, 0x0091009100640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_387(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 3, 2), (1, 0, 2, 3), (3, 2, 1, 0))
//[12, 5, 2, 11, 8, 1, 6, 15, 4, 9, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_388(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c2ULL, 0x00005a580000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200540054ULL, 0x0031003100c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_388(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a03050c0a0305ULL, 0x0c0a03050c0a0c0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 1, 3, 2), (3, 2, 1, 0), (1, 0, 2, 3))
//[4, 13, 2, 11, 0, 9, 6, 15, 8, 5, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_389(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c2ULL, 0x0000969400006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200980098ULL, 0x0031003100c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_389(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0603090c060309ULL, 0x0c0603090c060c06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 2, 1, 3), (1, 0, 3, 2), (3, 1, 2, 0))
//[12, 5, 2, 11, 4, 1, 10, 15, 8, 13, 6, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_390(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3400006968ULL, 0x000096920000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200540054ULL, 0x0051005100a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_390(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x00660099009900c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0e04ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (0, 2, 1, 3), (3, 1, 2, 0), (1, 0, 3, 2))
//[4, 13, 2, 11, 0, 5, 10, 15, 12, 9, 6, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_391(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000a5a4ULL, 0x00005a520000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200980098ULL, 0x0091009100640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_391(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0e08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 2, 3), (0, 1, 3, 2), (3, 2, 1, 0))
//[12, 1, 6, 11, 8, 5, 2, 15, 4, 13, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_392(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a4ULL, 0x00003c380000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400320032ULL, 0x0051005100a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_392(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c05030a0c0503ULL, 0x0a0c05030a0c0a0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 2, 3), (3, 2, 1, 0), (0, 1, 3, 2))
//[0, 13, 6, 11, 4, 9, 2, 15, 12, 5, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_393(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006968ULL, 0x00003c340000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800320032ULL, 0x0091009100640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_393(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060c0903060c0903ULL, 0x060c0903060c060cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 3, 2), (0, 1, 2, 3), (3, 2, 1, 0))
//[12, 1, 6, 11, 8, 5, 2, 15, 4, 9, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_394(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a4ULL, 0x00005a580000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400320032ULL, 0x0031003100c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_394(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 3, 2), (0, 2, 1, 3), (3, 1, 2, 0))
//[12, 1, 6, 11, 4, 9, 2, 15, 8, 5, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_395(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5200006968ULL, 0x000096940000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400320032ULL, 0x0031003100c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_395(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x00660099009900a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0e02ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 3, 2), (3, 1, 2, 0), (0, 2, 1, 3))
//[0, 13, 6, 11, 8, 5, 2, 15, 4, 9, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_396(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096920000a5a4ULL, 0x00005a5800006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800320032ULL, 0x0031003100c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_396(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060e02ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 0, 3, 2), (3, 2, 1, 0), (0, 1, 2, 3))
//[0, 13, 6, 11, 4, 9, 2, 15, 8, 5, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_397(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006968ULL, 0x0000969400006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800320032ULL, 0x0031003100c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_397(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 2, 3, 0), (0, 1, 2, 3), (3, 0, 1, 2))
//[12, 1, 6, 11, 0, 5, 10, 15, 4, 9, 14, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_398(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x666c9993ccc93336ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_398(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c60039003900c6ULL, 0x00c6003900390063ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (1, 2, 3, 0), (3, 0, 1, 2), (0, 1, 2, 3))
//[0, 13, 6, 11, 4, 1, 10, 15, 8, 5, 14, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_399(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaac5553ccc5333aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_399(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ca0035003500caULL, 0x00ca0035003500a3ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 0, 1, 2), (0, 1, 2, 3), (1, 2, 3, 0))
//[4, 1, 14, 11, 8, 5, 2, 15, 12, 9, 6, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_400(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000faf00000f5fULL, 0x0000f0500000f0a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000a0055000500aaULL, 0x0055000a00aa0005ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0f0f05050f0fULL, 0x0f0f0a0a0f0f0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc633396663999cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_400(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006c00930093006cULL, 0x006c0093009300c9ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 0, 1, 2), (1, 2, 3, 0), (0, 1, 2, 3))
//[0, 5, 14, 11, 4, 9, 2, 15, 8, 13, 6, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_401(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f6f00000f9fULL, 0x0000f0900000f060ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0006009900090066ULL, 0x0099000600660009ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06060f0f09090f0fULL, 0x0f0f06060f0f0909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccca3335aaa3555cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_401(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ac0053005300acULL, 0x00ac0053005300c5ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 1, 2, 0), (0, 2, 1, 3), (1, 0, 3, 2))
//[4, 1, 14, 11, 0, 9, 6, 15, 12, 5, 10, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_402(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000c3c2ULL, 0x00003c340000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400980098ULL, 0x0091009100620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_402(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0e08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 1, 2, 0), (1, 0, 3, 2), (0, 2, 1, 3))
//[0, 5, 14, 11, 8, 1, 6, 15, 4, 13, 10, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_403(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096940000c3c2ULL, 0x00003c3800006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800540054ULL, 0x0051005100a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_403(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060e04ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 2, 1, 0), (0, 1, 2, 3), (1, 0, 3, 2))
//[4, 1, 14, 11, 0, 5, 10, 15, 12, 9, 6, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_404(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a4ULL, 0x00005a520000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400980098ULL, 0x0091009100620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_404(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 2, 1, 0), (0, 1, 3, 2), (1, 0, 2, 3))
//[4, 1, 14, 11, 0, 5, 10, 15, 8, 13, 6, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_405(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a4ULL, 0x0000969200006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400980098ULL, 0x0051005100a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_405(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0605090a060509ULL, 0x0a0605090a060a06ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 2, 1, 0), (1, 0, 2, 3), (0, 1, 3, 2))
//[0, 5, 14, 11, 4, 1, 10, 15, 12, 9, 6, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_406(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006968ULL, 0x00005a520000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800540054ULL, 0x0091009100620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_406(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060a0905060a0905ULL, 0x060a0905060a060aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 0, 1), (3, 2, 1, 0), (1, 0, 3, 2), (0, 1, 2, 3))
//[0, 5, 14, 11, 4, 1, 10, 15, 8, 13, 6, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_407(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006968ULL, 0x0000969200006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800540054ULL, 0x0051005100a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_407(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 2, 3), (1, 0, 3, 2), (3, 2, 0, 1))
//[12, 5, 2, 11, 8, 1, 6, 15, 0, 13, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_408(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c340000c3c2ULL, 0x0000a5a100005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a200a200540054ULL, 0x00c800c800310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_408(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050c0a03050c0a03ULL, 0x050c0a03050c050cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 2, 3), (3, 2, 0, 1), (1, 0, 3, 2))
//[4, 13, 2, 11, 0, 9, 6, 15, 12, 1, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_409(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c380000c3c2ULL, 0x0000696100009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0062006200980098ULL, 0x00c400c400310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_409(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090c0603090c0603ULL, 0x090c0603090c090cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 3, 2), (1, 0, 2, 3), (3, 2, 0, 1))
//[12, 5, 2, 11, 8, 1, 6, 15, 0, 9, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_410(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000006960ULL, 0x0000969000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000900090ULL, 0x0060006000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_410(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600960096ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000696ULL, 0x0000f9690000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3a55aULL, 0x000000003cc3a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00990000006600ffULL, 0x00990000006600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 3, 2), (1, 2, 0, 3), (3, 0, 2, 1))
//[12, 5, 2, 11, 0, 9, 6, 15, 8, 1, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_411(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006996aa55ULL, 0x00000000a6599966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a50055005aULL, 0x009900950066006aULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_411(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0065009a009a0065ULL, 0x0065009a009a0065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa659a659a65959a6ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 3, 2), (3, 0, 2, 1), (1, 2, 0, 3))
//[4, 13, 2, 11, 8, 1, 6, 15, 0, 9, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_412(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c3c3c33c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a6699ULL, 0x000000006a9555aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00003c3cULL, 0x0000c3c30000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006900990096ULL, 0x0055005900aa00a6ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_412(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030c0c030c03030cULL, 0x0c03030c030c0c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000c33c00003cc3ULL, 0x00003cc30000c33cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a90056005600a9ULL, 0x00a90056005600a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6a956a956a95956aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 1, 3, 2), (3, 2, 0, 1), (1, 0, 2, 3))
//[4, 13, 2, 11, 0, 9, 6, 15, 8, 1, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_413(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000a5a0ULL, 0x00005a5000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000500050ULL, 0x00a000a000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_413(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a005a005aULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000a5aULL, 0x0000f5a50000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc36996ULL, 0x000000003cc36996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0055000000aa00ffULL, 0x0055000000aa00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 2, 3, 1), (1, 0, 2, 3), (3, 1, 0, 2))
//[12, 5, 2, 11, 4, 1, 10, 15, 0, 9, 14, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_414(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69699696c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000069600000c3c0ULL, 0x0000969000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000300030ULL, 0x0060006000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_414(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600140014ULL, 0x0028002800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff9000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000c3cULL, 0x0000f9690000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000be41a55aULL, 0x00000000be41a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001b000000e400ffULL, 0x001b000000e400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (0, 2, 3, 1), (3, 1, 0, 2), (1, 0, 2, 3))
//[4, 13, 2, 11, 0, 5, 10, 15, 8, 1, 14, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_415(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5ac3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000c3c0ULL, 0x00005a5000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000300030ULL, 0x00a000a000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_415(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00180018ULL, 0x0024002400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000c3cULL, 0x0000f5a50000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007e816996ULL, 0x000000007e816996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0017000000e800ffULL, 0x0017000000e800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 2, 3), (0, 1, 3, 2), (3, 2, 0, 1))
//[12, 1, 6, 11, 8, 5, 2, 15, 0, 13, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_416(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000006960ULL, 0x0000969000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000900090ULL, 0x0060006000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_416(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600960096ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000696ULL, 0x0000f9690000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5c33cULL, 0x000000005aa5c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00990000006600ffULL, 0x00990000006600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 2, 3), (0, 2, 3, 1), (3, 1, 0, 2))
//[12, 1, 6, 11, 4, 9, 2, 15, 0, 13, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_417(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69699696a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000069600000a5a0ULL, 0x0000969000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000500050ULL, 0x0060006000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_417(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600120012ULL, 0x0048004800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff9000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000a5aULL, 0x0000f9690000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000de21c33cULL, 0x00000000de21c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001d000000e200ffULL, 0x001d000000e200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 2, 3), (3, 1, 0, 2), (0, 2, 3, 1))
//[0, 13, 6, 11, 8, 5, 2, 15, 12, 1, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_418(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a69699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000006960ULL, 0x00005a5000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000900090ULL, 0x00a000a000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_418(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00120012ULL, 0x0084008400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000696ULL, 0x0000f5a50000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000de21c33cULL, 0x00000000de21c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001d000000e200ffULL, 0x001d000000e200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 2, 3), (3, 2, 0, 1), (0, 1, 3, 2))
//[0, 13, 6, 11, 4, 9, 2, 15, 12, 1, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_419(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x000000003cc33cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000a5a0ULL, 0x00005a5000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000500050ULL, 0x00a000a000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_419(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a005a005aULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000a5aULL, 0x0000f5a50000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000009669c33cULL, 0x000000009669c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0055000000aa00ffULL, 0x0055000000aa00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 3, 2), (0, 1, 2, 3), (3, 2, 0, 1))
//[12, 1, 6, 11, 8, 5, 2, 15, 0, 9, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_420(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a520000a5a4ULL, 0x0000c3c100003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c400c400320032ULL, 0x00a800a800510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_420(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030a0c05030a0c05ULL, 0x030a0c05030a030aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 0, 3, 2), (3, 2, 0, 1), (0, 1, 2, 3))
//[0, 13, 6, 11, 4, 9, 2, 15, 8, 1, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_421(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969200006968ULL, 0x0000c3c100003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c800c800320032ULL, 0x0064006400910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_421(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03060c0903060c09ULL, 0x03060c0903060306ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 2, 0, 3), (0, 1, 3, 2), (3, 0, 2, 1))
//[12, 1, 6, 11, 0, 5, 10, 15, 8, 13, 2, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_422(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000006996cc33ULL, 0x00000000c6399966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c30033003cULL, 0x009900930066006cULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_422(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0063009c009c0063ULL, 0x0063009c009c0063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc639c639c63939c6ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (1, 2, 0, 3), (3, 0, 2, 1), (0, 1, 3, 2))
//[0, 13, 6, 11, 4, 1, 10, 15, 12, 9, 2, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_423(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030c0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55acc33ULL, 0x00000000ca3555aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c30033003cULL, 0x0055005300aa00acULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_423(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a3005c005c00a3ULL, 0x00a3005c005c00a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xca35ca35ca3535caULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 0, 2, 1), (0, 1, 3, 2), (1, 2, 0, 3))
//[4, 1, 14, 11, 8, 5, 2, 15, 0, 13, 10, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_424(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09060f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a5a5a55a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c6699ULL, 0x000000006c9333ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00005a5aULL, 0x0000a5a50000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006900990096ULL, 0x0033003900cc00c6ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_424(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050a0a050a05050aULL, 0x0a05050a050a0a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000a55a00005aa5ULL, 0x00005aa50000a55aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c90036003600c9ULL, 0x00c90036003600c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6c936c936c93936cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 0, 2, 1), (1, 2, 0, 3), (0, 1, 3, 2))
//[0, 5, 14, 11, 4, 9, 2, 15, 12, 1, 10, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_425(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050a0f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969696996966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33caa55ULL, 0x00000000ac5333ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600009696ULL, 0x0000696900006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a50055005aULL, 0x0033003500cc00caULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_425(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0906060906090906ULL, 0x0609090609060609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000699600009669ULL, 0x0000966900006996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c5003a003a00c5ULL, 0x00c5003a003a00c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xac53ac53ac5353acULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 1, 0, 2), (0, 2, 3, 1), (1, 0, 2, 3))
//[4, 1, 14, 11, 0, 9, 6, 15, 8, 13, 2, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_426(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3ca5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000a5a0ULL, 0x00003c3000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000500050ULL, 0x00c000c000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_426(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00180018ULL, 0x0042004200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000a5aULL, 0x0000f3c30000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007e816996ULL, 0x000000007e816996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0017000000e800ffULL, 0x0017000000e800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 1, 0, 2), (1, 0, 2, 3), (0, 2, 3, 1))
//[0, 5, 14, 11, 8, 1, 6, 15, 12, 9, 2, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_427(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c69699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000006960ULL, 0x00003c3000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000900090ULL, 0x00c000c000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_427(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00140014ULL, 0x0082008200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000696ULL, 0x0000f3c30000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000be41a55aULL, 0x00000000be41a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001b000000e400ffULL, 0x001b000000e400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 2, 0, 1), (0, 1, 2, 3), (1, 0, 3, 2))
//[4, 1, 14, 11, 0, 5, 10, 15, 12, 9, 2, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_428(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a580000a5a4ULL, 0x0000696100009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0064006400980098ULL, 0x00a200a200510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_428(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090a0605090a0605ULL, 0x090a0605090a090aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 2, 0, 1), (0, 1, 3, 2), (1, 0, 2, 3))
//[4, 1, 14, 11, 0, 5, 10, 15, 8, 13, 2, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_429(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000096699669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000c3c0ULL, 0x00003c3000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000300030ULL, 0x00c000c000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_429(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c003c003cULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000c3cULL, 0x0000f3c30000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa56996ULL, 0x000000005aa56996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0033000000cc00ffULL, 0x0033000000cc00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 2, 0, 1), (1, 0, 2, 3), (0, 1, 3, 2))
//[0, 5, 14, 11, 4, 1, 10, 15, 12, 9, 2, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_430(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x000000005aa55aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000c3c0ULL, 0x00003c3000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000300030ULL, 0x00c000c000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_430(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c003c003cULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000c3cULL, 0x0000f3c30000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000009669a55aULL, 0x000000009669a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0033000000cc00ffULL, 0x0033000000cc00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((2, 3, 1, 0), (3, 2, 0, 1), (1, 0, 3, 2), (0, 1, 2, 3))
//[0, 5, 14, 11, 4, 1, 10, 15, 8, 13, 2, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_431(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969400006968ULL, 0x0000a5a100005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a800a800540054ULL, 0x0062006200910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_431(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05060a0905060a09ULL, 0x05060a0905060506ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 1, 2, 3), (1, 2, 3, 0), (2, 3, 0, 1))
//[8, 5, 2, 15, 12, 9, 6, 3, 0, 13, 10, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_432(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6663999c3339ccc6ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_432(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003600c900c90036ULL, 0x003600c900c9006cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 1, 2, 3), (2, 3, 0, 1), (1, 2, 3, 0))
//[4, 9, 2, 15, 8, 13, 6, 3, 12, 1, 10, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_433(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa3555c3335cccaULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_433(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x003a00c500c5003aULL, 0x003a00c500c500acULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 2, 3, 1), (1, 3, 2, 0), (2, 1, 0, 3))
//[8, 5, 2, 15, 4, 13, 10, 3, 0, 9, 14, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_434(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a0000c3c3ULL, 0x00003c3c0000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a03030303ULL, 0x0c0c0c0c05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_434(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c050c05030a030aULL, 0x030a030a0c050c05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc35a3ca5c35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 2, 3, 1), (2, 1, 0, 3), (1, 3, 2, 0))
//[4, 9, 2, 15, 12, 5, 10, 3, 8, 1, 14, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_435(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000c3c3ULL, 0x00003c3c00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060603030303ULL, 0x0c0c0c0c09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_435(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c090c0903060306ULL, 0x030603060c090c09ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3963c69c3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 3, 2, 1), (1, 2, 0, 3), (2, 1, 3, 0))
//[8, 5, 2, 15, 4, 9, 14, 3, 12, 1, 10, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_436(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_436(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001400bb00280077ULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006d6200009e91ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe916fb04d629f708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00d500c700ea00cbULL, 0x002a003800150034ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a965a99aULL, 0x00000000a99a569aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 3, 2, 1), (1, 2, 3, 0), (2, 1, 0, 3))
//[8, 5, 2, 15, 4, 9, 14, 3, 0, 13, 10, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_437(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_437(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c030c03030c030cULL, 0x030c030c0c030c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc3c33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 3, 2, 1), (2, 1, 0, 3), (1, 2, 3, 0))
//[4, 9, 2, 15, 8, 5, 14, 3, 12, 1, 10, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_438(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_438(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c030c03030c030cULL, 0x030c030c0c030c03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc33c3cc3c33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (0, 3, 2, 1), (2, 1, 3, 0), (1, 2, 0, 3))
//[4, 9, 2, 15, 8, 5, 14, 3, 0, 13, 10, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_439(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_439(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00180077002400bbULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ada200005e51ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe51af708da25fb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00d900cb00e600c7ULL, 0x0026003400190038ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000065a96556ULL, 0x0000000065569a56ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 0, 3), (0, 3, 2, 1), (2, 1, 3, 0))
//[8, 1, 6, 15, 4, 13, 10, 3, 12, 9, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_440(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_440(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001200dd00480077ULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006b6400009e91ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe916fd02b649f708ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00b300a700ec00adULL, 0x004c005800130052ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c963c99cULL, 0x00000000c99c369cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 0, 3), (2, 1, 3, 0), (0, 3, 2, 1))
//[0, 9, 6, 15, 12, 5, 10, 3, 8, 13, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_441(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_441(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001200dd008400bbULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000a7a800005e51ULL, 0x0000333c0000ccc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe51afd027a85fb04ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0073006b00ec006dULL, 0x008c009400130092ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c5a3c55cULL, 0x00000000c55c3a5cULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 3, 0), (0, 1, 2, 3), (2, 3, 0, 1))
//[8, 1, 6, 15, 12, 5, 10, 3, 0, 9, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_442(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6665999a5559aaa6ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_442(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005600a900a90056ULL, 0x005600a900a9006aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 3, 0), (0, 3, 2, 1), (2, 1, 0, 3))
//[8, 1, 6, 15, 4, 13, 10, 3, 0, 9, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_443(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_443(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a050a05050a050aULL, 0x050a050a0a050a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa5a55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 3, 0), (2, 1, 0, 3), (0, 3, 2, 1))
//[0, 9, 6, 15, 12, 5, 10, 3, 8, 1, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_444(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_444(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609060909060906ULL, 0x0906090606090609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966969969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 2, 3, 0), (2, 3, 0, 1), (0, 1, 2, 3))
//[0, 9, 6, 15, 4, 13, 10, 3, 8, 1, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_445(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa955569995666aULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_445(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009a00650065009aULL, 0x009a0065006500a6ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 3, 2, 0), (0, 2, 3, 1), (2, 1, 0, 3))
//[8, 1, 6, 15, 4, 9, 14, 3, 0, 13, 10, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_446(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c0000a5a5ULL, 0x00005a5a0000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c05050505ULL, 0x0a0a0a0a03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_446(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a030a03050c050cULL, 0x050c050c0a030a03ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa53c5ac3a53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (1, 3, 2, 0), (2, 1, 0, 3), (0, 2, 3, 1))
//[0, 9, 6, 15, 8, 5, 14, 3, 12, 1, 10, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_447(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00006969ULL, 0x000096960000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c09090909ULL, 0x0606060603030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_447(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x06030603090c090cULL, 0x090c090c06030603ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x693c96c3693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 0, 3), (0, 2, 3, 1), (1, 3, 2, 0))
//[4, 1, 10, 15, 12, 9, 6, 3, 8, 13, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_448(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000a5a5ULL, 0x00005a5a00006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060605050505ULL, 0x0a0a0a0a09090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_448(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a090a0905060506ULL, 0x050605060a090a09ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5965a69a5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 0, 3), (0, 3, 2, 1), (1, 2, 3, 0))
//[4, 1, 10, 15, 8, 13, 6, 3, 12, 9, 2, 7, 0, 5, 14, 11]
void BOGI128_omega_diffusion_449(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000069969669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_449(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a050a05050a050aULL, 0x050a050a0a050a05ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa55a5aa5a55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 0, 3), (1, 2, 3, 0), (0, 3, 2, 1))
//[0, 5, 10, 15, 12, 9, 6, 3, 8, 13, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_450(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000a55a5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_450(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609060909060906ULL, 0x0906090606090609ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6996966969969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 0, 3), (1, 3, 2, 0), (0, 2, 3, 1))
//[0, 5, 10, 15, 8, 13, 6, 3, 12, 9, 2, 7, 4, 1, 14, 11]
void BOGI128_omega_diffusion_451(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00006969ULL, 0x000096960000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a09090909ULL, 0x0606060605050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_451(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x06050605090a090aULL, 0x090a090a06050605ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x695a96a5695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 3, 0), (0, 3, 2, 1), (1, 2, 0, 3))
//[4, 1, 10, 15, 8, 13, 6, 3, 0, 9, 14, 7, 12, 5, 2, 11]
void BOGI128_omega_diffusion_452(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_452(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00180077004200ddULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000cbc400003e31ULL, 0x0000999600006669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe31cf708bc43fd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00b900ad00e600a7ULL, 0x0046005200190058ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000063c96336ULL, 0x0000000063369c36ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 1, 3, 0), (1, 2, 0, 3), (0, 3, 2, 1))
//[0, 5, 10, 15, 12, 9, 6, 3, 8, 1, 14, 7, 4, 13, 2, 11]
void BOGI128_omega_diffusion_453(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x00000000c33c3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_453(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x001400bb008200ddULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000c7c800003e31ULL, 0x0000555a0000aaa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xe31cfb047c83fd02ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0075006d00ea006bULL, 0x008a009200150094ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a3c5a33aULL, 0x00000000a33a5c3aULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 3, 0, 1), (0, 1, 2, 3), (1, 2, 3, 0))
//[4, 1, 10, 15, 8, 5, 14, 3, 12, 9, 2, 7, 0, 13, 6, 11]
void BOGI128_omega_diffusion_454(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc5333a5553aaacULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_454(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x005c00a300a3005cULL, 0x005c00a300a300caULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 1, 2), (2, 3, 0, 1), (1, 2, 3, 0), (0, 1, 2, 3))
//[0, 5, 10, 15, 4, 9, 14, 3, 8, 13, 2, 7, 12, 1, 6, 11]
void BOGI128_omega_diffusion_455(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc933369993666cULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_455(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x009c00630063009cULL, 0x009c0063006300c6ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 1, 3, 2), (1, 2, 0, 3), (2, 3, 1, 0))
//[8, 5, 2, 15, 12, 9, 6, 3, 4, 1, 14, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_456(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000699633ccULL, 0x0000000036c99966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003c00cc00c3ULL, 0x0099009c00660063ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_456(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006c00930093006cULL, 0x006c00930093006cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x36c936c936c9c936ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 1, 3, 2), (2, 3, 1, 0), (1, 2, 0, 3))
//[4, 9, 2, 15, 8, 13, 6, 3, 0, 5, 14, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_457(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a33ccULL, 0x000000003ac555aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0033003c00cc00c3ULL, 0x0055005c00aa00a3ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_457(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ac0053005300acULL, 0x00ac0053005300acULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ac53ac53ac5c53aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 2, 1, 3), (1, 3, 0, 2), (2, 1, 3, 0))
//[8, 5, 2, 15, 4, 13, 10, 3, 12, 1, 6, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_458(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a0000c3c3ULL, 0x0000a5a500003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a03030303ULL, 0x050505050c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_458(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x030a0c05030a0c05ULL, 0x030a0c05030a0c05ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 2, 1, 3), (2, 1, 3, 0), (1, 3, 0, 2))
//[4, 9, 2, 15, 12, 5, 10, 3, 0, 13, 6, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_459(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000c3c3ULL, 0x0000696900003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060603030303ULL, 0x090909090c0c0c0cULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_459(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03060c0903060c09ULL, 0x03060c0903060c09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 3, 1, 2), (1, 2, 0, 3), (2, 1, 3, 0))
//[8, 5, 2, 15, 4, 9, 14, 3, 12, 1, 6, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_460(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_460(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc3c33cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 3, 1, 2), (1, 2, 3, 0), (2, 1, 0, 3))
//[8, 5, 2, 15, 4, 9, 14, 3, 0, 13, 6, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_461(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_461(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008200dd004100eeULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006b6400009798ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9768df206b94ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ab00e3005700d3ULL, 0x0054001c00a8002cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000095a69559ULL, 0x0000000095596a59ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 3, 1, 2), (2, 1, 0, 3), (1, 2, 3, 0))
//[4, 9, 2, 15, 8, 5, 14, 3, 12, 1, 6, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_462(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_462(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004200dd008100eeULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000a7a800005b54ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x03030c0c03030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ba4df20a758ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x006700e3009b00d3ULL, 0x0098001c0064002cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000596a5995ULL, 0x000000005995a695ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (0, 3, 1, 2), (2, 1, 3, 0), (1, 2, 0, 3))
//[4, 9, 2, 15, 8, 5, 14, 3, 0, 13, 6, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_463(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc33cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x003c00c300c3003cULL, 0x00c3003c003c00c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_463(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc33cc33cc3c33cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 0, 3), (0, 1, 3, 2), (2, 3, 1, 0))
//[8, 1, 6, 15, 12, 5, 10, 3, 4, 13, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_464(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000699655aaULL, 0x0000000056a99966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005a00aa00a5ULL, 0x0099009a00660065ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_464(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006a00950095006aULL, 0x006a00950095006aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x56a956a956a9a956ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 0, 3), (0, 3, 1, 2), (2, 1, 3, 0))
//[8, 1, 6, 15, 4, 13, 10, 3, 12, 5, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_465(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_465(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa5a55aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 0, 3), (2, 1, 3, 0), (0, 3, 1, 2))
//[0, 9, 6, 15, 12, 5, 10, 3, 4, 13, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_466(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_466(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996696996ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 0, 3), (2, 3, 1, 0), (0, 1, 3, 2))
//[0, 9, 6, 15, 4, 13, 10, 3, 12, 5, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_467(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00c000ff00ff0000ULL, 0x000000ff00c00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a9966ULL, 0x000000009a6555aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009600660069ULL, 0x0055005600aa00a9ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_467(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a60059005900a6ULL, 0x00a60059005900a6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9a659a659a65659aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 3, 0), (0, 3, 1, 2), (2, 1, 0, 3))
//[8, 1, 6, 15, 4, 13, 10, 3, 0, 5, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_468(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_468(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008400bb002100eeULL, 0x009f0000006f0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00006d6200009798ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9768bf406d92ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cd00e5003700b5ULL, 0x0032001a00c8004aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000093c69339ULL, 0x0000000093396c39ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 2, 3, 0), (2, 1, 0, 3), (0, 3, 1, 2))
//[0, 9, 6, 15, 12, 5, 10, 3, 4, 1, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_469(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c3cc3ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c030303030c0cULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_469(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00480077002100eeULL, 0x005f000000af0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ada200005b54ULL, 0x0000ccc30000333cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ba47f80ad52ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cd00e9003b0079ULL, 0x0032001600c40086ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053ca5335ULL, 0x000000005335ac35ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 3, 0, 2), (0, 2, 1, 3), (2, 1, 3, 0))
//[8, 1, 6, 15, 4, 9, 14, 3, 12, 5, 2, 11, 0, 13, 10, 7]
void BOGI128_omega_diffusion_470(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c0000a5a5ULL, 0x0000c3c300005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c05050505ULL, 0x030303030a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_470(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x050c0a03050c0a03ULL, 0x050c0a03050c0a03ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (1, 3, 0, 2), (2, 1, 3, 0), (0, 2, 1, 3))
//[0, 9, 6, 15, 8, 5, 14, 3, 4, 13, 2, 11, 12, 1, 10, 7]
void BOGI128_omega_diffusion_471(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00006969ULL, 0x0000c3c300009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c09090909ULL, 0x0303030306060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_471(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090c0603090c0603ULL, 0x090c0603090c0603ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 0, 3), (0, 3, 1, 2), (1, 2, 3, 0))
//[4, 1, 10, 15, 8, 13, 6, 3, 12, 5, 2, 11, 0, 9, 14, 7]
void BOGI128_omega_diffusion_472(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_472(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002400bb008100eeULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000c7c800003d32ULL, 0x0000666900009996ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x05050a0a05050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3dc2bf40c738ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x006700e5009d00b5ULL, 0x0098001a0062004aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000396c3993ULL, 0x000000003993c693ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 0, 3), (1, 2, 3, 0), (0, 3, 1, 2))
//[0, 5, 10, 15, 12, 9, 6, 3, 4, 13, 2, 11, 8, 1, 14, 7]
void BOGI128_omega_diffusion_473(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_473(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000000fff00f00ULL, 0x00000000fff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00280077004100eeULL, 0x003f000000cf0000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000cbc400003d32ULL, 0x0000aaa50000555aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0909060609090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3dc27f80cb34ef10ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ab00e9005d0079ULL, 0x0054001600a20086ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035ac3553ULL, 0x000000003553ca53ULL, 32);
	ROL128(&H, &L, H, L, 104);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 3, 0), (0, 2, 1, 3), (1, 3, 0, 2))
//[4, 1, 10, 15, 12, 9, 6, 3, 0, 5, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_474(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000096960000a5a5ULL, 0x0000696900005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606060605050505ULL, 0x090909090a0a0a0aULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_474(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05060a0905060a09ULL, 0x05060a0905060a09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 3, 0), (0, 3, 1, 2), (1, 2, 0, 3))
//[4, 1, 10, 15, 8, 13, 6, 3, 0, 5, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_475(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa55aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x005a00a500a5005aULL, 0x00a5005a005a00a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069969669ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0606090909090606ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_475(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa55aa55aa5a55aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 3, 0), (1, 2, 0, 3), (0, 3, 1, 2))
//[0, 5, 10, 15, 12, 9, 6, 3, 4, 1, 14, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_476(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 112);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996699669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0096006900690096ULL, 0x0069009600960069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a5aa5ULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a050505050a0aULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_476(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669966996696996ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 1, 3, 0), (1, 3, 0, 2), (0, 2, 1, 3))
//[0, 5, 10, 15, 8, 13, 6, 3, 4, 1, 14, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_477(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00006969ULL, 0x0000a5a500009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a09090909ULL, 0x0505050506060606ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_477(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x090a0605090a0605ULL, 0x090a0605090a0605ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 3, 1, 0), (0, 1, 3, 2), (1, 2, 0, 3))
//[4, 1, 10, 15, 8, 5, 14, 3, 0, 13, 6, 11, 12, 9, 2, 7]
void BOGI128_omega_diffusion_478(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x006000ff00ff0000ULL, 0x000000ff00600000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c55aaULL, 0x000000005ca333ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0055005a00aa00a5ULL, 0x0033003a00cc00c5ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_478(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ca0035003500caULL, 0x00ca0035003500caULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ca35ca35ca3a35cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 0, 2, 1), (2, 3, 1, 0), (1, 2, 0, 3), (0, 1, 3, 2))
//[0, 5, 10, 15, 4, 9, 14, 3, 12, 1, 6, 11, 8, 13, 2, 7]
void BOGI128_omega_diffusion_479(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00a000ff00ff0000ULL, 0x000000ff00a00000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c9966ULL, 0x000000009c6333ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0099009600660069ULL, 0x0033003600cc00c9ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_479(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c60039003900c6ULL, 0x00c60039003900c6ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9c639c639c63639cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 1, 3), (1, 3, 2, 0), (2, 0, 3, 1))
//[8, 5, 2, 15, 0, 13, 10, 7, 12, 9, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_480(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000639c639cULL, 0x0000000039c639c6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_480(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0036006300c9009cULL, 0x0036006300c9009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 1, 3), (2, 0, 3, 1), (1, 3, 2, 0))
//[4, 9, 2, 15, 12, 1, 10, 7, 8, 13, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_481(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a35ca35cULL, 0x0000000035ca35caULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_481(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003a00a300c5005cULL, 0x003a00a300c5005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 3, 1), (1, 0, 2, 3), (2, 3, 1, 0))
//[8, 5, 2, 15, 12, 1, 10, 7, 4, 9, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_482(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x696996965a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000005a50ULL, 0x000096900000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000a000a0ULL, 0x0060006000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_482(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600840084ULL, 0x0021002100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000696000005a5ULL, 0x0000f9690000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000b7483cc3ULL, 0x00000000b7483cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008b0000007400ffULL, 0x008b0000007400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 3, 1), (1, 3, 2, 0), (2, 0, 1, 3))
//[8, 5, 2, 15, 0, 13, 10, 7, 4, 9, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_483(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000639c639cULL, 0x00000000936c936cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_483(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600000000ULL, 0x0069006900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000969ULL, 0x0000f9690000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff003cc3ULL, 0x00000000ff003cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00c30000003c00ffULL, 0x00c30000003c00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 3, 1), (2, 0, 1, 3), (1, 3, 2, 0))
//[4, 9, 2, 15, 12, 1, 10, 7, 8, 5, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_484(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a35ca35cULL, 0x0000000053ac53acULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_484(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00000000ULL, 0x00a500a500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000005a5ULL, 0x0000f5a50000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff003cc3ULL, 0x00000000ff003cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00c30000003c00ffULL, 0x00c30000003c00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 2, 3, 1), (2, 3, 1, 0), (1, 0, 2, 3))
//[4, 9, 2, 15, 0, 13, 10, 7, 8, 5, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_485(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a96966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000009690ULL, 0x00005a5000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000600060ULL, 0x00a000a000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_485(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00480048ULL, 0x0021002100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000969ULL, 0x0000f5a50000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007b843cc3ULL, 0x000000007b843cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0047000000b800ffULL, 0x0047000000b800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 3, 2, 1), (1, 2, 3, 0), (2, 0, 1, 3))
//[8, 5, 2, 15, 0, 9, 14, 7, 4, 13, 10, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_486(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00003c3cULL, 0x0000c3c30000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a0c0c0c0cULL, 0x0303030305050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_486(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050305030a0c0a0cULL, 0x0a0c0a0c05030503ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c35a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (0, 3, 2, 1), (2, 0, 1, 3), (1, 2, 3, 0))
//[4, 9, 2, 15, 8, 1, 14, 7, 12, 5, 10, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_487(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600003c3cULL, 0x0000c3c300006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060c0c0c0cULL, 0x0303030309090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_487(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x09030903060c060cULL, 0x060c060c09030903ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c3963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 0, 2, 3), (0, 2, 3, 1), (2, 3, 1, 0))
//[8, 1, 6, 15, 12, 9, 2, 7, 4, 13, 10, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_488(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x696996963c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000003c30ULL, 0x000096900000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000c000c0ULL, 0x0060006000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_488(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600820082ULL, 0x0041004100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000696000003c3ULL, 0x0000f9690000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000d7285aa5ULL, 0x00000000d7285aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x008d0000007200ffULL, 0x008d0000007200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 0, 2, 3), (2, 3, 1, 0), (0, 2, 3, 1))
//[0, 9, 6, 15, 8, 13, 2, 7, 12, 5, 10, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_489(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a3c3cc3c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000003c30ULL, 0x00005a500000c3c0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000c000c0ULL, 0x00a000a000300030ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_489(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00420042ULL, 0x0081008100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000003c3ULL, 0x0000f5a50000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000db249669ULL, 0x00000000db249669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x004d000000b200ffULL, 0x004d000000b200ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 2, 3, 0), (0, 3, 2, 1), (2, 0, 1, 3))
//[8, 1, 6, 15, 0, 13, 10, 7, 4, 9, 14, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_490(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00005a5aULL, 0x0000a5a50000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c0a0a0a0aULL, 0x0505050503030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_490(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0099009900660066ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096699669ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030503050c0a0c0aULL, 0x0c0a0c0a03050305ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a53c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 2, 3, 0), (2, 0, 1, 3), (0, 3, 2, 1))
//[0, 9, 6, 15, 12, 1, 10, 7, 8, 5, 14, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_491(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00009696ULL, 0x000069690000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c06060606ULL, 0x0909090903030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_491(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x0055005500aa00aaULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa55aa5ULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x030903090c060c06ULL, 0x0c060c0603090309ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c3693c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 3, 2, 0), (0, 2, 1, 3), (2, 0, 3, 1))
//[8, 1, 6, 15, 0, 9, 14, 7, 12, 5, 10, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_492(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000659a659aULL, 0x0000000059a659a6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_492(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0056006500a9009aULL, 0x0056006500a9009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 3, 2, 0), (0, 2, 3, 1), (2, 0, 1, 3))
//[8, 1, 6, 15, 0, 9, 14, 7, 4, 13, 10, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_493(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000659a659aULL, 0x00000000956a956aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_493(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600000000ULL, 0x0069006900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000969ULL, 0x0000f9690000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff005aa5ULL, 0x00000000ff005aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00a50000005a00ffULL, 0x00a50000005a00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 3, 2, 0), (2, 0, 1, 3), (0, 2, 3, 1))
//[0, 9, 6, 15, 8, 1, 14, 7, 12, 5, 10, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_494(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a956a956ULL, 0x0000000059a659a6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_494(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00000000ULL, 0x00a500a500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a000005a5ULL, 0x0000f5a50000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff009669ULL, 0x00000000ff009669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00690000009600ffULL, 0x00690000009600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (1, 3, 2, 0), (2, 0, 3, 1), (0, 2, 1, 3))
//[0, 9, 6, 15, 8, 1, 14, 7, 4, 13, 10, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_495(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a956a956ULL, 0x00000000956a956aULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_495(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009a00a900650056ULL, 0x009a00a900650056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 1, 3), (0, 2, 3, 1), (1, 3, 2, 0))
//[4, 1, 10, 15, 12, 9, 2, 7, 8, 13, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_496(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c53ac53aULL, 0x0000000035ca35caULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_496(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00000000ULL, 0x00c300c300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000003c3ULL, 0x0000f3c30000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff005aa5ULL, 0x00000000ff005aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00a50000005a00ffULL, 0x00a50000005a00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 1, 3), (0, 3, 2, 1), (1, 2, 3, 0))
//[4, 1, 10, 15, 8, 13, 2, 7, 12, 9, 6, 3, 0, 5, 14, 11]
void BOGI128_omega_diffusion_497(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600005a5aULL, 0x0000a5a500006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060a0a0a0aULL, 0x0505050509090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_497(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x09050905060a060aULL, 0x060a060a09050905ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a5965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 1, 3), (1, 2, 3, 0), (0, 3, 2, 1))
//[0, 5, 10, 15, 12, 9, 2, 7, 8, 13, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_498(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00009696ULL, 0x000069690000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a06060606ULL, 0x0909090905050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_498(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x00000f0000000f00ULL, 0x00000f0000000f00ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xff00ff00ff00ff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x0033003300cc00ccULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc33cc3ULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x050905090a060a06ULL, 0x0a060a0605090509ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a5695a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff00000000ULL, 0x000000000000ffffULL, 16);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 1, 3), (1, 3, 2, 0), (0, 2, 3, 1))
//[0, 5, 10, 15, 8, 13, 2, 7, 12, 9, 6, 3, 4, 1, 14, 11]
void BOGI128_omega_diffusion_499(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c936c936ULL, 0x0000000039c639c6ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_499(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00000000ULL, 0x00c300c300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30003fffc000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000003c3ULL, 0x0000f3c30000fc3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff009669ULL, 0x00000000ff009669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00690000009600ffULL, 0x00690000009600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 3, 1), (0, 2, 1, 3), (1, 3, 2, 0))
//[4, 1, 10, 15, 12, 9, 2, 7, 8, 5, 14, 3, 0, 13, 6, 11]
void BOGI128_omega_diffusion_500(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c53ac53aULL, 0x0000000053ac53acULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_500(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005c00c500a3003aULL, 0x005c00c500a3003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 0, 3, 1), (1, 3, 2, 0), (0, 2, 1, 3))
//[0, 5, 10, 15, 8, 13, 2, 7, 4, 9, 14, 3, 12, 1, 6, 11]
void BOGI128_omega_diffusion_501(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c936c936ULL, 0x00000000936c936cULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_501(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009c00c900630036ULL, 0x009c00c900630036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 3, 1, 0), (0, 2, 3, 1), (1, 0, 2, 3))
//[4, 1, 10, 15, 0, 9, 14, 7, 8, 13, 6, 3, 12, 5, 2, 11]
void BOGI128_omega_diffusion_502(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c96966969ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000009690ULL, 0x00003c3000006960ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000600060ULL, 0x00c000c000900090ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_502(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00280028ULL, 0x0041004100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30009fff6000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000969ULL, 0x0000f3c30000f696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x000000007d825aa5ULL, 0x000000007d825aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0027000000d800ffULL, 0x0027000000d800ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 0, 2), (2, 3, 1, 0), (1, 0, 2, 3), (0, 2, 3, 1))
//[0, 5, 10, 15, 8, 1, 14, 7, 12, 9, 6, 3, 4, 13, 2, 11]
void BOGI128_omega_diffusion_503(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c5a5aa5a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000005a50ULL, 0x00003c300000a5a0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000a000a0ULL, 0x00c000c000500050ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_503(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00240024ULL, 0x0081008100000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30005fffa000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c000005a5ULL, 0x0000f3c30000fa5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000bd429669ULL, 0x00000000bd429669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x002b000000d400ffULL, 0x002b000000d400ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 1, 3), (1, 0, 3, 2), (2, 3, 0, 1))
//[8, 5, 2, 15, 12, 1, 10, 7, 0, 13, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_504(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5695a9669a5965aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400006961ULL, 0x0000969200005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c400c4ULL, 0x00c800c800310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0202020208080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_504(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050704ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 1, 3), (1, 3, 0, 2), (2, 0, 3, 1))
//[8, 5, 2, 15, 0, 13, 10, 7, 12, 1, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_505(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000639c639cULL, 0x000000006c936c93ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_505(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 1, 3), (2, 0, 3, 1), (1, 3, 0, 2))
//[4, 9, 2, 15, 12, 1, 10, 7, 0, 13, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_506(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a35ca35cULL, 0x00000000ac53ac53ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_506(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 1, 3), (2, 3, 0, 1), (1, 0, 3, 2))
//[4, 9, 2, 15, 0, 13, 10, 7, 12, 1, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_507(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69a5965aa5695a96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069680000a5a1ULL, 0x00005a5200009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c800c8ULL, 0x00c400c400310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0202020204040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_507(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f03030303ULL, 0x0c0c0c0c00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00cc00ccULL, 0x0033003300000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000003cffc30ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000030fc00000000ULL, 0x0000cf030000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090b08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 3, 1), (1, 3, 0, 2), (2, 0, 1, 3))
//[8, 5, 2, 15, 0, 13, 10, 7, 4, 1, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_508(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000639c639cULL, 0x00000000c639c639ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_508(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00c6006c00390093ULL, 0x00c6006c00390093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 2, 3, 1), (2, 0, 1, 3), (1, 3, 0, 2))
//[4, 9, 2, 15, 12, 1, 10, 7, 0, 5, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_509(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a35ca35cULL, 0x00000000ca35ca35ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_509(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ca00ac00350053ULL, 0x00ca00ac00350053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 3, 1, 2), (1, 2, 0, 3), (2, 0, 3, 1))
//[8, 5, 2, 15, 0, 9, 14, 7, 12, 1, 6, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_510(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c5ac3a55a3ca5c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00003c3cULL, 0x0000a5a50000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a0c0c0c0cULL, 0x0505050503030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_510(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0c05030a0c0503ULL, 0x0a0c05030a0c0503ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (0, 3, 1, 2), (2, 0, 3, 1), (1, 2, 0, 3))
//[4, 9, 2, 15, 8, 1, 14, 7, 0, 13, 6, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_511(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c96c369963c69c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600003c3cULL, 0x000069690000c3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060c0c0c0cULL, 0x0909090903030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_511(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060c0903060c0903ULL, 0x060c0903060c0903ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 0, 3, 2), (0, 2, 1, 3), (2, 3, 0, 1))
//[8, 1, 6, 15, 12, 9, 2, 7, 0, 5, 14, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_512(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3693c9669c3963cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200006961ULL, 0x0000969400003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a200a2ULL, 0x00a800a800510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0404040408080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_512(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030702ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 0, 3, 2), (2, 3, 0, 1), (0, 2, 1, 3))
//[0, 9, 6, 15, 8, 13, 2, 7, 4, 1, 14, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_513(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3a53c5aa5c35a3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c20000a5a1ULL, 0x00005a5800003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800620062ULL, 0x0064006400910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020201010101ULL, 0x0808080804040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_513(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030b02ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 2, 0, 3), (0, 3, 1, 2), (2, 0, 3, 1))
//[8, 1, 6, 15, 0, 13, 10, 7, 12, 5, 2, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_514(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a3ca5c33c5ac3a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000066996699ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00005a5aULL, 0x0000c3c30000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c0a0a0a0aULL, 0x0303030305050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_514(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0a03050c0a0305ULL, 0x0c0a03050c0a0305ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 2, 0, 3), (2, 0, 3, 1), (0, 3, 1, 2))
//[0, 9, 6, 15, 12, 1, 10, 7, 4, 13, 2, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_515(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x963c69c33c96c369ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000aa55aa55ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00003c3c00009696ULL, 0x0000c3c300006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c0c0c06060606ULL, 0x0303030309090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_515(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0603090c060309ULL, 0x0c0603090c060309ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 3, 0, 2), (0, 2, 1, 3), (2, 0, 3, 1))
//[8, 1, 6, 15, 0, 9, 14, 7, 12, 5, 2, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_516(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000659a659aULL, 0x000000006a956a95ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_516(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066006600990099ULL, 0x0066006600990099ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 3, 0, 2), (0, 2, 3, 1), (2, 0, 1, 3))
//[8, 1, 6, 15, 0, 9, 14, 7, 4, 13, 2, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_517(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000659a659aULL, 0x00000000a659a659ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_517(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00a6006a00590095ULL, 0x00a6006a00590095ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 3, 0, 2), (2, 0, 1, 3), (0, 2, 3, 1))
//[0, 9, 6, 15, 8, 1, 14, 7, 12, 5, 2, 11, 4, 13, 10, 3]
void BOGI128_omega_diffusion_518(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a956a956ULL, 0x000000006a956a95ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_518(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006a00a600950059ULL, 0x006a00a600950059ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (1, 3, 0, 2), (2, 0, 3, 1), (0, 2, 1, 3))
//[0, 9, 6, 15, 8, 1, 14, 7, 4, 13, 2, 11, 12, 5, 10, 3]
void BOGI128_omega_diffusion_519(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a956a956ULL, 0x00000000a659a659ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_519(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00aa00550055ULL, 0x00aa00aa00550055ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0c03030c0c0303ULL, 0x0c0c03030c0c0303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 1, 3), (0, 2, 3, 1), (1, 3, 0, 2))
//[4, 1, 10, 15, 12, 9, 2, 7, 0, 13, 6, 11, 8, 5, 14, 3]
void BOGI128_omega_diffusion_520(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c53ac53aULL, 0x00000000ac53ac53ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_520(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00ac00ca00530035ULL, 0x00ac00ca00530035ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 1, 3), (1, 3, 0, 2), (0, 2, 3, 1))
//[0, 5, 10, 15, 8, 13, 2, 7, 12, 1, 6, 11, 4, 9, 14, 3]
void BOGI128_omega_diffusion_521(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c936c936ULL, 0x000000006c936c93ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_521(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x006c00c600930039ULL, 0x006c00c600930039ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 3, 1), (0, 2, 1, 3), (1, 3, 0, 2))
//[4, 1, 10, 15, 12, 9, 2, 7, 0, 5, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_522(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c53ac53aULL, 0x00000000ca35ca35ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_522(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0606090906060909ULL, 0x0606090906060909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 3, 1), (0, 3, 1, 2), (1, 2, 0, 3))
//[4, 1, 10, 15, 8, 13, 2, 7, 0, 5, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_523(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a96a569965a69a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000969600005a5aULL, 0x000069690000a5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060606060a0a0a0aULL, 0x0909090905050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_523(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x060a0905060a0905ULL, 0x060a0905060a0905ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 3, 1), (1, 2, 0, 3), (0, 3, 1, 2))
//[0, 5, 10, 15, 12, 9, 2, 7, 4, 1, 14, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_524(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x965a69a55a96a569ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000cc33cc33ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00005a5a00009696ULL, 0x0000a5a500006969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a0a0a06060606ULL, 0x0505050509090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_524(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0605090a060509ULL, 0x0a0605090a060509ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 0, 3, 1), (1, 3, 0, 2), (0, 2, 1, 3))
//[0, 5, 10, 15, 8, 13, 2, 7, 4, 1, 14, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_525(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00f00ULL, 0x000000000ff00f00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0f0f00f0ff0f00f0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000ff0f0000ff0fULL, 0x000000f0000000f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000f00ff00f00000ULL, 0x00ff000f000000f0ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c936c936ULL, 0x00000000c639c639ULL, 32);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_525(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 40 cycles, 8 masks
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0f0f0f0fULL, 0x0000000000000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ff00ff00ULL, 0x00000000ff00ff00ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f0f00000f0fULL, 0x0000f0f00000f0f0ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00cc00330033ULL, 0x00cc00cc00330033ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0a05050a0a0505ULL, 0x0a0a05050a0a0505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 3, 0, 1), (0, 2, 1, 3), (1, 0, 3, 2))
//[4, 1, 10, 15, 0, 9, 14, 7, 12, 5, 2, 11, 8, 13, 6, 3]
void BOGI128_omega_diffusion_526(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69c3963cc3693c96ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x000069680000c3c1ULL, 0x00003c3400009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a800a8ULL, 0x00a200a200510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080801010101ULL, 0x0404040402020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_526(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f05050505ULL, 0x0a0a0a0a00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00aa00aaULL, 0x0055005500000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000005affa50ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000050fa00000000ULL, 0x0000af050000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330093ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090d08ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 1, 2, 0), (2, 3, 0, 1), (1, 0, 3, 2), (0, 2, 1, 3))
//[0, 5, 10, 15, 8, 1, 14, 7, 4, 13, 2, 11, 12, 9, 6, 3]
void BOGI128_omega_diffusion_527(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5c35a3cc3a53c5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a40000c3c1ULL, 0x00003c3800005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800640064ULL, 0x0062006200910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040401010101ULL, 0x0808080802020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_527(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f09090909ULL, 0x0606060600000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00660066ULL, 0x0099009900000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000096ff690ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000090f600000000ULL, 0x00006f090000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330053ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050d04ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 2, 3), (1, 0, 3, 2), (2, 3, 1, 0))
//[8, 5, 2, 15, 12, 1, 6, 11, 4, 13, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_528(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a52ULL, 0x00003c380000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c400c4ULL, 0x0051005100a800a8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_528(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c05030a0c05030aULL, 0x0c05030a0c050c05ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 2, 3), (2, 3, 1, 0), (1, 0, 3, 2))
//[4, 9, 2, 15, 0, 13, 6, 11, 12, 5, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_529(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009692ULL, 0x00003c340000c3c1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c800c8ULL, 0x0091009100640064ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_529(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c0903060c090306ULL, 0x0c0903060c090c09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 3, 2), (1, 0, 2, 3), (2, 3, 1, 0))
//[8, 5, 2, 15, 12, 1, 6, 11, 4, 9, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_530(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000006960ULL, 0x0000969000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000900090ULL, 0x0060006000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_530(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600960096ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000696ULL, 0x0000f9690000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a3cc3ULL, 0x00000000a55a3cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00990000006600ffULL, 0x00990000006600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 3, 2), (1, 3, 2, 0), (2, 0, 1, 3))
//[8, 5, 2, 15, 0, 13, 6, 11, 4, 9, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_531(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69699696a5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000069600000a5a0ULL, 0x0000969000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000500050ULL, 0x0060006000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_531(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600120012ULL, 0x0048004800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff9000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000a5aULL, 0x0000f9690000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ed123cc3ULL, 0x00000000ed123cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d10000002e00ffULL, 0x00d10000002e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 3, 2), (2, 0, 1, 3), (1, 3, 2, 0))
//[4, 9, 2, 15, 12, 1, 6, 11, 8, 5, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_532(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5a69699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a000006960ULL, 0x00005a5000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000900090ULL, 0x00a000a000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_532(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00120012ULL, 0x0084008400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff50006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000696ULL, 0x0000f5a50000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000ed123cc3ULL, 0x00000000ed123cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00d10000002e00ffULL, 0x00d10000002e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 1, 3, 2), (2, 3, 1, 0), (1, 0, 2, 3))
//[4, 9, 2, 15, 0, 13, 6, 11, 8, 5, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_533(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33cc33cULL, 0x00000000c33cc33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x0c0c030303030c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000a5a0ULL, 0x00005a5000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000500050ULL, 0x00a000a000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_533(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a005a005aULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000a5aULL, 0x0000f5a50000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c0c0c0303ULL, 0x03030c0c0c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069963cc3ULL, 0x0000000069963cc3ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0055000000aa00ffULL, 0x0055000000aa00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 3, 1, 2), (1, 0, 2, 3), (2, 1, 3, 0))
//[8, 5, 2, 15, 4, 1, 14, 11, 12, 9, 6, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_534(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000936ccc33ULL, 0x000000003cc39966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00c600330039ULL, 0x0099009600660069ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_534(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003600c900c90036ULL, 0x003600c900c90036ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6c936c936c93936cULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (0, 3, 1, 2), (2, 1, 3, 0), (1, 0, 2, 3))
//[4, 9, 2, 15, 0, 5, 14, 11, 8, 13, 6, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_535(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000053accc33ULL, 0x000000003cc355aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00cc00ca00330035ULL, 0x0055005a00aa00a5ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_535(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x003a00c500c5003aULL, 0x003a00c500c5003aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xac53ac53ac5353acULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 2, 3), (0, 1, 3, 2), (2, 3, 1, 0))
//[8, 1, 6, 15, 12, 5, 2, 11, 4, 13, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_536(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6969969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000696000006960ULL, 0x0000969000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000900090ULL, 0x0060006000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_536(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600960096ULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff90006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000696ULL, 0x0000f9690000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c5aa5ULL, 0x00000000c33c5aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00990000006600ffULL, 0x00990000006600ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 2, 3), (0, 3, 1, 2), (2, 1, 3, 0))
//[8, 1, 6, 15, 4, 13, 2, 11, 12, 5, 10, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_537(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x06090f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000000956aaa55ULL, 0x000000005aa59966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00a600550059ULL, 0x0099009600660069ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_537(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005600a900a90056ULL, 0x005600a900a90056ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6a956a956a95956aULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 2, 3), (2, 1, 3, 0), (0, 3, 1, 2))
//[0, 9, 6, 15, 12, 5, 2, 11, 4, 13, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_538(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003000ff00ff0000ULL, 0x000000ff00300000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a050f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c3c3c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000059a66699ULL, 0x00000000966955aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c30000c3c3ULL, 0x00003c3c00003c3cULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x0c0c03030c0c0303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006a00990095ULL, 0x0055005a00aa00a5ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_538(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0c03030c030c0c03ULL, 0x030c0c030c03030cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00003cc30000c33cULL, 0x0000c33c00003cc3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009a00650065009aULL, 0x009a00650065009aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa659a659a65959a6ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 2, 3), (2, 3, 1, 0), (0, 1, 3, 2))
//[0, 9, 6, 15, 4, 13, 2, 11, 12, 5, 10, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_539(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000a5a0ULL, 0x00005a5000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000500050ULL, 0x00a000a000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_539(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a005a005aULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000a5aULL, 0x0000f5a50000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000c33c9669ULL, 0x00000000c33c9669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0055000000aa00ffULL, 0x0055000000aa00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 3, 2), (0, 1, 2, 3), (2, 3, 1, 0))
//[8, 1, 6, 15, 12, 5, 2, 11, 4, 9, 14, 3, 0, 13, 10, 7]
void BOGI128_omega_diffusion_540(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c34ULL, 0x00005a580000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a200a2ULL, 0x0031003100c800c8ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0808080801010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_540(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a03050c0a03050cULL, 0x0a03050c0a030a03ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 0, 3, 2), (2, 3, 1, 0), (0, 1, 2, 3))
//[0, 9, 6, 15, 4, 13, 2, 11, 8, 5, 14, 3, 12, 1, 10, 7]
void BOGI128_omega_diffusion_541(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000033cc33ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c38ULL, 0x0000969400006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800620062ULL, 0x0031003100c400c4ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0404040401010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_541(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0603090c0603090cULL, 0x0603090c06030603ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 3, 2, 0), (0, 1, 3, 2), (2, 0, 1, 3))
//[8, 1, 6, 15, 0, 5, 14, 11, 4, 13, 10, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_542(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x69699696c3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000069600000c3c0ULL, 0x0000969000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0090009000300030ULL, 0x0060006000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_542(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0096009600140014ULL, 0x0028002800000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6fff9000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000069600000c3cULL, 0x0000f9690000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000eb145aa5ULL, 0x00000000eb145aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b10000004e00ffULL, 0x00b10000004e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (1, 3, 2, 0), (2, 0, 1, 3), (0, 1, 3, 2))
//[0, 9, 6, 15, 4, 1, 14, 11, 12, 5, 10, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_543(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xa5a55a5ac3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a00000c3c0ULL, 0x00005a5000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0050005000300030ULL, 0x00a000a000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_543(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005a005a00180018ULL, 0x0024002400000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xafff5000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000a5a00000c3cULL, 0x0000f5a50000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000e7189669ULL, 0x00000000e7189669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00710000008e00ffULL, 0x00710000008e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 0, 1, 3), (0, 1, 3, 2), (1, 3, 2, 0))
//[4, 1, 10, 15, 12, 5, 2, 11, 8, 13, 6, 3, 0, 9, 14, 7]
void BOGI128_omega_diffusion_544(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3c69699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c000006960ULL, 0x00003c3000009690ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000900090ULL, 0x00c000c000600060ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_544(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00140014ULL, 0x0082008200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff30006fff9000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000696ULL, 0x0000f3c30000f969ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000eb145aa5ULL, 0x00000000eb145aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00b10000004e00ffULL, 0x00b10000004e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 0, 1, 3), (1, 3, 2, 0), (0, 1, 3, 2))
//[0, 5, 10, 15, 4, 13, 2, 11, 12, 9, 6, 3, 8, 1, 14, 7]
void BOGI128_omega_diffusion_545(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3ca5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000a5a0ULL, 0x00003c3000005a50ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000500050ULL, 0x00c000c000a000a0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_545(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c00180018ULL, 0x0042004200000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000afff5000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000a5aULL, 0x0000f3c30000f5a5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000e7189669ULL, 0x00000000e7189669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00710000008e00ffULL, 0x00710000008e00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 1, 3, 0), (0, 3, 1, 2), (1, 0, 2, 3))
//[4, 1, 10, 15, 0, 13, 6, 11, 8, 5, 14, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_546(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x009000ff00ff0000ULL, 0x000000ff00900000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9696969669699696ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000035caaa55ULL, 0x000000005aa533ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696900006969ULL, 0x0000969600009696ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0606090906060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00aa00ac00550053ULL, 0x0033003c00cc00c3ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_546(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0609090609060609ULL, 0x0906060906090906ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000966900006996ULL, 0x0000699600009669ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x005c00a300a3005cULL, 0x005c00a300a3005cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xca35ca35ca3535caULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 1, 3, 0), (1, 0, 2, 3), (0, 3, 1, 2))
//[0, 5, 10, 15, 12, 1, 6, 11, 4, 9, 14, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_547(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x005000ff00ff0000ULL, 0x000000ff00500000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0c030f0f00000f0fULL, 0x000000000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a5a5a5aa5a55a5aULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000039c66699ULL, 0x00000000966933ccULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a50000a5a5ULL, 0x00005a5a00005a5aULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x0a0a05050a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0066006c00990093ULL, 0x0033003c00cc00c3ULL, 8);
	ROL128(&H, &L, H, L, 92);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_547(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000ffff0000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000000ff00ff0ULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0a05050a050a0a05ULL, 0x050a0a050a05050aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00005aa50000a55aULL, 0x0000a55a00005aa5ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x009c00630063009cULL, 0x009c00630063009cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc639c639c63939c6ULL, 64);
	ROL128(&H, &L, H, L, 112);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 3, 1, 0), (0, 1, 2, 3), (1, 0, 3, 2))
//[4, 1, 10, 15, 0, 5, 14, 11, 12, 9, 6, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_548(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000099669966ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009694ULL, 0x00005a520000a5a1ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a800a8ULL, 0x0091009100620062ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_548(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0a0905060a090506ULL, 0x0a0905060a090a09ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 3, 1, 0), (0, 1, 3, 2), (1, 0, 2, 3))
//[4, 1, 10, 15, 0, 5, 14, 11, 8, 13, 6, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_549(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55aa55aULL, 0x00000000a55aa55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x0a0a050505050a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000c3c0ULL, 0x00003c3000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000300030ULL, 0x00c000c000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_549(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c003c003cULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000c3cULL, 0x0000f3c30000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a0a0a0505ULL, 0x05050a0a0a0a0505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069965aa5ULL, 0x0000000069965aa5ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0033000000cc00ffULL, 0x0033000000cc00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 3, 1, 0), (1, 0, 2, 3), (0, 1, 3, 2))
//[0, 5, 10, 15, 4, 1, 14, 11, 12, 9, 6, 3, 8, 13, 2, 7]
void BOGI128_omega_diffusion_550(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	ROL128(&H, &L, H, L, 120);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000069966996ULL, 0x0000000069966996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0606090909090606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xc3c33c3cc3c33c3cULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c00000c3c0ULL, 0x00003c3000003c30ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0030003000300030ULL, 0x00c000c000c000c0ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_550(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 36 cycles, 7 masks
	bit_permute_step_128(&H, &L, H, L, 0x003c003c003c003cULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000fff00000fffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xcfff3000cfff3000ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x00000c3c00000c3cULL, 0x0000f3c30000f3c3ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060606060909ULL, 0x0909060606060909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000000a55a9669ULL, 0x00000000a55a9669ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0033000000cc00ffULL, 0x0033000000cc00ffULL, 8);
	ROL128(&H, &L, H, L, 120);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 0, 1), (2, 3, 1, 0), (1, 0, 3, 2), (0, 1, 2, 3))
//[0, 5, 10, 15, 4, 1, 14, 11, 8, 13, 6, 3, 12, 9, 2, 7]
void BOGI128_omega_diffusion_551(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000055aa55aaULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a58ULL, 0x0000969200006961ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800640064ULL, 0x0051005100a200a2ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0202020201010101ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_551(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0605090a0605090aULL, 0x0605090a06050605ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 2, 3), (1, 0, 3, 2), (2, 3, 0, 1))
//[8, 5, 2, 15, 12, 1, 6, 11, 0, 13, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_552(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a52ULL, 0x0000a5a100005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c400c4ULL, 0x00c800c800310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_552(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 2, 3), (1, 3, 0, 2), (2, 0, 3, 1))
//[8, 5, 2, 15, 0, 13, 6, 11, 12, 1, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_553(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96a5695aa5965a69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400009692ULL, 0x0000696100005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c800c8ULL, 0x00c400c400310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_553(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x00660099009900a9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a0505010dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 2, 3), (2, 0, 3, 1), (1, 3, 0, 2))
//[4, 9, 2, 15, 12, 1, 6, 11, 0, 13, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_554(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5a69a596695a96a5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800005a52ULL, 0x0000a5a100009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c400c4ULL, 0x00c800c800310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_554(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0c0c0c0cULL, 0x0303030300000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00330033ULL, 0x00cc00cc00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000c3ff3c0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000c0f300000000ULL, 0x00003f0c0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa005500550065ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x090906060909010dULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 2, 3), (2, 3, 0, 1), (1, 0, 3, 2))
//[4, 9, 2, 15, 0, 13, 6, 11, 12, 1, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_555(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009692ULL, 0x0000696100009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c800c8ULL, 0x00c400c400310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_555(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 3, 2), (1, 0, 2, 3), (2, 3, 0, 1))
//[8, 5, 2, 15, 12, 1, 6, 11, 0, 9, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_556(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a52ULL, 0x0000c3c100003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c400c4ULL, 0x00a800a800510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040402020202ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_556(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05030a0c05030a0cULL, 0x05030a0c05030503ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 1, 3, 2), (2, 3, 0, 1), (1, 0, 2, 3))
//[4, 9, 2, 15, 0, 13, 6, 11, 8, 1, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_557(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000033cc33ccULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009692ULL, 0x0000c3c100003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0032003200c800c8ULL, 0x0064006400910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080802020202ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_557(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0903060c0903060cULL, 0x0903060c09030903ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 3, 2, 1), (1, 0, 3, 2), (2, 1, 0, 3))
//[8, 5, 2, 15, 4, 1, 14, 11, 0, 13, 10, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_558(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6663999cccc63339ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_558(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006c00930093006cULL, 0x006c009300930036ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (0, 3, 2, 1), (2, 1, 0, 3), (1, 0, 3, 2))
//[4, 9, 2, 15, 0, 5, 14, 11, 12, 1, 10, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_559(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa3555cccca3335ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_559(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ac0053005300acULL, 0x00ac00530053003aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 2, 3), (0, 1, 3, 2), (2, 3, 0, 1))
//[8, 1, 6, 15, 12, 5, 2, 11, 0, 13, 10, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_560(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c34ULL, 0x0000a5a100005a58ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a200a2ULL, 0x00c800c800310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_560(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03050c0a03050c0aULL, 0x03050c0a03050305ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 2, 3), (2, 3, 0, 1), (0, 1, 3, 2))
//[0, 9, 6, 15, 4, 13, 2, 11, 12, 1, 10, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_561(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x00000000cc33cc33ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c38ULL, 0x0000696100009694ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800620062ULL, 0x00c400c400310031ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_561(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03090c0603090c06ULL, 0x03090c0603090309ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 3, 2), (0, 1, 2, 3), (2, 3, 0, 1))
//[8, 1, 6, 15, 12, 5, 2, 11, 0, 9, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_562(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x9669699669969669ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c34ULL, 0x0000c3c100003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a200a2ULL, 0x00a800a800510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_562(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x0066009900990069ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 3, 2), (0, 3, 2, 1), (2, 1, 0, 3))
//[8, 1, 6, 15, 4, 13, 2, 11, 0, 9, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_563(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x6665999aaaa65559ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_563(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x006a00950095006aULL, 0x006a009500950056ULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 3, 2), (2, 1, 0, 3), (0, 3, 2, 1))
//[0, 9, 6, 15, 12, 5, 2, 11, 8, 1, 14, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_564(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000003cc3c33cULL, 0x000000003cc3c33cULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f3f00000fcfULL, 0x0000f0c00000f030ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000300cc000c0033ULL, 0x00cc00030033000cULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030f0f0c0c0f0fULL, 0x0f0f03030f0f0c0cULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xaaa95556666a9995ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_564(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00a60059005900a6ULL, 0x00a600590059009aULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 0, 3, 2), (2, 3, 0, 1), (0, 1, 2, 3))
//[0, 9, 6, 15, 4, 13, 2, 11, 8, 1, 14, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_565(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5aa5a55aa55a5aa5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200003c38ULL, 0x0000c3c100003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800620062ULL, 0x0064006400910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_565(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500a5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030303ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 3, 0, 2), (0, 1, 2, 3), (2, 0, 3, 1))
//[8, 1, 6, 15, 0, 5, 14, 11, 12, 9, 2, 7, 4, 13, 10, 3]
void BOGI128_omega_diffusion_566(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x96c3693cc3963c69ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200009694ULL, 0x0000696100003c38ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a800a8ULL, 0x00a200a200510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020204040404ULL, 0x0101010108080808ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_566(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0066009900990066ULL, 0x00660099009900c9ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c0303010bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (1, 3, 0, 2), (2, 0, 3, 1), (0, 1, 2, 3))
//[0, 9, 6, 15, 4, 1, 14, 11, 8, 13, 2, 7, 12, 5, 10, 3]
void BOGI128_omega_diffusion_567(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x5ac3a53cc35a3ca5ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000c3c200005a58ULL, 0x0000a5a100003c34ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800640064ULL, 0x0062006200910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0202020208080808ULL, 0x0101010104040404ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_567(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00aa0055005500aaULL, 0x00aa0055005500c5ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x03030c0c03030c0cULL, 0x03030c0c03030107ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 0, 3, 1), (0, 1, 2, 3), (1, 3, 0, 2))
//[4, 1, 10, 15, 12, 5, 2, 11, 0, 9, 14, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_568(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3c69c396693c96c3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800003c34ULL, 0x0000c3c100009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a200a2ULL, 0x00a800a800510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_568(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f0a0a0a0aULL, 0x0505050500000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00550055ULL, 0x00aa00aa00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff00000a5ff5a0ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000a0f500000000ULL, 0x00005f0a0000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc003300330063ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x090906060909010bULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 0, 3, 1), (1, 3, 0, 2), (0, 1, 2, 3))
//[0, 5, 10, 15, 4, 13, 2, 11, 8, 1, 14, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_569(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3ca5c35aa53c5ac3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400003c38ULL, 0x0000c3c100005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800620062ULL, 0x0064006400910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_569(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f06060606ULL, 0x0909090900000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00990099ULL, 0x0066006600000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff0000069ff960ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000060f900000000ULL, 0x00009f060000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300a3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050107ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 1, 0, 3), (0, 3, 2, 1), (1, 0, 3, 2))
//[4, 1, 10, 15, 0, 13, 6, 11, 12, 9, 2, 7, 8, 5, 14, 3]
void BOGI128_omega_diffusion_570(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000000096696996ULL, 0x0000000096696996ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f9f00000f6fULL, 0x0000f0600000f090ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0009006600060099ULL, 0x0066000900990006ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x09090f0f06060f0fULL, 0x0f0f09090f0f0606ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc5333aaaac5553ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_570(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ca0035003500caULL, 0x00ca00350035005cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 1, 0, 3), (1, 0, 3, 2), (0, 3, 2, 1))
//[0, 5, 10, 15, 12, 1, 6, 11, 8, 13, 2, 7, 4, 9, 14, 3]
void BOGI128_omega_diffusion_571(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x0ff00ff00ff00f00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000f0f00000f0f00ULL, 0x000f0f00000f0f00ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x000000005aa5a55aULL, 0x000000005aa5a55aULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x00000f5f00000fafULL, 0x0000f0a00000f050ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x000500aa000a0055ULL, 0x00aa00050055000aULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050f0f0a0a0f0fULL, 0x0f0f05050f0f0a0aULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xccc93336666c9993ULL, 64);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_571(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00c60039003900c6ULL, 0x00c600390039009cULL, 8);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 3, 0, 1), (0, 1, 2, 3), (1, 0, 3, 2))
//[4, 1, 10, 15, 0, 5, 14, 11, 12, 9, 2, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_572(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009694ULL, 0x0000696100009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a800a8ULL, 0x00a200a200510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_572(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0909060609090606ULL, 0x0909060609090909ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 3, 0, 1), (0, 1, 3, 2), (1, 0, 2, 3))
//[4, 1, 10, 15, 0, 5, 14, 11, 8, 13, 2, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_573(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000055aa55aaULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000696800009694ULL, 0x0000a5a100005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0054005400a800a8ULL, 0x0062006200910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0808080804040404ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_573(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0905060a0905060aULL, 0x0905060a09050905ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 3, 0, 1), (1, 0, 2, 3), (0, 1, 3, 2))
//[0, 5, 10, 15, 4, 1, 14, 11, 12, 9, 2, 7, 8, 13, 6, 3]
void BOGI128_omega_diffusion_574(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x00000000aa55aa55ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a58ULL, 0x0000696100009692ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800640064ULL, 0x00a200a200510051ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_574(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05090a0605090a06ULL, 0x05090a0605090509ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
//((3, 2, 1, 0), (2, 3, 0, 1), (1, 0, 3, 2), (0, 1, 2, 3))
//[0, 5, 10, 15, 4, 1, 14, 11, 8, 13, 2, 7, 12, 9, 6, 3]
void BOGI128_omega_diffusion_575(DIFF_O_WRD_t * out, DIFF_I_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f00000f0f0000ULL, 0x0f0f00000f0f0000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00000000ULL, 0x00ff00ff00000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0000ffff0000ffffULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0x3cc3c33cc33c3cc3ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x0000000099669966ULL, 0x0000000066996699ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000a5a400005a58ULL, 0x0000a5a100005a52ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x0098009800640064ULL, 0x0062006200910091ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x0404040408080808ULL, 0x0101010102020202ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}

void BOGI128_omega_inv_diffusion_575(DIFF_I_WRD_t * out, DIFF_O_WRD_t * in)
{
	uint64_t H = 0ULL;
	uint64_t L = 0ULL;
	int i;
	for (i = 0; i < 16; i++)
	{
		H = (H << 4) | (in[i] & 0xf);
		L = (L << 4) | (in[i + 16] & 0xf);
	}
	// 41 cycles, 8 masks
	ROL128(&H, &L, H, L, 124);
	bit_permute_step_128(&H, &L, H, L, 0x0f0f0f0f00000000ULL, 0x0f0f0f0f00000000ULL, 4);
	bit_permute_step_128(&H, &L, H, L, 0x00ff00ff00ff00ffULL, 0x0000000000000000ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x00000ff00000f00fULL, 0x0000000000000000ULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00000000f00ff00fULL, 0x000000000ff00ff0ULL, 32);
	bit_permute_step_128(&H, &L, H, L, 0x0000000000000000ULL, 0xffff000000ffff00ULL, 64);
	bit_permute_step_128(&H, &L, H, L, 0x000000ff00000000ULL, 0x0000ff000000ffffULL, 16);
	bit_permute_step_128(&H, &L, H, L, 0x00cc0033003300ccULL, 0x00cc0033003300c3ULL, 8);
	bit_permute_step_128(&H, &L, H, L, 0x05050a0a05050a0aULL, 0x05050a0a05050505ULL, 4);

	for (i = 15; i >= 0; i--)
	{
		out[i] = (DIFF_O_WRD_t)(H & 0xf);
		H = H >> 4;
		out[i + 16] = (DIFF_O_WRD_t)(L & 0xf);
		L = L >> 4;
	}
}
void(*BOGI128_omega_diffusion[576])(DIFF_O_WRD_t *, DIFF_I_WRD_t *) = {
	BOGI128_omega_diffusion_0, BOGI128_omega_diffusion_1, BOGI128_omega_diffusion_2, BOGI128_omega_diffusion_3,
	BOGI128_omega_diffusion_4, BOGI128_omega_diffusion_5, BOGI128_omega_diffusion_6, BOGI128_omega_diffusion_7,
	BOGI128_omega_diffusion_8, BOGI128_omega_diffusion_9, BOGI128_omega_diffusion_10, BOGI128_omega_diffusion_11,
	BOGI128_omega_diffusion_12, BOGI128_omega_diffusion_13, BOGI128_omega_diffusion_14, BOGI128_omega_diffusion_15,
	BOGI128_omega_diffusion_16, BOGI128_omega_diffusion_17, BOGI128_omega_diffusion_18, BOGI128_omega_diffusion_19,
	BOGI128_omega_diffusion_20, BOGI128_omega_diffusion_21, BOGI128_omega_diffusion_22, BOGI128_omega_diffusion_23,
	BOGI128_omega_diffusion_24, BOGI128_omega_diffusion_25, BOGI128_omega_diffusion_26, BOGI128_omega_diffusion_27,
	BOGI128_omega_diffusion_28, BOGI128_omega_diffusion_29, BOGI128_omega_diffusion_30, BOGI128_omega_diffusion_31,
	BOGI128_omega_diffusion_32, BOGI128_omega_diffusion_33, BOGI128_omega_diffusion_34, BOGI128_omega_diffusion_35,
	BOGI128_omega_diffusion_36, BOGI128_omega_diffusion_37, BOGI128_omega_diffusion_38, BOGI128_omega_diffusion_39,
	BOGI128_omega_diffusion_40, BOGI128_omega_diffusion_41, BOGI128_omega_diffusion_42, BOGI128_omega_diffusion_43,
	BOGI128_omega_diffusion_44, BOGI128_omega_diffusion_45, BOGI128_omega_diffusion_46, BOGI128_omega_diffusion_47,
	BOGI128_omega_diffusion_48, BOGI128_omega_diffusion_49, BOGI128_omega_diffusion_50, BOGI128_omega_diffusion_51,
	BOGI128_omega_diffusion_52, BOGI128_omega_diffusion_53, BOGI128_omega_diffusion_54, BOGI128_omega_diffusion_55,
	BOGI128_omega_diffusion_56, BOGI128_omega_diffusion_57, BOGI128_omega_diffusion_58, BOGI128_omega_diffusion_59,
	BOGI128_omega_diffusion_60, BOGI128_omega_diffusion_61, BOGI128_omega_diffusion_62, BOGI128_omega_diffusion_63,
	BOGI128_omega_diffusion_64, BOGI128_omega_diffusion_65, BOGI128_omega_diffusion_66, BOGI128_omega_diffusion_67,
	BOGI128_omega_diffusion_68, BOGI128_omega_diffusion_69, BOGI128_omega_diffusion_70, BOGI128_omega_diffusion_71,
	BOGI128_omega_diffusion_72, BOGI128_omega_diffusion_73, BOGI128_omega_diffusion_74, BOGI128_omega_diffusion_75,
	BOGI128_omega_diffusion_76, BOGI128_omega_diffusion_77, BOGI128_omega_diffusion_78, BOGI128_omega_diffusion_79,
	BOGI128_omega_diffusion_80, BOGI128_omega_diffusion_81, BOGI128_omega_diffusion_82, BOGI128_omega_diffusion_83,
	BOGI128_omega_diffusion_84, BOGI128_omega_diffusion_85, BOGI128_omega_diffusion_86, BOGI128_omega_diffusion_87,
	BOGI128_omega_diffusion_88, BOGI128_omega_diffusion_89, BOGI128_omega_diffusion_90, BOGI128_omega_diffusion_91,
	BOGI128_omega_diffusion_92, BOGI128_omega_diffusion_93, BOGI128_omega_diffusion_94, BOGI128_omega_diffusion_95,
	BOGI128_omega_diffusion_96, BOGI128_omega_diffusion_97, BOGI128_omega_diffusion_98, BOGI128_omega_diffusion_99,
	BOGI128_omega_diffusion_100, BOGI128_omega_diffusion_101, BOGI128_omega_diffusion_102, BOGI128_omega_diffusion_103,
	BOGI128_omega_diffusion_104, BOGI128_omega_diffusion_105, BOGI128_omega_diffusion_106, BOGI128_omega_diffusion_107,
	BOGI128_omega_diffusion_108, BOGI128_omega_diffusion_109, BOGI128_omega_diffusion_110, BOGI128_omega_diffusion_111,
	BOGI128_omega_diffusion_112, BOGI128_omega_diffusion_113, BOGI128_omega_diffusion_114, BOGI128_omega_diffusion_115,
	BOGI128_omega_diffusion_116, BOGI128_omega_diffusion_117, BOGI128_omega_diffusion_118, BOGI128_omega_diffusion_119,
	BOGI128_omega_diffusion_120, BOGI128_omega_diffusion_121, BOGI128_omega_diffusion_122, BOGI128_omega_diffusion_123,
	BOGI128_omega_diffusion_124, BOGI128_omega_diffusion_125, BOGI128_omega_diffusion_126, BOGI128_omega_diffusion_127,
	BOGI128_omega_diffusion_128, BOGI128_omega_diffusion_129, BOGI128_omega_diffusion_130, BOGI128_omega_diffusion_131,
	BOGI128_omega_diffusion_132, BOGI128_omega_diffusion_133, BOGI128_omega_diffusion_134, BOGI128_omega_diffusion_135,
	BOGI128_omega_diffusion_136, BOGI128_omega_diffusion_137, BOGI128_omega_diffusion_138, BOGI128_omega_diffusion_139,
	BOGI128_omega_diffusion_140, BOGI128_omega_diffusion_141, BOGI128_omega_diffusion_142, BOGI128_omega_diffusion_143,
	BOGI128_omega_diffusion_144, BOGI128_omega_diffusion_145, BOGI128_omega_diffusion_146, BOGI128_omega_diffusion_147,
	BOGI128_omega_diffusion_148, BOGI128_omega_diffusion_149, BOGI128_omega_diffusion_150, BOGI128_omega_diffusion_151,
	BOGI128_omega_diffusion_152, BOGI128_omega_diffusion_153, BOGI128_omega_diffusion_154, BOGI128_omega_diffusion_155,
	BOGI128_omega_diffusion_156, BOGI128_omega_diffusion_157, BOGI128_omega_diffusion_158, BOGI128_omega_diffusion_159,
	BOGI128_omega_diffusion_160, BOGI128_omega_diffusion_161, BOGI128_omega_diffusion_162, BOGI128_omega_diffusion_163,
	BOGI128_omega_diffusion_164, BOGI128_omega_diffusion_165, BOGI128_omega_diffusion_166, BOGI128_omega_diffusion_167,
	BOGI128_omega_diffusion_168, BOGI128_omega_diffusion_169, BOGI128_omega_diffusion_170, BOGI128_omega_diffusion_171,
	BOGI128_omega_diffusion_172, BOGI128_omega_diffusion_173, BOGI128_omega_diffusion_174, BOGI128_omega_diffusion_175,
	BOGI128_omega_diffusion_176, BOGI128_omega_diffusion_177, BOGI128_omega_diffusion_178, BOGI128_omega_diffusion_179,
	BOGI128_omega_diffusion_180, BOGI128_omega_diffusion_181, BOGI128_omega_diffusion_182, BOGI128_omega_diffusion_183,
	BOGI128_omega_diffusion_184, BOGI128_omega_diffusion_185, BOGI128_omega_diffusion_186, BOGI128_omega_diffusion_187,
	BOGI128_omega_diffusion_188, BOGI128_omega_diffusion_189, BOGI128_omega_diffusion_190, BOGI128_omega_diffusion_191,
	BOGI128_omega_diffusion_192, BOGI128_omega_diffusion_193, BOGI128_omega_diffusion_194, BOGI128_omega_diffusion_195,
	BOGI128_omega_diffusion_196, BOGI128_omega_diffusion_197, BOGI128_omega_diffusion_198, BOGI128_omega_diffusion_199,
	BOGI128_omega_diffusion_200, BOGI128_omega_diffusion_201, BOGI128_omega_diffusion_202, BOGI128_omega_diffusion_203,
	BOGI128_omega_diffusion_204, BOGI128_omega_diffusion_205, BOGI128_omega_diffusion_206, BOGI128_omega_diffusion_207,
	BOGI128_omega_diffusion_208, BOGI128_omega_diffusion_209, BOGI128_omega_diffusion_210, BOGI128_omega_diffusion_211,
	BOGI128_omega_diffusion_212, BOGI128_omega_diffusion_213, BOGI128_omega_diffusion_214, BOGI128_omega_diffusion_215,
	BOGI128_omega_diffusion_216, BOGI128_omega_diffusion_217, BOGI128_omega_diffusion_218, BOGI128_omega_diffusion_219,
	BOGI128_omega_diffusion_220, BOGI128_omega_diffusion_221, BOGI128_omega_diffusion_222, BOGI128_omega_diffusion_223,
	BOGI128_omega_diffusion_224, BOGI128_omega_diffusion_225, BOGI128_omega_diffusion_226, BOGI128_omega_diffusion_227,
	BOGI128_omega_diffusion_228, BOGI128_omega_diffusion_229, BOGI128_omega_diffusion_230, BOGI128_omega_diffusion_231,
	BOGI128_omega_diffusion_232, BOGI128_omega_diffusion_233, BOGI128_omega_diffusion_234, BOGI128_omega_diffusion_235,
	BOGI128_omega_diffusion_236, BOGI128_omega_diffusion_237, BOGI128_omega_diffusion_238, BOGI128_omega_diffusion_239,
	BOGI128_omega_diffusion_240, BOGI128_omega_diffusion_241, BOGI128_omega_diffusion_242, BOGI128_omega_diffusion_243,
	BOGI128_omega_diffusion_244, BOGI128_omega_diffusion_245, BOGI128_omega_diffusion_246, BOGI128_omega_diffusion_247,
	BOGI128_omega_diffusion_248, BOGI128_omega_diffusion_249, BOGI128_omega_diffusion_250, BOGI128_omega_diffusion_251,
	BOGI128_omega_diffusion_252, BOGI128_omega_diffusion_253, BOGI128_omega_diffusion_254, BOGI128_omega_diffusion_255,
	BOGI128_omega_diffusion_256, BOGI128_omega_diffusion_257, BOGI128_omega_diffusion_258, BOGI128_omega_diffusion_259,
	BOGI128_omega_diffusion_260, BOGI128_omega_diffusion_261, BOGI128_omega_diffusion_262, BOGI128_omega_diffusion_263,
	BOGI128_omega_diffusion_264, BOGI128_omega_diffusion_265, BOGI128_omega_diffusion_266, BOGI128_omega_diffusion_267,
	BOGI128_omega_diffusion_268, BOGI128_omega_diffusion_269, BOGI128_omega_diffusion_270, BOGI128_omega_diffusion_271,
	BOGI128_omega_diffusion_272, BOGI128_omega_diffusion_273, BOGI128_omega_diffusion_274, BOGI128_omega_diffusion_275,
	BOGI128_omega_diffusion_276, BOGI128_omega_diffusion_277, BOGI128_omega_diffusion_278, BOGI128_omega_diffusion_279,
	BOGI128_omega_diffusion_280, BOGI128_omega_diffusion_281, BOGI128_omega_diffusion_282, BOGI128_omega_diffusion_283,
	BOGI128_omega_diffusion_284, BOGI128_omega_diffusion_285, BOGI128_omega_diffusion_286, BOGI128_omega_diffusion_287,
	BOGI128_omega_diffusion_288, BOGI128_omega_diffusion_289, BOGI128_omega_diffusion_290, BOGI128_omega_diffusion_291,
	BOGI128_omega_diffusion_292, BOGI128_omega_diffusion_293, BOGI128_omega_diffusion_294, BOGI128_omega_diffusion_295,
	BOGI128_omega_diffusion_296, BOGI128_omega_diffusion_297, BOGI128_omega_diffusion_298, BOGI128_omega_diffusion_299,
	BOGI128_omega_diffusion_300, BOGI128_omega_diffusion_301, BOGI128_omega_diffusion_302, BOGI128_omega_diffusion_303,
	BOGI128_omega_diffusion_304, BOGI128_omega_diffusion_305, BOGI128_omega_diffusion_306, BOGI128_omega_diffusion_307,
	BOGI128_omega_diffusion_308, BOGI128_omega_diffusion_309, BOGI128_omega_diffusion_310, BOGI128_omega_diffusion_311,
	BOGI128_omega_diffusion_312, BOGI128_omega_diffusion_313, BOGI128_omega_diffusion_314, BOGI128_omega_diffusion_315,
	BOGI128_omega_diffusion_316, BOGI128_omega_diffusion_317, BOGI128_omega_diffusion_318, BOGI128_omega_diffusion_319,
	BOGI128_omega_diffusion_320, BOGI128_omega_diffusion_321, BOGI128_omega_diffusion_322, BOGI128_omega_diffusion_323,
	BOGI128_omega_diffusion_324, BOGI128_omega_diffusion_325, BOGI128_omega_diffusion_326, BOGI128_omega_diffusion_327,
	BOGI128_omega_diffusion_328, BOGI128_omega_diffusion_329, BOGI128_omega_diffusion_330, BOGI128_omega_diffusion_331,
	BOGI128_omega_diffusion_332, BOGI128_omega_diffusion_333, BOGI128_omega_diffusion_334, BOGI128_omega_diffusion_335,
	BOGI128_omega_diffusion_336, BOGI128_omega_diffusion_337, BOGI128_omega_diffusion_338, BOGI128_omega_diffusion_339,
	BOGI128_omega_diffusion_340, BOGI128_omega_diffusion_341, BOGI128_omega_diffusion_342, BOGI128_omega_diffusion_343,
	BOGI128_omega_diffusion_344, BOGI128_omega_diffusion_345, BOGI128_omega_diffusion_346, BOGI128_omega_diffusion_347,
	BOGI128_omega_diffusion_348, BOGI128_omega_diffusion_349, BOGI128_omega_diffusion_350, BOGI128_omega_diffusion_351,
	BOGI128_omega_diffusion_352, BOGI128_omega_diffusion_353, BOGI128_omega_diffusion_354, BOGI128_omega_diffusion_355,
	BOGI128_omega_diffusion_356, BOGI128_omega_diffusion_357, BOGI128_omega_diffusion_358, BOGI128_omega_diffusion_359,
	BOGI128_omega_diffusion_360, BOGI128_omega_diffusion_361, BOGI128_omega_diffusion_362, BOGI128_omega_diffusion_363,
	BOGI128_omega_diffusion_364, BOGI128_omega_diffusion_365, BOGI128_omega_diffusion_366, BOGI128_omega_diffusion_367,
	BOGI128_omega_diffusion_368, BOGI128_omega_diffusion_369, BOGI128_omega_diffusion_370, BOGI128_omega_diffusion_371,
	BOGI128_omega_diffusion_372, BOGI128_omega_diffusion_373, BOGI128_omega_diffusion_374, BOGI128_omega_diffusion_375,
	BOGI128_omega_diffusion_376, BOGI128_omega_diffusion_377, BOGI128_omega_diffusion_378, BOGI128_omega_diffusion_379,
	BOGI128_omega_diffusion_380, BOGI128_omega_diffusion_381, BOGI128_omega_diffusion_382, BOGI128_omega_diffusion_383,
	BOGI128_omega_diffusion_384, BOGI128_omega_diffusion_385, BOGI128_omega_diffusion_386, BOGI128_omega_diffusion_387,
	BOGI128_omega_diffusion_388, BOGI128_omega_diffusion_389, BOGI128_omega_diffusion_390, BOGI128_omega_diffusion_391,
	BOGI128_omega_diffusion_392, BOGI128_omega_diffusion_393, BOGI128_omega_diffusion_394, BOGI128_omega_diffusion_395,
	BOGI128_omega_diffusion_396, BOGI128_omega_diffusion_397, BOGI128_omega_diffusion_398, BOGI128_omega_diffusion_399,
	BOGI128_omega_diffusion_400, BOGI128_omega_diffusion_401, BOGI128_omega_diffusion_402, BOGI128_omega_diffusion_403,
	BOGI128_omega_diffusion_404, BOGI128_omega_diffusion_405, BOGI128_omega_diffusion_406, BOGI128_omega_diffusion_407,
	BOGI128_omega_diffusion_408, BOGI128_omega_diffusion_409, BOGI128_omega_diffusion_410, BOGI128_omega_diffusion_411,
	BOGI128_omega_diffusion_412, BOGI128_omega_diffusion_413, BOGI128_omega_diffusion_414, BOGI128_omega_diffusion_415,
	BOGI128_omega_diffusion_416, BOGI128_omega_diffusion_417, BOGI128_omega_diffusion_418, BOGI128_omega_diffusion_419,
	BOGI128_omega_diffusion_420, BOGI128_omega_diffusion_421, BOGI128_omega_diffusion_422, BOGI128_omega_diffusion_423,
	BOGI128_omega_diffusion_424, BOGI128_omega_diffusion_425, BOGI128_omega_diffusion_426, BOGI128_omega_diffusion_427,
	BOGI128_omega_diffusion_428, BOGI128_omega_diffusion_429, BOGI128_omega_diffusion_430, BOGI128_omega_diffusion_431,
	BOGI128_omega_diffusion_432, BOGI128_omega_diffusion_433, BOGI128_omega_diffusion_434, BOGI128_omega_diffusion_435,
	BOGI128_omega_diffusion_436, BOGI128_omega_diffusion_437, BOGI128_omega_diffusion_438, BOGI128_omega_diffusion_439,
	BOGI128_omega_diffusion_440, BOGI128_omega_diffusion_441, BOGI128_omega_diffusion_442, BOGI128_omega_diffusion_443,
	BOGI128_omega_diffusion_444, BOGI128_omega_diffusion_445, BOGI128_omega_diffusion_446, BOGI128_omega_diffusion_447,
	BOGI128_omega_diffusion_448, BOGI128_omega_diffusion_449, BOGI128_omega_diffusion_450, BOGI128_omega_diffusion_451,
	BOGI128_omega_diffusion_452, BOGI128_omega_diffusion_453, BOGI128_omega_diffusion_454, BOGI128_omega_diffusion_455,
	BOGI128_omega_diffusion_456, BOGI128_omega_diffusion_457, BOGI128_omega_diffusion_458, BOGI128_omega_diffusion_459,
	BOGI128_omega_diffusion_460, BOGI128_omega_diffusion_461, BOGI128_omega_diffusion_462, BOGI128_omega_diffusion_463,
	BOGI128_omega_diffusion_464, BOGI128_omega_diffusion_465, BOGI128_omega_diffusion_466, BOGI128_omega_diffusion_467,
	BOGI128_omega_diffusion_468, BOGI128_omega_diffusion_469, BOGI128_omega_diffusion_470, BOGI128_omega_diffusion_471,
	BOGI128_omega_diffusion_472, BOGI128_omega_diffusion_473, BOGI128_omega_diffusion_474, BOGI128_omega_diffusion_475,
	BOGI128_omega_diffusion_476, BOGI128_omega_diffusion_477, BOGI128_omega_diffusion_478, BOGI128_omega_diffusion_479,
	BOGI128_omega_diffusion_480, BOGI128_omega_diffusion_481, BOGI128_omega_diffusion_482, BOGI128_omega_diffusion_483,
	BOGI128_omega_diffusion_484, BOGI128_omega_diffusion_485, BOGI128_omega_diffusion_486, BOGI128_omega_diffusion_487,
	BOGI128_omega_diffusion_488, BOGI128_omega_diffusion_489, BOGI128_omega_diffusion_490, BOGI128_omega_diffusion_491,
	BOGI128_omega_diffusion_492, BOGI128_omega_diffusion_493, BOGI128_omega_diffusion_494, BOGI128_omega_diffusion_495,
	BOGI128_omega_diffusion_496, BOGI128_omega_diffusion_497, BOGI128_omega_diffusion_498, BOGI128_omega_diffusion_499,
	BOGI128_omega_diffusion_500, BOGI128_omega_diffusion_501, BOGI128_omega_diffusion_502, BOGI128_omega_diffusion_503,
	BOGI128_omega_diffusion_504, BOGI128_omega_diffusion_505, BOGI128_omega_diffusion_506, BOGI128_omega_diffusion_507,
	BOGI128_omega_diffusion_508, BOGI128_omega_diffusion_509, BOGI128_omega_diffusion_510, BOGI128_omega_diffusion_511,
	BOGI128_omega_diffusion_512, BOGI128_omega_diffusion_513, BOGI128_omega_diffusion_514, BOGI128_omega_diffusion_515,
	BOGI128_omega_diffusion_516, BOGI128_omega_diffusion_517, BOGI128_omega_diffusion_518, BOGI128_omega_diffusion_519,
	BOGI128_omega_diffusion_520, BOGI128_omega_diffusion_521, BOGI128_omega_diffusion_522, BOGI128_omega_diffusion_523,
	BOGI128_omega_diffusion_524, BOGI128_omega_diffusion_525, BOGI128_omega_diffusion_526, BOGI128_omega_diffusion_527,
	BOGI128_omega_diffusion_528, BOGI128_omega_diffusion_529, BOGI128_omega_diffusion_530, BOGI128_omega_diffusion_531,
	BOGI128_omega_diffusion_532, BOGI128_omega_diffusion_533, BOGI128_omega_diffusion_534, BOGI128_omega_diffusion_535,
	BOGI128_omega_diffusion_536, BOGI128_omega_diffusion_537, BOGI128_omega_diffusion_538, BOGI128_omega_diffusion_539,
	BOGI128_omega_diffusion_540, BOGI128_omega_diffusion_541, BOGI128_omega_diffusion_542, BOGI128_omega_diffusion_543,
	BOGI128_omega_diffusion_544, BOGI128_omega_diffusion_545, BOGI128_omega_diffusion_546, BOGI128_omega_diffusion_547,
	BOGI128_omega_diffusion_548, BOGI128_omega_diffusion_549, BOGI128_omega_diffusion_550, BOGI128_omega_diffusion_551,
	BOGI128_omega_diffusion_552, BOGI128_omega_diffusion_553, BOGI128_omega_diffusion_554, BOGI128_omega_diffusion_555,
	BOGI128_omega_diffusion_556, BOGI128_omega_diffusion_557, BOGI128_omega_diffusion_558, BOGI128_omega_diffusion_559,
	BOGI128_omega_diffusion_560, BOGI128_omega_diffusion_561, BOGI128_omega_diffusion_562, BOGI128_omega_diffusion_563,
	BOGI128_omega_diffusion_564, BOGI128_omega_diffusion_565, BOGI128_omega_diffusion_566, BOGI128_omega_diffusion_567,
	BOGI128_omega_diffusion_568, BOGI128_omega_diffusion_569, BOGI128_omega_diffusion_570, BOGI128_omega_diffusion_571,
	BOGI128_omega_diffusion_572, BOGI128_omega_diffusion_573, BOGI128_omega_diffusion_574, BOGI128_omega_diffusion_575,
};
void(*BOGI128_omega_inv_diffusion[576])(DIFF_I_WRD_t *, DIFF_O_WRD_t *) = {
	BOGI128_omega_inv_diffusion_0, BOGI128_omega_inv_diffusion_1, BOGI128_omega_inv_diffusion_2, BOGI128_omega_inv_diffusion_3,
	BOGI128_omega_inv_diffusion_4, BOGI128_omega_inv_diffusion_5, BOGI128_omega_inv_diffusion_6, BOGI128_omega_inv_diffusion_7,
	BOGI128_omega_inv_diffusion_8, BOGI128_omega_inv_diffusion_9, BOGI128_omega_inv_diffusion_10, BOGI128_omega_inv_diffusion_11,
	BOGI128_omega_inv_diffusion_12, BOGI128_omega_inv_diffusion_13, BOGI128_omega_inv_diffusion_14, BOGI128_omega_inv_diffusion_15,
	BOGI128_omega_inv_diffusion_16, BOGI128_omega_inv_diffusion_17, BOGI128_omega_inv_diffusion_18, BOGI128_omega_inv_diffusion_19,
	BOGI128_omega_inv_diffusion_20, BOGI128_omega_inv_diffusion_21, BOGI128_omega_inv_diffusion_22, BOGI128_omega_inv_diffusion_23,
	BOGI128_omega_inv_diffusion_24, BOGI128_omega_inv_diffusion_25, BOGI128_omega_inv_diffusion_26, BOGI128_omega_inv_diffusion_27,
	BOGI128_omega_inv_diffusion_28, BOGI128_omega_inv_diffusion_29, BOGI128_omega_inv_diffusion_30, BOGI128_omega_inv_diffusion_31,
	BOGI128_omega_inv_diffusion_32, BOGI128_omega_inv_diffusion_33, BOGI128_omega_inv_diffusion_34, BOGI128_omega_inv_diffusion_35,
	BOGI128_omega_inv_diffusion_36, BOGI128_omega_inv_diffusion_37, BOGI128_omega_inv_diffusion_38, BOGI128_omega_inv_diffusion_39,
	BOGI128_omega_inv_diffusion_40, BOGI128_omega_inv_diffusion_41, BOGI128_omega_inv_diffusion_42, BOGI128_omega_inv_diffusion_43,
	BOGI128_omega_inv_diffusion_44, BOGI128_omega_inv_diffusion_45, BOGI128_omega_inv_diffusion_46, BOGI128_omega_inv_diffusion_47,
	BOGI128_omega_inv_diffusion_48, BOGI128_omega_inv_diffusion_49, BOGI128_omega_inv_diffusion_50, BOGI128_omega_inv_diffusion_51,
	BOGI128_omega_inv_diffusion_52, BOGI128_omega_inv_diffusion_53, BOGI128_omega_inv_diffusion_54, BOGI128_omega_inv_diffusion_55,
	BOGI128_omega_inv_diffusion_56, BOGI128_omega_inv_diffusion_57, BOGI128_omega_inv_diffusion_58, BOGI128_omega_inv_diffusion_59,
	BOGI128_omega_inv_diffusion_60, BOGI128_omega_inv_diffusion_61, BOGI128_omega_inv_diffusion_62, BOGI128_omega_inv_diffusion_63,
	BOGI128_omega_inv_diffusion_64, BOGI128_omega_inv_diffusion_65, BOGI128_omega_inv_diffusion_66, BOGI128_omega_inv_diffusion_67,
	BOGI128_omega_inv_diffusion_68, BOGI128_omega_inv_diffusion_69, BOGI128_omega_inv_diffusion_70, BOGI128_omega_inv_diffusion_71,
	BOGI128_omega_inv_diffusion_72, BOGI128_omega_inv_diffusion_73, BOGI128_omega_inv_diffusion_74, BOGI128_omega_inv_diffusion_75,
	BOGI128_omega_inv_diffusion_76, BOGI128_omega_inv_diffusion_77, BOGI128_omega_inv_diffusion_78, BOGI128_omega_inv_diffusion_79,
	BOGI128_omega_inv_diffusion_80, BOGI128_omega_inv_diffusion_81, BOGI128_omega_inv_diffusion_82, BOGI128_omega_inv_diffusion_83,
	BOGI128_omega_inv_diffusion_84, BOGI128_omega_inv_diffusion_85, BOGI128_omega_inv_diffusion_86, BOGI128_omega_inv_diffusion_87,
	BOGI128_omega_inv_diffusion_88, BOGI128_omega_inv_diffusion_89, BOGI128_omega_inv_diffusion_90, BOGI128_omega_inv_diffusion_91,
	BOGI128_omega_inv_diffusion_92, BOGI128_omega_inv_diffusion_93, BOGI128_omega_inv_diffusion_94, BOGI128_omega_inv_diffusion_95,
	BOGI128_omega_inv_diffusion_96, BOGI128_omega_inv_diffusion_97, BOGI128_omega_inv_diffusion_98, BOGI128_omega_inv_diffusion_99,
	BOGI128_omega_inv_diffusion_100, BOGI128_omega_inv_diffusion_101, BOGI128_omega_inv_diffusion_102, BOGI128_omega_inv_diffusion_103,
	BOGI128_omega_inv_diffusion_104, BOGI128_omega_inv_diffusion_105, BOGI128_omega_inv_diffusion_106, BOGI128_omega_inv_diffusion_107,
	BOGI128_omega_inv_diffusion_108, BOGI128_omega_inv_diffusion_109, BOGI128_omega_inv_diffusion_110, BOGI128_omega_inv_diffusion_111,
	BOGI128_omega_inv_diffusion_112, BOGI128_omega_inv_diffusion_113, BOGI128_omega_inv_diffusion_114, BOGI128_omega_inv_diffusion_115,
	BOGI128_omega_inv_diffusion_116, BOGI128_omega_inv_diffusion_117, BOGI128_omega_inv_diffusion_118, BOGI128_omega_inv_diffusion_119,
	BOGI128_omega_inv_diffusion_120, BOGI128_omega_inv_diffusion_121, BOGI128_omega_inv_diffusion_122, BOGI128_omega_inv_diffusion_123,
	BOGI128_omega_inv_diffusion_124, BOGI128_omega_inv_diffusion_125, BOGI128_omega_inv_diffusion_126, BOGI128_omega_inv_diffusion_127,
	BOGI128_omega_inv_diffusion_128, BOGI128_omega_inv_diffusion_129, BOGI128_omega_inv_diffusion_130, BOGI128_omega_inv_diffusion_131,
	BOGI128_omega_inv_diffusion_132, BOGI128_omega_inv_diffusion_133, BOGI128_omega_inv_diffusion_134, BOGI128_omega_inv_diffusion_135,
	BOGI128_omega_inv_diffusion_136, BOGI128_omega_inv_diffusion_137, BOGI128_omega_inv_diffusion_138, BOGI128_omega_inv_diffusion_139,
	BOGI128_omega_inv_diffusion_140, BOGI128_omega_inv_diffusion_141, BOGI128_omega_inv_diffusion_142, BOGI128_omega_inv_diffusion_143,
	BOGI128_omega_inv_diffusion_144, BOGI128_omega_inv_diffusion_145, BOGI128_omega_inv_diffusion_146, BOGI128_omega_inv_diffusion_147,
	BOGI128_omega_inv_diffusion_148, BOGI128_omega_inv_diffusion_149, BOGI128_omega_inv_diffusion_150, BOGI128_omega_inv_diffusion_151,
	BOGI128_omega_inv_diffusion_152, BOGI128_omega_inv_diffusion_153, BOGI128_omega_inv_diffusion_154, BOGI128_omega_inv_diffusion_155,
	BOGI128_omega_inv_diffusion_156, BOGI128_omega_inv_diffusion_157, BOGI128_omega_inv_diffusion_158, BOGI128_omega_inv_diffusion_159,
	BOGI128_omega_inv_diffusion_160, BOGI128_omega_inv_diffusion_161, BOGI128_omega_inv_diffusion_162, BOGI128_omega_inv_diffusion_163,
	BOGI128_omega_inv_diffusion_164, BOGI128_omega_inv_diffusion_165, BOGI128_omega_inv_diffusion_166, BOGI128_omega_inv_diffusion_167,
	BOGI128_omega_inv_diffusion_168, BOGI128_omega_inv_diffusion_169, BOGI128_omega_inv_diffusion_170, BOGI128_omega_inv_diffusion_171,
	BOGI128_omega_inv_diffusion_172, BOGI128_omega_inv_diffusion_173, BOGI128_omega_inv_diffusion_174, BOGI128_omega_inv_diffusion_175,
	BOGI128_omega_inv_diffusion_176, BOGI128_omega_inv_diffusion_177, BOGI128_omega_inv_diffusion_178, BOGI128_omega_inv_diffusion_179,
	BOGI128_omega_inv_diffusion_180, BOGI128_omega_inv_diffusion_181, BOGI128_omega_inv_diffusion_182, BOGI128_omega_inv_diffusion_183,
	BOGI128_omega_inv_diffusion_184, BOGI128_omega_inv_diffusion_185, BOGI128_omega_inv_diffusion_186, BOGI128_omega_inv_diffusion_187,
	BOGI128_omega_inv_diffusion_188, BOGI128_omega_inv_diffusion_189, BOGI128_omega_inv_diffusion_190, BOGI128_omega_inv_diffusion_191,
	BOGI128_omega_inv_diffusion_192, BOGI128_omega_inv_diffusion_193, BOGI128_omega_inv_diffusion_194, BOGI128_omega_inv_diffusion_195,
	BOGI128_omega_inv_diffusion_196, BOGI128_omega_inv_diffusion_197, BOGI128_omega_inv_diffusion_198, BOGI128_omega_inv_diffusion_199,
	BOGI128_omega_inv_diffusion_200, BOGI128_omega_inv_diffusion_201, BOGI128_omega_inv_diffusion_202, BOGI128_omega_inv_diffusion_203,
	BOGI128_omega_inv_diffusion_204, BOGI128_omega_inv_diffusion_205, BOGI128_omega_inv_diffusion_206, BOGI128_omega_inv_diffusion_207,
	BOGI128_omega_inv_diffusion_208, BOGI128_omega_inv_diffusion_209, BOGI128_omega_inv_diffusion_210, BOGI128_omega_inv_diffusion_211,
	BOGI128_omega_inv_diffusion_212, BOGI128_omega_inv_diffusion_213, BOGI128_omega_inv_diffusion_214, BOGI128_omega_inv_diffusion_215,
	BOGI128_omega_inv_diffusion_216, BOGI128_omega_inv_diffusion_217, BOGI128_omega_inv_diffusion_218, BOGI128_omega_inv_diffusion_219,
	BOGI128_omega_inv_diffusion_220, BOGI128_omega_inv_diffusion_221, BOGI128_omega_inv_diffusion_222, BOGI128_omega_inv_diffusion_223,
	BOGI128_omega_inv_diffusion_224, BOGI128_omega_inv_diffusion_225, BOGI128_omega_inv_diffusion_226, BOGI128_omega_inv_diffusion_227,
	BOGI128_omega_inv_diffusion_228, BOGI128_omega_inv_diffusion_229, BOGI128_omega_inv_diffusion_230, BOGI128_omega_inv_diffusion_231,
	BOGI128_omega_inv_diffusion_232, BOGI128_omega_inv_diffusion_233, BOGI128_omega_inv_diffusion_234, BOGI128_omega_inv_diffusion_235,
	BOGI128_omega_inv_diffusion_236, BOGI128_omega_inv_diffusion_237, BOGI128_omega_inv_diffusion_238, BOGI128_omega_inv_diffusion_239,
	BOGI128_omega_inv_diffusion_240, BOGI128_omega_inv_diffusion_241, BOGI128_omega_inv_diffusion_242, BOGI128_omega_inv_diffusion_243,
	BOGI128_omega_inv_diffusion_244, BOGI128_omega_inv_diffusion_245, BOGI128_omega_inv_diffusion_246, BOGI128_omega_inv_diffusion_247,
	BOGI128_omega_inv_diffusion_248, BOGI128_omega_inv_diffusion_249, BOGI128_omega_inv_diffusion_250, BOGI128_omega_inv_diffusion_251,
	BOGI128_omega_inv_diffusion_252, BOGI128_omega_inv_diffusion_253, BOGI128_omega_inv_diffusion_254, BOGI128_omega_inv_diffusion_255,
	BOGI128_omega_inv_diffusion_256, BOGI128_omega_inv_diffusion_257, BOGI128_omega_inv_diffusion_258, BOGI128_omega_inv_diffusion_259,
	BOGI128_omega_inv_diffusion_260, BOGI128_omega_inv_diffusion_261, BOGI128_omega_inv_diffusion_262, BOGI128_omega_inv_diffusion_263,
	BOGI128_omega_inv_diffusion_264, BOGI128_omega_inv_diffusion_265, BOGI128_omega_inv_diffusion_266, BOGI128_omega_inv_diffusion_267,
	BOGI128_omega_inv_diffusion_268, BOGI128_omega_inv_diffusion_269, BOGI128_omega_inv_diffusion_270, BOGI128_omega_inv_diffusion_271,
	BOGI128_omega_inv_diffusion_272, BOGI128_omega_inv_diffusion_273, BOGI128_omega_inv_diffusion_274, BOGI128_omega_inv_diffusion_275,
	BOGI128_omega_inv_diffusion_276, BOGI128_omega_inv_diffusion_277, BOGI128_omega_inv_diffusion_278, BOGI128_omega_inv_diffusion_279,
	BOGI128_omega_inv_diffusion_280, BOGI128_omega_inv_diffusion_281, BOGI128_omega_inv_diffusion_282, BOGI128_omega_inv_diffusion_283,
	BOGI128_omega_inv_diffusion_284, BOGI128_omega_inv_diffusion_285, BOGI128_omega_inv_diffusion_286, BOGI128_omega_inv_diffusion_287,
	BOGI128_omega_inv_diffusion_288, BOGI128_omega_inv_diffusion_289, BOGI128_omega_inv_diffusion_290, BOGI128_omega_inv_diffusion_291,
	BOGI128_omega_inv_diffusion_292, BOGI128_omega_inv_diffusion_293, BOGI128_omega_inv_diffusion_294, BOGI128_omega_inv_diffusion_295,
	BOGI128_omega_inv_diffusion_296, BOGI128_omega_inv_diffusion_297, BOGI128_omega_inv_diffusion_298, BOGI128_omega_inv_diffusion_299,
	BOGI128_omega_inv_diffusion_300, BOGI128_omega_inv_diffusion_301, BOGI128_omega_inv_diffusion_302, BOGI128_omega_inv_diffusion_303,
	BOGI128_omega_inv_diffusion_304, BOGI128_omega_inv_diffusion_305, BOGI128_omega_inv_diffusion_306, BOGI128_omega_inv_diffusion_307,
	BOGI128_omega_inv_diffusion_308, BOGI128_omega_inv_diffusion_309, BOGI128_omega_inv_diffusion_310, BOGI128_omega_inv_diffusion_311,
	BOGI128_omega_inv_diffusion_312, BOGI128_omega_inv_diffusion_313, BOGI128_omega_inv_diffusion_314, BOGI128_omega_inv_diffusion_315,
	BOGI128_omega_inv_diffusion_316, BOGI128_omega_inv_diffusion_317, BOGI128_omega_inv_diffusion_318, BOGI128_omega_inv_diffusion_319,
	BOGI128_omega_inv_diffusion_320, BOGI128_omega_inv_diffusion_321, BOGI128_omega_inv_diffusion_322, BOGI128_omega_inv_diffusion_323,
	BOGI128_omega_inv_diffusion_324, BOGI128_omega_inv_diffusion_325, BOGI128_omega_inv_diffusion_326, BOGI128_omega_inv_diffusion_327,
	BOGI128_omega_inv_diffusion_328, BOGI128_omega_inv_diffusion_329, BOGI128_omega_inv_diffusion_330, BOGI128_omega_inv_diffusion_331,
	BOGI128_omega_inv_diffusion_332, BOGI128_omega_inv_diffusion_333, BOGI128_omega_inv_diffusion_334, BOGI128_omega_inv_diffusion_335,
	BOGI128_omega_inv_diffusion_336, BOGI128_omega_inv_diffusion_337, BOGI128_omega_inv_diffusion_338, BOGI128_omega_inv_diffusion_339,
	BOGI128_omega_inv_diffusion_340, BOGI128_omega_inv_diffusion_341, BOGI128_omega_inv_diffusion_342, BOGI128_omega_inv_diffusion_343,
	BOGI128_omega_inv_diffusion_344, BOGI128_omega_inv_diffusion_345, BOGI128_omega_inv_diffusion_346, BOGI128_omega_inv_diffusion_347,
	BOGI128_omega_inv_diffusion_348, BOGI128_omega_inv_diffusion_349, BOGI128_omega_inv_diffusion_350, BOGI128_omega_inv_diffusion_351,
	BOGI128_omega_inv_diffusion_352, BOGI128_omega_inv_diffusion_353, BOGI128_omega_inv_diffusion_354, BOGI128_omega_inv_diffusion_355,
	BOGI128_omega_inv_diffusion_356, BOGI128_omega_inv_diffusion_357, BOGI128_omega_inv_diffusion_358, BOGI128_omega_inv_diffusion_359,
	BOGI128_omega_inv_diffusion_360, BOGI128_omega_inv_diffusion_361, BOGI128_omega_inv_diffusion_362, BOGI128_omega_inv_diffusion_363,
	BOGI128_omega_inv_diffusion_364, BOGI128_omega_inv_diffusion_365, BOGI128_omega_inv_diffusion_366, BOGI128_omega_inv_diffusion_367,
	BOGI128_omega_inv_diffusion_368, BOGI128_omega_inv_diffusion_369, BOGI128_omega_inv_diffusion_370, BOGI128_omega_inv_diffusion_371,
	BOGI128_omega_inv_diffusion_372, BOGI128_omega_inv_diffusion_373, BOGI128_omega_inv_diffusion_374, BOGI128_omega_inv_diffusion_375,
	BOGI128_omega_inv_diffusion_376, BOGI128_omega_inv_diffusion_377, BOGI128_omega_inv_diffusion_378, BOGI128_omega_inv_diffusion_379,
	BOGI128_omega_inv_diffusion_380, BOGI128_omega_inv_diffusion_381, BOGI128_omega_inv_diffusion_382, BOGI128_omega_inv_diffusion_383,
	BOGI128_omega_inv_diffusion_384, BOGI128_omega_inv_diffusion_385, BOGI128_omega_inv_diffusion_386, BOGI128_omega_inv_diffusion_387,
	BOGI128_omega_inv_diffusion_388, BOGI128_omega_inv_diffusion_389, BOGI128_omega_inv_diffusion_390, BOGI128_omega_inv_diffusion_391,
	BOGI128_omega_inv_diffusion_392, BOGI128_omega_inv_diffusion_393, BOGI128_omega_inv_diffusion_394, BOGI128_omega_inv_diffusion_395,
	BOGI128_omega_inv_diffusion_396, BOGI128_omega_inv_diffusion_397, BOGI128_omega_inv_diffusion_398, BOGI128_omega_inv_diffusion_399,
	BOGI128_omega_inv_diffusion_400, BOGI128_omega_inv_diffusion_401, BOGI128_omega_inv_diffusion_402, BOGI128_omega_inv_diffusion_403,
	BOGI128_omega_inv_diffusion_404, BOGI128_omega_inv_diffusion_405, BOGI128_omega_inv_diffusion_406, BOGI128_omega_inv_diffusion_407,
	BOGI128_omega_inv_diffusion_408, BOGI128_omega_inv_diffusion_409, BOGI128_omega_inv_diffusion_410, BOGI128_omega_inv_diffusion_411,
	BOGI128_omega_inv_diffusion_412, BOGI128_omega_inv_diffusion_413, BOGI128_omega_inv_diffusion_414, BOGI128_omega_inv_diffusion_415,
	BOGI128_omega_inv_diffusion_416, BOGI128_omega_inv_diffusion_417, BOGI128_omega_inv_diffusion_418, BOGI128_omega_inv_diffusion_419,
	BOGI128_omega_inv_diffusion_420, BOGI128_omega_inv_diffusion_421, BOGI128_omega_inv_diffusion_422, BOGI128_omega_inv_diffusion_423,
	BOGI128_omega_inv_diffusion_424, BOGI128_omega_inv_diffusion_425, BOGI128_omega_inv_diffusion_426, BOGI128_omega_inv_diffusion_427,
	BOGI128_omega_inv_diffusion_428, BOGI128_omega_inv_diffusion_429, BOGI128_omega_inv_diffusion_430, BOGI128_omega_inv_diffusion_431,
	BOGI128_omega_inv_diffusion_432, BOGI128_omega_inv_diffusion_433, BOGI128_omega_inv_diffusion_434, BOGI128_omega_inv_diffusion_435,
	BOGI128_omega_inv_diffusion_436, BOGI128_omega_inv_diffusion_437, BOGI128_omega_inv_diffusion_438, BOGI128_omega_inv_diffusion_439,
	BOGI128_omega_inv_diffusion_440, BOGI128_omega_inv_diffusion_441, BOGI128_omega_inv_diffusion_442, BOGI128_omega_inv_diffusion_443,
	BOGI128_omega_inv_diffusion_444, BOGI128_omega_inv_diffusion_445, BOGI128_omega_inv_diffusion_446, BOGI128_omega_inv_diffusion_447,
	BOGI128_omega_inv_diffusion_448, BOGI128_omega_inv_diffusion_449, BOGI128_omega_inv_diffusion_450, BOGI128_omega_inv_diffusion_451,
	BOGI128_omega_inv_diffusion_452, BOGI128_omega_inv_diffusion_453, BOGI128_omega_inv_diffusion_454, BOGI128_omega_inv_diffusion_455,
	BOGI128_omega_inv_diffusion_456, BOGI128_omega_inv_diffusion_457, BOGI128_omega_inv_diffusion_458, BOGI128_omega_inv_diffusion_459,
	BOGI128_omega_inv_diffusion_460, BOGI128_omega_inv_diffusion_461, BOGI128_omega_inv_diffusion_462, BOGI128_omega_inv_diffusion_463,
	BOGI128_omega_inv_diffusion_464, BOGI128_omega_inv_diffusion_465, BOGI128_omega_inv_diffusion_466, BOGI128_omega_inv_diffusion_467,
	BOGI128_omega_inv_diffusion_468, BOGI128_omega_inv_diffusion_469, BOGI128_omega_inv_diffusion_470, BOGI128_omega_inv_diffusion_471,
	BOGI128_omega_inv_diffusion_472, BOGI128_omega_inv_diffusion_473, BOGI128_omega_inv_diffusion_474, BOGI128_omega_inv_diffusion_475,
	BOGI128_omega_inv_diffusion_476, BOGI128_omega_inv_diffusion_477, BOGI128_omega_inv_diffusion_478, BOGI128_omega_inv_diffusion_479,
	BOGI128_omega_inv_diffusion_480, BOGI128_omega_inv_diffusion_481, BOGI128_omega_inv_diffusion_482, BOGI128_omega_inv_diffusion_483,
	BOGI128_omega_inv_diffusion_484, BOGI128_omega_inv_diffusion_485, BOGI128_omega_inv_diffusion_486, BOGI128_omega_inv_diffusion_487,
	BOGI128_omega_inv_diffusion_488, BOGI128_omega_inv_diffusion_489, BOGI128_omega_inv_diffusion_490, BOGI128_omega_inv_diffusion_491,
	BOGI128_omega_inv_diffusion_492, BOGI128_omega_inv_diffusion_493, BOGI128_omega_inv_diffusion_494, BOGI128_omega_inv_diffusion_495,
	BOGI128_omega_inv_diffusion_496, BOGI128_omega_inv_diffusion_497, BOGI128_omega_inv_diffusion_498, BOGI128_omega_inv_diffusion_499,
	BOGI128_omega_inv_diffusion_500, BOGI128_omega_inv_diffusion_501, BOGI128_omega_inv_diffusion_502, BOGI128_omega_inv_diffusion_503,
	BOGI128_omega_inv_diffusion_504, BOGI128_omega_inv_diffusion_505, BOGI128_omega_inv_diffusion_506, BOGI128_omega_inv_diffusion_507,
	BOGI128_omega_inv_diffusion_508, BOGI128_omega_inv_diffusion_509, BOGI128_omega_inv_diffusion_510, BOGI128_omega_inv_diffusion_511,
	BOGI128_omega_inv_diffusion_512, BOGI128_omega_inv_diffusion_513, BOGI128_omega_inv_diffusion_514, BOGI128_omega_inv_diffusion_515,
	BOGI128_omega_inv_diffusion_516, BOGI128_omega_inv_diffusion_517, BOGI128_omega_inv_diffusion_518, BOGI128_omega_inv_diffusion_519,
	BOGI128_omega_inv_diffusion_520, BOGI128_omega_inv_diffusion_521, BOGI128_omega_inv_diffusion_522, BOGI128_omega_inv_diffusion_523,
	BOGI128_omega_inv_diffusion_524, BOGI128_omega_inv_diffusion_525, BOGI128_omega_inv_diffusion_526, BOGI128_omega_inv_diffusion_527,
	BOGI128_omega_inv_diffusion_528, BOGI128_omega_inv_diffusion_529, BOGI128_omega_inv_diffusion_530, BOGI128_omega_inv_diffusion_531,
	BOGI128_omega_inv_diffusion_532, BOGI128_omega_inv_diffusion_533, BOGI128_omega_inv_diffusion_534, BOGI128_omega_inv_diffusion_535,
	BOGI128_omega_inv_diffusion_536, BOGI128_omega_inv_diffusion_537, BOGI128_omega_inv_diffusion_538, BOGI128_omega_inv_diffusion_539,
	BOGI128_omega_inv_diffusion_540, BOGI128_omega_inv_diffusion_541, BOGI128_omega_inv_diffusion_542, BOGI128_omega_inv_diffusion_543,
	BOGI128_omega_inv_diffusion_544, BOGI128_omega_inv_diffusion_545, BOGI128_omega_inv_diffusion_546, BOGI128_omega_inv_diffusion_547,
	BOGI128_omega_inv_diffusion_548, BOGI128_omega_inv_diffusion_549, BOGI128_omega_inv_diffusion_550, BOGI128_omega_inv_diffusion_551,
	BOGI128_omega_inv_diffusion_552, BOGI128_omega_inv_diffusion_553, BOGI128_omega_inv_diffusion_554, BOGI128_omega_inv_diffusion_555,
	BOGI128_omega_inv_diffusion_556, BOGI128_omega_inv_diffusion_557, BOGI128_omega_inv_diffusion_558, BOGI128_omega_inv_diffusion_559,
	BOGI128_omega_inv_diffusion_560, BOGI128_omega_inv_diffusion_561, BOGI128_omega_inv_diffusion_562, BOGI128_omega_inv_diffusion_563,
	BOGI128_omega_inv_diffusion_564, BOGI128_omega_inv_diffusion_565, BOGI128_omega_inv_diffusion_566, BOGI128_omega_inv_diffusion_567,
	BOGI128_omega_inv_diffusion_568, BOGI128_omega_inv_diffusion_569, BOGI128_omega_inv_diffusion_570, BOGI128_omega_inv_diffusion_571,
	BOGI128_omega_inv_diffusion_572, BOGI128_omega_inv_diffusion_573, BOGI128_omega_inv_diffusion_574, BOGI128_omega_inv_diffusion_575,
};
